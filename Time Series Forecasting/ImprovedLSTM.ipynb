{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S4248SM144NCEN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1992-01-01</th>\n",
       "      <td>3459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-02-01</th>\n",
       "      <td>3458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-03-01</th>\n",
       "      <td>4002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-04-01</th>\n",
       "      <td>4564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-05-01</th>\n",
       "      <td>4221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            S4248SM144NCEN\n",
       "DATE                      \n",
       "1992-01-01            3459\n",
       "1992-02-01            3458\n",
       "1992-03-01            4002\n",
       "1992-04-01            4564\n",
       "1992-05-01            4221"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Alcohol_Sales.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3459,  3458,  4002,  4564,  4221,  4529,  4466,  4137,  4126,\n",
       "        4259,  4240,  4936,  3031,  3261,  4160,  4377,  4307,  4696,\n",
       "        4458,  4457,  4364,  4236,  4500,  4974,  3075,  3377,  4443,\n",
       "        4261,  4460,  4985,  4324,  4719,  4374,  4248,  4784,  4971,\n",
       "        3370,  3484,  4269,  3994,  4715,  4974,  4223,  5000,  4235,\n",
       "        4554,  4851,  4826,  3699,  3983,  4262,  4619,  5219,  4836,\n",
       "        4941,  5062,  4365,  5012,  4850,  5097,  3758,  3825,  4454,\n",
       "        4635,  5210,  5057,  5231,  5034,  4970,  5342,  4831,  5965,\n",
       "        3796,  4019,  4898,  5090,  5237,  5447,  5435,  5107,  5515,\n",
       "        5583,  5346,  6286,  4032,  4435,  5479,  5483,  5587,  6176,\n",
       "        5621,  5889,  5828,  5849,  6180,  6771,  4243,  4952,  6008,\n",
       "        5353,  6435,  6673,  5636,  6630,  5887,  6322,  6520,  6678,\n",
       "        5082,  5216,  5893,  5894,  6799,  6667,  6374,  6840,  5575,\n",
       "        6545,  6789,  7180,  5117,  5442,  6337,  6525,  7216,  6761,\n",
       "        6958,  7070,  6148,  6924,  6716,  7975,  5326,  5609,  6414,\n",
       "        6741,  7144,  7133,  7568,  7266,  6634,  7626,  6843,  8540,\n",
       "        5629,  5898,  7045,  7094,  7333,  7918,  7289,  7396,  7259,\n",
       "        7268,  7731,  9058,  5557,  6237,  7723,  7262,  8241,  8757,\n",
       "        7352,  8496,  7741,  7710,  8247,  8902,  6066,  6590,  7923,\n",
       "        7335,  8843,  9327,  7792,  9156,  8037,  8640,  9128,  9545,\n",
       "        6627,  6743,  8195,  7828,  9570,  9484,  8608,  9543,  8123,\n",
       "        9649,  9390, 10065,  7093,  7483,  8365,  8895,  9794,  9977,\n",
       "        9553,  9375,  9225,  9948,  8758, 10839,  7266,  7578,  8688,\n",
       "        9162,  9369, 10167,  9507,  8923,  9272,  9075,  8949, 10843,\n",
       "        6558,  7481,  9475,  9424,  9351, 10552,  9077,  9273,  9420,\n",
       "        9413,  9866, 11455,  6901,  8014,  9832,  9281,  9967, 11344,\n",
       "        9106, 10469, 10085,  9612, 10328, 11483,  7486,  8641,  9709,\n",
       "        9423, 11342, 11274,  9845, 11163,  9532, 10754, 10953, 11922,\n",
       "        8395,  8888, 10110, 10493, 12218, 11385, 11186, 11462, 10494,\n",
       "       11540, 11138, 12709,  8557,  9059, 10055, 10977, 11792, 11904,\n",
       "       10965, 10981, 10828, 11817, 10470, 13310,  8400,  9062, 10722,\n",
       "       11107, 11508, 12904, 11869, 11224, 12022, 11983, 11506, 14183,\n",
       "        8648, 10321, 12107, 11420, 12238, 13681, 10950, 12700, 12272,\n",
       "       11905, 13016, 14421,  9043, 10452, 12481, 11491, 13545, 14730,\n",
       "       11416, 13402, 11907, 12711, 13261, 14265,  9564, 10415, 12683,\n",
       "       11919, 14138, 14583, 12640, 14257, 12396, 13914, 14174, 15504,\n",
       "       10718])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename({'S4248SM144NCEN': \"Sales\"}, axis=1)\n",
    "data = df['Sales'].values\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "plt.title('Alcohol Sales')\n",
    "plt.ylabel('Sales in million dollars')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis= 'x', tight=True)\n",
    "plt.plot(df['Sales'], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        target = data[i+seq_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "seq_length = 12  # Adjust as needed\n",
    "X, y = create_sequences(data_normalized, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.03431412],\n",
       "        [0.03423395],\n",
       "        [0.07784815],\n",
       "        ...,\n",
       "        [0.09845266],\n",
       "        [0.09692937],\n",
       "        [0.1527299 ]],\n",
       "\n",
       "       [[0.03423395],\n",
       "        [0.07784815],\n",
       "        [0.12290548],\n",
       "        ...,\n",
       "        [0.09692937],\n",
       "        [0.1527299 ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.07784815],\n",
       "        [0.12290548],\n",
       "        [0.09540608],\n",
       "        ...,\n",
       "        [0.1527299 ],\n",
       "        [0.        ],\n",
       "        [0.01843983]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.82017157],\n",
       "        [0.90066544],\n",
       "        [0.52377135],\n",
       "        ...,\n",
       "        [0.90002405],\n",
       "        [0.75082178],\n",
       "        [0.87252465]],\n",
       "\n",
       "       [[0.90066544],\n",
       "        [0.52377135],\n",
       "        [0.59199872],\n",
       "        ...,\n",
       "        [0.75082178],\n",
       "        [0.87252465],\n",
       "        [0.89336968]],\n",
       "\n",
       "       [[0.52377135],\n",
       "        [0.59199872],\n",
       "        [0.77383148],\n",
       "        ...,\n",
       "        [0.87252465],\n",
       "        [0.89336968],\n",
       "        [1.        ]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "val_data = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "test_data = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # *2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # *2 for bidirectional\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "\n",
    "def grid_search(param_grid, train_loader, val_loader):\n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        \n",
    "        # Separate model params from training params\n",
    "        model_params = {k: v for k, v in param_dict.items() if k in ['hidden_size', 'num_layers', 'dropout']}\n",
    "        \n",
    "        model = BidirectionalLSTM(input_size=1, output_size=1, **model_params)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=param_dict['learning_rate'])\n",
    "\n",
    "        print(f\"\\nTraining with parameters: {param_dict}\")\n",
    "        for epoch in range(param_dict['epochs']):\n",
    "            train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "            val_loss = evaluate(model, val_loader, criterion)\n",
    "            print(f\"Epoch: {epoch+1:3d} Train Loss: {train_loss:.8f} Val Loss: {val_loss:.8f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_params = param_dict\n",
    "            best_model = model\n",
    "\n",
    "    return best_model, best_params, best_loss\n",
    "\n",
    "# Define parameter grid (this can remain the same)\n",
    "param_grid = {\n",
    "    'hidden_size': [32, 64, 128],\n",
    "    'num_layers': [1, 2],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'epochs': [50, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youweicheng/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.25629812 Val Loss: 0.24562234\n",
      "Epoch:   2 Train Loss: 0.21048800 Val Loss: 0.18312832\n",
      "Epoch:   3 Train Loss: 0.14579746 Val Loss: 0.11622709\n",
      "Epoch:   4 Train Loss: 0.08791854 Val Loss: 0.04909542\n",
      "Epoch:   5 Train Loss: 0.04927302 Val Loss: 0.03403086\n",
      "Epoch:   6 Train Loss: 0.05237372 Val Loss: 0.02783000\n",
      "Epoch:   7 Train Loss: 0.04098560 Val Loss: 0.03028478\n",
      "Epoch:   8 Train Loss: 0.04256937 Val Loss: 0.02746889\n",
      "Epoch:   9 Train Loss: 0.03772277 Val Loss: 0.02330957\n",
      "Epoch:  10 Train Loss: 0.03410881 Val Loss: 0.02192595\n",
      "Epoch:  11 Train Loss: 0.03147709 Val Loss: 0.02073816\n",
      "Epoch:  12 Train Loss: 0.02861329 Val Loss: 0.01865473\n",
      "Epoch:  13 Train Loss: 0.02538752 Val Loss: 0.01593394\n",
      "Epoch:  14 Train Loss: 0.02156454 Val Loss: 0.01394224\n",
      "Epoch:  15 Train Loss: 0.01829676 Val Loss: 0.01188780\n",
      "Epoch:  16 Train Loss: 0.01395893 Val Loss: 0.00981789\n",
      "Epoch:  17 Train Loss: 0.01056670 Val Loss: 0.00926313\n",
      "Epoch:  18 Train Loss: 0.01059040 Val Loss: 0.00825741\n",
      "Epoch:  19 Train Loss: 0.00940390 Val Loss: 0.01044286\n",
      "Epoch:  20 Train Loss: 0.01179587 Val Loss: 0.00911496\n",
      "Epoch:  21 Train Loss: 0.00958751 Val Loss: 0.00892141\n",
      "Epoch:  22 Train Loss: 0.00867412 Val Loss: 0.00792170\n",
      "Epoch:  23 Train Loss: 0.01110379 Val Loss: 0.00998190\n",
      "Epoch:  24 Train Loss: 0.01041055 Val Loss: 0.00762804\n",
      "Epoch:  25 Train Loss: 0.00963894 Val Loss: 0.00956515\n",
      "Epoch:  26 Train Loss: 0.00844141 Val Loss: 0.00781150\n",
      "Epoch:  27 Train Loss: 0.00958592 Val Loss: 0.00898268\n",
      "Epoch:  28 Train Loss: 0.01151579 Val Loss: 0.00791934\n",
      "Epoch:  29 Train Loss: 0.00963900 Val Loss: 0.00838846\n",
      "Epoch:  30 Train Loss: 0.01196542 Val Loss: 0.00933810\n",
      "Epoch:  31 Train Loss: 0.01139475 Val Loss: 0.00797794\n",
      "Epoch:  32 Train Loss: 0.01145295 Val Loss: 0.00765341\n",
      "Epoch:  33 Train Loss: 0.00858424 Val Loss: 0.00804337\n",
      "Epoch:  34 Train Loss: 0.00916073 Val Loss: 0.00801319\n",
      "Epoch:  35 Train Loss: 0.00886787 Val Loss: 0.00783125\n",
      "Epoch:  36 Train Loss: 0.00744578 Val Loss: 0.00731828\n",
      "Epoch:  37 Train Loss: 0.00855920 Val Loss: 0.00799501\n",
      "Epoch:  38 Train Loss: 0.00831768 Val Loss: 0.00779921\n",
      "Epoch:  39 Train Loss: 0.01063326 Val Loss: 0.00729324\n",
      "Epoch:  40 Train Loss: 0.00927151 Val Loss: 0.00819037\n",
      "Epoch:  41 Train Loss: 0.00756455 Val Loss: 0.00727285\n",
      "Epoch:  42 Train Loss: 0.00827393 Val Loss: 0.00783376\n",
      "Epoch:  43 Train Loss: 0.00747338 Val Loss: 0.00738553\n",
      "Epoch:  44 Train Loss: 0.00760841 Val Loss: 0.00765210\n",
      "Epoch:  45 Train Loss: 0.00701545 Val Loss: 0.00752744\n",
      "Epoch:  46 Train Loss: 0.00975152 Val Loss: 0.00774604\n",
      "Epoch:  47 Train Loss: 0.00854568 Val Loss: 0.00716228\n",
      "Epoch:  48 Train Loss: 0.00872941 Val Loss: 0.00782570\n",
      "Epoch:  49 Train Loss: 0.00756536 Val Loss: 0.00714192\n",
      "Epoch:  50 Train Loss: 0.01049628 Val Loss: 0.00757694\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youweicheng/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 Train Loss: 0.31441986 Val Loss: 0.27563995\n",
      "Epoch:   2 Train Loss: 0.23470065 Val Loss: 0.21884060\n",
      "Epoch:   3 Train Loss: 0.18998138 Val Loss: 0.16851143\n",
      "Epoch:   4 Train Loss: 0.15611936 Val Loss: 0.12035664\n",
      "Epoch:   5 Train Loss: 0.09802693 Val Loss: 0.07282841\n",
      "Epoch:   6 Train Loss: 0.06818944 Val Loss: 0.03460341\n",
      "Epoch:   7 Train Loss: 0.04774302 Val Loss: 0.03011035\n",
      "Epoch:   8 Train Loss: 0.04347950 Val Loss: 0.02602463\n",
      "Epoch:   9 Train Loss: 0.03789772 Val Loss: 0.02561631\n",
      "Epoch:  10 Train Loss: 0.03747353 Val Loss: 0.02501140\n",
      "Epoch:  11 Train Loss: 0.03151300 Val Loss: 0.02042644\n",
      "Epoch:  12 Train Loss: 0.02689370 Val Loss: 0.01732131\n",
      "Epoch:  13 Train Loss: 0.02572307 Val Loss: 0.01457425\n",
      "Epoch:  14 Train Loss: 0.01683787 Val Loss: 0.01183580\n",
      "Epoch:  15 Train Loss: 0.01583252 Val Loss: 0.00880186\n",
      "Epoch:  16 Train Loss: 0.00945811 Val Loss: 0.00807188\n",
      "Epoch:  17 Train Loss: 0.00976369 Val Loss: 0.00887556\n",
      "Epoch:  18 Train Loss: 0.01192591 Val Loss: 0.00891729\n",
      "Epoch:  19 Train Loss: 0.00979101 Val Loss: 0.00931643\n",
      "Epoch:  20 Train Loss: 0.00982532 Val Loss: 0.00756416\n",
      "Epoch:  21 Train Loss: 0.00990420 Val Loss: 0.00893069\n",
      "Epoch:  22 Train Loss: 0.01061370 Val Loss: 0.00857214\n",
      "Epoch:  23 Train Loss: 0.00979778 Val Loss: 0.00746732\n",
      "Epoch:  24 Train Loss: 0.00963033 Val Loss: 0.01060934\n",
      "Epoch:  25 Train Loss: 0.00825790 Val Loss: 0.00741016\n",
      "Epoch:  26 Train Loss: 0.00905891 Val Loss: 0.00770666\n",
      "Epoch:  27 Train Loss: 0.00889375 Val Loss: 0.00831501\n",
      "Epoch:  28 Train Loss: 0.00868944 Val Loss: 0.00743513\n",
      "Epoch:  29 Train Loss: 0.00816581 Val Loss: 0.00810306\n",
      "Epoch:  30 Train Loss: 0.00790854 Val Loss: 0.00735577\n",
      "Epoch:  31 Train Loss: 0.00849365 Val Loss: 0.00742052\n",
      "Epoch:  32 Train Loss: 0.00836832 Val Loss: 0.00816150\n",
      "Epoch:  33 Train Loss: 0.00802729 Val Loss: 0.00718161\n",
      "Epoch:  34 Train Loss: 0.00792902 Val Loss: 0.00774144\n",
      "Epoch:  35 Train Loss: 0.00837403 Val Loss: 0.00769576\n",
      "Epoch:  36 Train Loss: 0.00853693 Val Loss: 0.00718646\n",
      "Epoch:  37 Train Loss: 0.00995518 Val Loss: 0.00754694\n",
      "Epoch:  38 Train Loss: 0.00804239 Val Loss: 0.00751799\n",
      "Epoch:  39 Train Loss: 0.00795988 Val Loss: 0.00708382\n",
      "Epoch:  40 Train Loss: 0.00811389 Val Loss: 0.00709265\n",
      "Epoch:  41 Train Loss: 0.00736613 Val Loss: 0.00826789\n",
      "Epoch:  42 Train Loss: 0.00826423 Val Loss: 0.00711747\n",
      "Epoch:  43 Train Loss: 0.00788059 Val Loss: 0.00719919\n",
      "Epoch:  44 Train Loss: 0.00873367 Val Loss: 0.00738374\n",
      "Epoch:  45 Train Loss: 0.00798173 Val Loss: 0.00703889\n",
      "Epoch:  46 Train Loss: 0.00725889 Val Loss: 0.00775614\n",
      "Epoch:  47 Train Loss: 0.00818984 Val Loss: 0.00707578\n",
      "Epoch:  48 Train Loss: 0.00731923 Val Loss: 0.00708831\n",
      "Epoch:  49 Train Loss: 0.00814527 Val Loss: 0.00710143\n",
      "Epoch:  50 Train Loss: 0.00800084 Val Loss: 0.00771743\n",
      "Epoch:  51 Train Loss: 0.00820916 Val Loss: 0.00700103\n",
      "Epoch:  52 Train Loss: 0.00878900 Val Loss: 0.00716861\n",
      "Epoch:  53 Train Loss: 0.00710064 Val Loss: 0.00697480\n",
      "Epoch:  54 Train Loss: 0.00891191 Val Loss: 0.00752063\n",
      "Epoch:  55 Train Loss: 0.00979523 Val Loss: 0.00744228\n",
      "Epoch:  56 Train Loss: 0.00957459 Val Loss: 0.00694955\n",
      "Epoch:  57 Train Loss: 0.00744024 Val Loss: 0.00751294\n",
      "Epoch:  58 Train Loss: 0.00763127 Val Loss: 0.00697522\n",
      "Epoch:  59 Train Loss: 0.00694915 Val Loss: 0.00702766\n",
      "Epoch:  60 Train Loss: 0.00855026 Val Loss: 0.00717724\n",
      "Epoch:  61 Train Loss: 0.00904009 Val Loss: 0.00705057\n",
      "Epoch:  62 Train Loss: 0.00717629 Val Loss: 0.00704967\n",
      "Epoch:  63 Train Loss: 0.00742947 Val Loss: 0.00712540\n",
      "Epoch:  64 Train Loss: 0.00739707 Val Loss: 0.00701041\n",
      "Epoch:  65 Train Loss: 0.00937701 Val Loss: 0.00742143\n",
      "Epoch:  66 Train Loss: 0.00699834 Val Loss: 0.00695561\n",
      "Epoch:  67 Train Loss: 0.00777567 Val Loss: 0.00698390\n",
      "Epoch:  68 Train Loss: 0.00794669 Val Loss: 0.00714751\n",
      "Epoch:  69 Train Loss: 0.00823248 Val Loss: 0.00713362\n",
      "Epoch:  70 Train Loss: 0.00694327 Val Loss: 0.00705138\n",
      "Epoch:  71 Train Loss: 0.00689458 Val Loss: 0.00695243\n",
      "Epoch:  72 Train Loss: 0.00701792 Val Loss: 0.00691178\n",
      "Epoch:  73 Train Loss: 0.00757319 Val Loss: 0.00695415\n",
      "Epoch:  74 Train Loss: 0.00741972 Val Loss: 0.00695288\n",
      "Epoch:  75 Train Loss: 0.00906465 Val Loss: 0.00722691\n",
      "Epoch:  76 Train Loss: 0.00825329 Val Loss: 0.00750605\n",
      "Epoch:  77 Train Loss: 0.00925509 Val Loss: 0.00697362\n",
      "Epoch:  78 Train Loss: 0.00745780 Val Loss: 0.00775363\n",
      "Epoch:  79 Train Loss: 0.00767789 Val Loss: 0.00688485\n",
      "Epoch:  80 Train Loss: 0.00916842 Val Loss: 0.00712888\n",
      "Epoch:  81 Train Loss: 0.00893380 Val Loss: 0.00746757\n",
      "Epoch:  82 Train Loss: 0.00826627 Val Loss: 0.00700659\n",
      "Epoch:  83 Train Loss: 0.00736038 Val Loss: 0.00792534\n",
      "Epoch:  84 Train Loss: 0.00711372 Val Loss: 0.00714200\n",
      "Epoch:  85 Train Loss: 0.00680813 Val Loss: 0.00789656\n",
      "Epoch:  86 Train Loss: 0.00806715 Val Loss: 0.00687429\n",
      "Epoch:  87 Train Loss: 0.00737966 Val Loss: 0.00692383\n",
      "Epoch:  88 Train Loss: 0.00766448 Val Loss: 0.00769308\n",
      "Epoch:  89 Train Loss: 0.00859606 Val Loss: 0.00685580\n",
      "Epoch:  90 Train Loss: 0.00818676 Val Loss: 0.00783143\n",
      "Epoch:  91 Train Loss: 0.00782221 Val Loss: 0.00708164\n",
      "Epoch:  92 Train Loss: 0.00697029 Val Loss: 0.00709061\n",
      "Epoch:  93 Train Loss: 0.00762331 Val Loss: 0.00739863\n",
      "Epoch:  94 Train Loss: 0.00740039 Val Loss: 0.00689817\n",
      "Epoch:  95 Train Loss: 0.00811967 Val Loss: 0.00688991\n",
      "Epoch:  96 Train Loss: 0.00834533 Val Loss: 0.00688648\n",
      "Epoch:  97 Train Loss: 0.00776658 Val Loss: 0.00695127\n",
      "Epoch:  98 Train Loss: 0.00759373 Val Loss: 0.00699428\n",
      "Epoch:  99 Train Loss: 0.00757158 Val Loss: 0.00684553\n",
      "Epoch: 100 Train Loss: 0.00742987 Val Loss: 0.00688480\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.11933041 Val Loss: 0.03582850\n",
      "Epoch:   2 Train Loss: 0.04156734 Val Loss: 0.03598434\n",
      "Epoch:   3 Train Loss: 0.03767790 Val Loss: 0.02041264\n",
      "Epoch:   4 Train Loss: 0.02540095 Val Loss: 0.01616862\n",
      "Epoch:   5 Train Loss: 0.01605371 Val Loss: 0.01129097\n",
      "Epoch:   6 Train Loss: 0.01357090 Val Loss: 0.01091651\n",
      "Epoch:   7 Train Loss: 0.01058961 Val Loss: 0.00974833\n",
      "Epoch:   8 Train Loss: 0.01094124 Val Loss: 0.00970188\n",
      "Epoch:   9 Train Loss: 0.01054028 Val Loss: 0.00961219\n",
      "Epoch:  10 Train Loss: 0.00942453 Val Loss: 0.00958610\n",
      "Epoch:  11 Train Loss: 0.01042863 Val Loss: 0.01309772\n",
      "Epoch:  12 Train Loss: 0.01098148 Val Loss: 0.01262490\n",
      "Epoch:  13 Train Loss: 0.00953449 Val Loss: 0.01056459\n",
      "Epoch:  14 Train Loss: 0.00879773 Val Loss: 0.00999815\n",
      "Epoch:  15 Train Loss: 0.01025217 Val Loss: 0.00942774\n",
      "Epoch:  16 Train Loss: 0.01122265 Val Loss: 0.00936711\n",
      "Epoch:  17 Train Loss: 0.01019436 Val Loss: 0.00857840\n",
      "Epoch:  18 Train Loss: 0.00907327 Val Loss: 0.00909585\n",
      "Epoch:  19 Train Loss: 0.00849870 Val Loss: 0.00852028\n",
      "Epoch:  20 Train Loss: 0.00781400 Val Loss: 0.00857432\n",
      "Epoch:  21 Train Loss: 0.00894756 Val Loss: 0.00837158\n",
      "Epoch:  22 Train Loss: 0.00972519 Val Loss: 0.00828319\n",
      "Epoch:  23 Train Loss: 0.00856893 Val Loss: 0.00869079\n",
      "Epoch:  24 Train Loss: 0.00975125 Val Loss: 0.01099611\n",
      "Epoch:  25 Train Loss: 0.00833369 Val Loss: 0.00809403\n",
      "Epoch:  26 Train Loss: 0.00924301 Val Loss: 0.00837559\n",
      "Epoch:  27 Train Loss: 0.00956532 Val Loss: 0.01054410\n",
      "Epoch:  28 Train Loss: 0.01059769 Val Loss: 0.00974245\n",
      "Epoch:  29 Train Loss: 0.00908291 Val Loss: 0.00834491\n",
      "Epoch:  30 Train Loss: 0.00728215 Val Loss: 0.00849077\n",
      "Epoch:  31 Train Loss: 0.00798671 Val Loss: 0.00982066\n",
      "Epoch:  32 Train Loss: 0.00865204 Val Loss: 0.00800206\n",
      "Epoch:  33 Train Loss: 0.00839693 Val Loss: 0.00813015\n",
      "Epoch:  34 Train Loss: 0.00828830 Val Loss: 0.00797160\n",
      "Epoch:  35 Train Loss: 0.00755475 Val Loss: 0.00804463\n",
      "Epoch:  36 Train Loss: 0.00754131 Val Loss: 0.00869731\n",
      "Epoch:  37 Train Loss: 0.00778749 Val Loss: 0.00852862\n",
      "Epoch:  38 Train Loss: 0.00830145 Val Loss: 0.00784383\n",
      "Epoch:  39 Train Loss: 0.00838064 Val Loss: 0.00783572\n",
      "Epoch:  40 Train Loss: 0.00757219 Val Loss: 0.00786052\n",
      "Epoch:  41 Train Loss: 0.00916631 Val Loss: 0.00787482\n",
      "Epoch:  42 Train Loss: 0.00817212 Val Loss: 0.00760247\n",
      "Epoch:  43 Train Loss: 0.00967103 Val Loss: 0.00854463\n",
      "Epoch:  44 Train Loss: 0.00981110 Val Loss: 0.01109814\n",
      "Epoch:  45 Train Loss: 0.00927081 Val Loss: 0.00772340\n",
      "Epoch:  46 Train Loss: 0.00883483 Val Loss: 0.00840437\n",
      "Epoch:  47 Train Loss: 0.00958099 Val Loss: 0.00903109\n",
      "Epoch:  48 Train Loss: 0.00920087 Val Loss: 0.00762299\n",
      "Epoch:  49 Train Loss: 0.00733120 Val Loss: 0.00785282\n",
      "Epoch:  50 Train Loss: 0.01017441 Val Loss: 0.00899828\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.07283300 Val Loss: 0.02685683\n",
      "Epoch:   2 Train Loss: 0.03663490 Val Loss: 0.01810005\n",
      "Epoch:   3 Train Loss: 0.01857149 Val Loss: 0.01614583\n",
      "Epoch:   4 Train Loss: 0.01183259 Val Loss: 0.00928594\n",
      "Epoch:   5 Train Loss: 0.01113489 Val Loss: 0.01223814\n",
      "Epoch:   6 Train Loss: 0.01021454 Val Loss: 0.00957961\n",
      "Epoch:   7 Train Loss: 0.00947033 Val Loss: 0.00849535\n",
      "Epoch:   8 Train Loss: 0.00859697 Val Loss: 0.00967935\n",
      "Epoch:   9 Train Loss: 0.01216076 Val Loss: 0.00954089\n",
      "Epoch:  10 Train Loss: 0.01178436 Val Loss: 0.00947507\n",
      "Epoch:  11 Train Loss: 0.01011703 Val Loss: 0.00931256\n",
      "Epoch:  12 Train Loss: 0.00975147 Val Loss: 0.00871329\n",
      "Epoch:  13 Train Loss: 0.01093693 Val Loss: 0.01014335\n",
      "Epoch:  14 Train Loss: 0.01054231 Val Loss: 0.01042609\n",
      "Epoch:  15 Train Loss: 0.00911446 Val Loss: 0.00955353\n",
      "Epoch:  16 Train Loss: 0.00818090 Val Loss: 0.00883158\n",
      "Epoch:  17 Train Loss: 0.00999801 Val Loss: 0.00822252\n",
      "Epoch:  18 Train Loss: 0.00805266 Val Loss: 0.00822482\n",
      "Epoch:  19 Train Loss: 0.00914991 Val Loss: 0.00916424\n",
      "Epoch:  20 Train Loss: 0.00839151 Val Loss: 0.00795774\n",
      "Epoch:  21 Train Loss: 0.00911423 Val Loss: 0.00862249\n",
      "Epoch:  22 Train Loss: 0.00682978 Val Loss: 0.00859169\n",
      "Epoch:  23 Train Loss: 0.00917944 Val Loss: 0.00803074\n",
      "Epoch:  24 Train Loss: 0.00861853 Val Loss: 0.00989954\n",
      "Epoch:  25 Train Loss: 0.01298816 Val Loss: 0.00949635\n",
      "Epoch:  26 Train Loss: 0.00813416 Val Loss: 0.00816607\n",
      "Epoch:  27 Train Loss: 0.00906848 Val Loss: 0.00793968\n",
      "Epoch:  28 Train Loss: 0.00895472 Val Loss: 0.00831930\n",
      "Epoch:  29 Train Loss: 0.00846335 Val Loss: 0.00864073\n",
      "Epoch:  30 Train Loss: 0.00924648 Val Loss: 0.00789727\n",
      "Epoch:  31 Train Loss: 0.00822301 Val Loss: 0.00787116\n",
      "Epoch:  32 Train Loss: 0.00744941 Val Loss: 0.00789716\n",
      "Epoch:  33 Train Loss: 0.00792002 Val Loss: 0.00820670\n",
      "Epoch:  34 Train Loss: 0.00754961 Val Loss: 0.00770448\n",
      "Epoch:  35 Train Loss: 0.00790827 Val Loss: 0.00800553\n",
      "Epoch:  36 Train Loss: 0.00737640 Val Loss: 0.00837703\n",
      "Epoch:  37 Train Loss: 0.00655008 Val Loss: 0.00773581\n",
      "Epoch:  38 Train Loss: 0.00756622 Val Loss: 0.00870500\n",
      "Epoch:  39 Train Loss: 0.00949512 Val Loss: 0.00775979\n",
      "Epoch:  40 Train Loss: 0.00737619 Val Loss: 0.00852326\n",
      "Epoch:  41 Train Loss: 0.00805870 Val Loss: 0.00919840\n",
      "Epoch:  42 Train Loss: 0.00770065 Val Loss: 0.00898163\n",
      "Epoch:  43 Train Loss: 0.00936959 Val Loss: 0.00893239\n",
      "Epoch:  44 Train Loss: 0.00859045 Val Loss: 0.01048780\n",
      "Epoch:  45 Train Loss: 0.00989550 Val Loss: 0.00790978\n",
      "Epoch:  46 Train Loss: 0.00769513 Val Loss: 0.00997389\n",
      "Epoch:  47 Train Loss: 0.00862141 Val Loss: 0.00755651\n",
      "Epoch:  48 Train Loss: 0.00804203 Val Loss: 0.00854571\n",
      "Epoch:  49 Train Loss: 0.00797070 Val Loss: 0.00977040\n",
      "Epoch:  50 Train Loss: 0.00896918 Val Loss: 0.00751667\n",
      "Epoch:  51 Train Loss: 0.00996432 Val Loss: 0.00796021\n",
      "Epoch:  52 Train Loss: 0.00752876 Val Loss: 0.00746614\n",
      "Epoch:  53 Train Loss: 0.00919287 Val Loss: 0.00738843\n",
      "Epoch:  54 Train Loss: 0.00752903 Val Loss: 0.00808226\n",
      "Epoch:  55 Train Loss: 0.00698492 Val Loss: 0.00803700\n",
      "Epoch:  56 Train Loss: 0.00732263 Val Loss: 0.00754053\n",
      "Epoch:  57 Train Loss: 0.00941331 Val Loss: 0.00820913\n",
      "Epoch:  58 Train Loss: 0.00861373 Val Loss: 0.00717319\n",
      "Epoch:  59 Train Loss: 0.00651843 Val Loss: 0.00708861\n",
      "Epoch:  60 Train Loss: 0.00674927 Val Loss: 0.00777353\n",
      "Epoch:  61 Train Loss: 0.00767487 Val Loss: 0.00934443\n",
      "Epoch:  62 Train Loss: 0.00748785 Val Loss: 0.00702401\n",
      "Epoch:  63 Train Loss: 0.00801845 Val Loss: 0.00788195\n",
      "Epoch:  64 Train Loss: 0.00852073 Val Loss: 0.01176216\n",
      "Epoch:  65 Train Loss: 0.01085967 Val Loss: 0.00792098\n",
      "Epoch:  66 Train Loss: 0.00689381 Val Loss: 0.00836788\n",
      "Epoch:  67 Train Loss: 0.00849013 Val Loss: 0.00895351\n",
      "Epoch:  68 Train Loss: 0.00848545 Val Loss: 0.00962297\n",
      "Epoch:  69 Train Loss: 0.00957744 Val Loss: 0.00938418\n",
      "Epoch:  70 Train Loss: 0.00788417 Val Loss: 0.00769042\n",
      "Epoch:  71 Train Loss: 0.00692617 Val Loss: 0.00724009\n",
      "Epoch:  72 Train Loss: 0.00671715 Val Loss: 0.00760642\n",
      "Epoch:  73 Train Loss: 0.00801334 Val Loss: 0.00725263\n",
      "Epoch:  74 Train Loss: 0.00931202 Val Loss: 0.00767504\n",
      "Epoch:  75 Train Loss: 0.00911220 Val Loss: 0.00715408\n",
      "Epoch:  76 Train Loss: 0.00797494 Val Loss: 0.00730063\n",
      "Epoch:  77 Train Loss: 0.00671752 Val Loss: 0.00758095\n",
      "Epoch:  78 Train Loss: 0.00691258 Val Loss: 0.00776922\n",
      "Epoch:  79 Train Loss: 0.00704621 Val Loss: 0.00967657\n",
      "Epoch:  80 Train Loss: 0.00794402 Val Loss: 0.00658547\n",
      "Epoch:  81 Train Loss: 0.00778573 Val Loss: 0.01214526\n",
      "Epoch:  82 Train Loss: 0.01174458 Val Loss: 0.00676421\n",
      "Epoch:  83 Train Loss: 0.00704562 Val Loss: 0.00710168\n",
      "Epoch:  84 Train Loss: 0.00696260 Val Loss: 0.01003579\n",
      "Epoch:  85 Train Loss: 0.00766068 Val Loss: 0.00678245\n",
      "Epoch:  86 Train Loss: 0.00775434 Val Loss: 0.00662390\n",
      "Epoch:  87 Train Loss: 0.00956290 Val Loss: 0.01053877\n",
      "Epoch:  88 Train Loss: 0.01193321 Val Loss: 0.01240686\n",
      "Epoch:  89 Train Loss: 0.01091520 Val Loss: 0.01053501\n",
      "Epoch:  90 Train Loss: 0.00964832 Val Loss: 0.00877986\n",
      "Epoch:  91 Train Loss: 0.00983201 Val Loss: 0.00739049\n",
      "Epoch:  92 Train Loss: 0.00868481 Val Loss: 0.00736819\n",
      "Epoch:  93 Train Loss: 0.00729762 Val Loss: 0.00964008\n",
      "Epoch:  94 Train Loss: 0.00825045 Val Loss: 0.00878350\n",
      "Epoch:  95 Train Loss: 0.00679402 Val Loss: 0.00673117\n",
      "Epoch:  96 Train Loss: 0.00776260 Val Loss: 0.00735387\n",
      "Epoch:  97 Train Loss: 0.00682979 Val Loss: 0.00919382\n",
      "Epoch:  98 Train Loss: 0.00717325 Val Loss: 0.00709114\n",
      "Epoch:  99 Train Loss: 0.00590393 Val Loss: 0.00686038\n",
      "Epoch: 100 Train Loss: 0.00700947 Val Loss: 0.00677412\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.11574219 Val Loss: 0.09512218\n",
      "Epoch:   2 Train Loss: 0.08472955 Val Loss: 0.06317341\n",
      "Epoch:   3 Train Loss: 0.06871621 Val Loss: 0.04188715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youweicheng/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   4 Train Loss: 0.05144557 Val Loss: 0.03333995\n",
      "Epoch:   5 Train Loss: 0.05046711 Val Loss: 0.03174908\n",
      "Epoch:   6 Train Loss: 0.05391156 Val Loss: 0.03080651\n",
      "Epoch:   7 Train Loss: 0.04428026 Val Loss: 0.02967446\n",
      "Epoch:   8 Train Loss: 0.04779601 Val Loss: 0.02779605\n",
      "Epoch:   9 Train Loss: 0.04027848 Val Loss: 0.02484467\n",
      "Epoch:  10 Train Loss: 0.03548531 Val Loss: 0.02214553\n",
      "Epoch:  11 Train Loss: 0.03059498 Val Loss: 0.01882024\n",
      "Epoch:  12 Train Loss: 0.02539674 Val Loss: 0.01504025\n",
      "Epoch:  13 Train Loss: 0.01895746 Val Loss: 0.01047788\n",
      "Epoch:  14 Train Loss: 0.01075947 Val Loss: 0.00926351\n",
      "Epoch:  15 Train Loss: 0.00894489 Val Loss: 0.01032502\n",
      "Epoch:  16 Train Loss: 0.01169965 Val Loss: 0.00866961\n",
      "Epoch:  17 Train Loss: 0.00969311 Val Loss: 0.00829514\n",
      "Epoch:  18 Train Loss: 0.01280334 Val Loss: 0.00984709\n",
      "Epoch:  19 Train Loss: 0.01081855 Val Loss: 0.00860609\n",
      "Epoch:  20 Train Loss: 0.00897076 Val Loss: 0.00811641\n",
      "Epoch:  21 Train Loss: 0.00884531 Val Loss: 0.00842992\n",
      "Epoch:  22 Train Loss: 0.01005438 Val Loss: 0.00773256\n",
      "Epoch:  23 Train Loss: 0.00866696 Val Loss: 0.00763744\n",
      "Epoch:  24 Train Loss: 0.00930655 Val Loss: 0.00876227\n",
      "Epoch:  25 Train Loss: 0.00915639 Val Loss: 0.00754762\n",
      "Epoch:  26 Train Loss: 0.00851171 Val Loss: 0.00889657\n",
      "Epoch:  27 Train Loss: 0.01204750 Val Loss: 0.00753985\n",
      "Epoch:  28 Train Loss: 0.00899354 Val Loss: 0.00786973\n",
      "Epoch:  29 Train Loss: 0.01035481 Val Loss: 0.00851521\n",
      "Epoch:  30 Train Loss: 0.00741072 Val Loss: 0.00738171\n",
      "Epoch:  31 Train Loss: 0.00914067 Val Loss: 0.00800515\n",
      "Epoch:  32 Train Loss: 0.01017589 Val Loss: 0.00735793\n",
      "Epoch:  33 Train Loss: 0.01116204 Val Loss: 0.00731070\n",
      "Epoch:  34 Train Loss: 0.00880367 Val Loss: 0.00711936\n",
      "Epoch:  35 Train Loss: 0.00805979 Val Loss: 0.00753702\n",
      "Epoch:  36 Train Loss: 0.00792758 Val Loss: 0.00784591\n",
      "Epoch:  37 Train Loss: 0.01019817 Val Loss: 0.00721165\n",
      "Epoch:  38 Train Loss: 0.00991897 Val Loss: 0.00723662\n",
      "Epoch:  39 Train Loss: 0.00903217 Val Loss: 0.00868125\n",
      "Epoch:  40 Train Loss: 0.00876791 Val Loss: 0.00712881\n",
      "Epoch:  41 Train Loss: 0.00872238 Val Loss: 0.00788374\n",
      "Epoch:  42 Train Loss: 0.00797936 Val Loss: 0.00745480\n",
      "Epoch:  43 Train Loss: 0.00966520 Val Loss: 0.00718856\n",
      "Epoch:  44 Train Loss: 0.00823827 Val Loss: 0.00732476\n",
      "Epoch:  45 Train Loss: 0.00901506 Val Loss: 0.00716529\n",
      "Epoch:  46 Train Loss: 0.00930461 Val Loss: 0.00774627\n",
      "Epoch:  47 Train Loss: 0.00898105 Val Loss: 0.00704883\n",
      "Epoch:  48 Train Loss: 0.00804832 Val Loss: 0.00787062\n",
      "Epoch:  49 Train Loss: 0.00857088 Val Loss: 0.00697221\n",
      "Epoch:  50 Train Loss: 0.00872356 Val Loss: 0.00714042\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.20533142 Val Loss: 0.18284159\n",
      "Epoch:   2 Train Loss: 0.14662918 Val Loss: 0.12909682\n",
      "Epoch:   3 Train Loss: 0.11139332 Val Loss: 0.07953855\n",
      "Epoch:   4 Train Loss: 0.06401808 Val Loss: 0.03790166\n",
      "Epoch:   5 Train Loss: 0.04402294 Val Loss: 0.02671307\n",
      "Epoch:   6 Train Loss: 0.04813473 Val Loss: 0.02512250\n",
      "Epoch:   7 Train Loss: 0.04136098 Val Loss: 0.02453905\n",
      "Epoch:   8 Train Loss: 0.03280648 Val Loss: 0.02497193\n",
      "Epoch:   9 Train Loss: 0.03143082 Val Loss: 0.02081828\n",
      "Epoch:  10 Train Loss: 0.03055378 Val Loss: 0.01809336\n",
      "Epoch:  11 Train Loss: 0.02622409 Val Loss: 0.01633625\n",
      "Epoch:  12 Train Loss: 0.01996530 Val Loss: 0.01473228\n",
      "Epoch:  13 Train Loss: 0.01938362 Val Loss: 0.01177551\n",
      "Epoch:  14 Train Loss: 0.01461627 Val Loss: 0.00938917\n",
      "Epoch:  15 Train Loss: 0.01178570 Val Loss: 0.01087173\n",
      "Epoch:  16 Train Loss: 0.01203812 Val Loss: 0.00859684\n",
      "Epoch:  17 Train Loss: 0.00963858 Val Loss: 0.00964394\n",
      "Epoch:  18 Train Loss: 0.01144475 Val Loss: 0.00843003\n",
      "Epoch:  19 Train Loss: 0.01100451 Val Loss: 0.00841322\n",
      "Epoch:  20 Train Loss: 0.00999338 Val Loss: 0.00765192\n",
      "Epoch:  21 Train Loss: 0.00985035 Val Loss: 0.01011901\n",
      "Epoch:  22 Train Loss: 0.01018372 Val Loss: 0.00777634\n",
      "Epoch:  23 Train Loss: 0.00961412 Val Loss: 0.00776161\n",
      "Epoch:  24 Train Loss: 0.01082555 Val Loss: 0.01111040\n",
      "Epoch:  25 Train Loss: 0.01090153 Val Loss: 0.00748238\n",
      "Epoch:  26 Train Loss: 0.01009004 Val Loss: 0.00832520\n",
      "Epoch:  27 Train Loss: 0.00797399 Val Loss: 0.00770694\n",
      "Epoch:  28 Train Loss: 0.00823370 Val Loss: 0.00807079\n",
      "Epoch:  29 Train Loss: 0.00941128 Val Loss: 0.00793297\n",
      "Epoch:  30 Train Loss: 0.01273006 Val Loss: 0.00847414\n",
      "Epoch:  31 Train Loss: 0.01025608 Val Loss: 0.00839273\n",
      "Epoch:  32 Train Loss: 0.00825669 Val Loss: 0.00758109\n",
      "Epoch:  33 Train Loss: 0.00888152 Val Loss: 0.00932105\n",
      "Epoch:  34 Train Loss: 0.00956700 Val Loss: 0.00735852\n",
      "Epoch:  35 Train Loss: 0.00868472 Val Loss: 0.00765080\n",
      "Epoch:  36 Train Loss: 0.00947339 Val Loss: 0.00844628\n",
      "Epoch:  37 Train Loss: 0.00771921 Val Loss: 0.00785689\n",
      "Epoch:  38 Train Loss: 0.00913485 Val Loss: 0.00732821\n",
      "Epoch:  39 Train Loss: 0.00857506 Val Loss: 0.00813030\n",
      "Epoch:  40 Train Loss: 0.00941492 Val Loss: 0.00730853\n",
      "Epoch:  41 Train Loss: 0.00740050 Val Loss: 0.00808469\n",
      "Epoch:  42 Train Loss: 0.00828794 Val Loss: 0.00726680\n",
      "Epoch:  43 Train Loss: 0.01063343 Val Loss: 0.00757471\n",
      "Epoch:  44 Train Loss: 0.00776639 Val Loss: 0.00746777\n",
      "Epoch:  45 Train Loss: 0.00848176 Val Loss: 0.00777555\n",
      "Epoch:  46 Train Loss: 0.00940869 Val Loss: 0.00720289\n",
      "Epoch:  47 Train Loss: 0.00963841 Val Loss: 0.00872708\n",
      "Epoch:  48 Train Loss: 0.00845291 Val Loss: 0.00723710\n",
      "Epoch:  49 Train Loss: 0.00889977 Val Loss: 0.00742159\n",
      "Epoch:  50 Train Loss: 0.00930355 Val Loss: 0.00767682\n",
      "Epoch:  51 Train Loss: 0.00814527 Val Loss: 0.00721652\n",
      "Epoch:  52 Train Loss: 0.00952051 Val Loss: 0.00888584\n",
      "Epoch:  53 Train Loss: 0.00965472 Val Loss: 0.00712217\n",
      "Epoch:  54 Train Loss: 0.00787068 Val Loss: 0.00747294\n",
      "Epoch:  55 Train Loss: 0.00835030 Val Loss: 0.00709876\n",
      "Epoch:  56 Train Loss: 0.00862635 Val Loss: 0.00885360\n",
      "Epoch:  57 Train Loss: 0.00981953 Val Loss: 0.00723113\n",
      "Epoch:  58 Train Loss: 0.00990561 Val Loss: 0.00735798\n",
      "Epoch:  59 Train Loss: 0.00855960 Val Loss: 0.00712248\n",
      "Epoch:  60 Train Loss: 0.00704354 Val Loss: 0.00724070\n",
      "Epoch:  61 Train Loss: 0.00839600 Val Loss: 0.00703594\n",
      "Epoch:  62 Train Loss: 0.00700205 Val Loss: 0.00720856\n",
      "Epoch:  63 Train Loss: 0.00886160 Val Loss: 0.00735943\n",
      "Epoch:  64 Train Loss: 0.00922409 Val Loss: 0.00707527\n",
      "Epoch:  65 Train Loss: 0.00809732 Val Loss: 0.00701284\n",
      "Epoch:  66 Train Loss: 0.00868643 Val Loss: 0.00713166\n",
      "Epoch:  67 Train Loss: 0.01014361 Val Loss: 0.00732203\n",
      "Epoch:  68 Train Loss: 0.00810991 Val Loss: 0.00749092\n",
      "Epoch:  69 Train Loss: 0.00900247 Val Loss: 0.00701694\n",
      "Epoch:  70 Train Loss: 0.00800764 Val Loss: 0.00751622\n",
      "Epoch:  71 Train Loss: 0.00937431 Val Loss: 0.00722430\n",
      "Epoch:  72 Train Loss: 0.00794617 Val Loss: 0.00700437\n",
      "Epoch:  73 Train Loss: 0.00837339 Val Loss: 0.00725203\n",
      "Epoch:  74 Train Loss: 0.00916153 Val Loss: 0.00699893\n",
      "Epoch:  75 Train Loss: 0.00833675 Val Loss: 0.00736243\n",
      "Epoch:  76 Train Loss: 0.00810285 Val Loss: 0.00763615\n",
      "Epoch:  77 Train Loss: 0.00830050 Val Loss: 0.00705051\n",
      "Epoch:  78 Train Loss: 0.00830187 Val Loss: 0.00701866\n",
      "Epoch:  79 Train Loss: 0.00791302 Val Loss: 0.00701499\n",
      "Epoch:  80 Train Loss: 0.00888739 Val Loss: 0.00746689\n",
      "Epoch:  81 Train Loss: 0.00823728 Val Loss: 0.00729295\n",
      "Epoch:  82 Train Loss: 0.00987276 Val Loss: 0.00705544\n",
      "Epoch:  83 Train Loss: 0.00826716 Val Loss: 0.00695448\n",
      "Epoch:  84 Train Loss: 0.00784021 Val Loss: 0.00759139\n",
      "Epoch:  85 Train Loss: 0.00962779 Val Loss: 0.00694293\n",
      "Epoch:  86 Train Loss: 0.00829182 Val Loss: 0.00777268\n",
      "Epoch:  87 Train Loss: 0.00812395 Val Loss: 0.00783784\n",
      "Epoch:  88 Train Loss: 0.00905655 Val Loss: 0.00725860\n",
      "Epoch:  89 Train Loss: 0.00873461 Val Loss: 0.00698612\n",
      "Epoch:  90 Train Loss: 0.00842762 Val Loss: 0.00693973\n",
      "Epoch:  91 Train Loss: 0.00788151 Val Loss: 0.00698781\n",
      "Epoch:  92 Train Loss: 0.00680196 Val Loss: 0.00701035\n",
      "Epoch:  93 Train Loss: 0.00780925 Val Loss: 0.00688767\n",
      "Epoch:  94 Train Loss: 0.00895267 Val Loss: 0.00729094\n",
      "Epoch:  95 Train Loss: 0.00907647 Val Loss: 0.00710335\n",
      "Epoch:  96 Train Loss: 0.00830876 Val Loss: 0.00712104\n",
      "Epoch:  97 Train Loss: 0.00815219 Val Loss: 0.00694620\n",
      "Epoch:  98 Train Loss: 0.00727190 Val Loss: 0.00725704\n",
      "Epoch:  99 Train Loss: 0.00829423 Val Loss: 0.00684845\n",
      "Epoch: 100 Train Loss: 0.00804629 Val Loss: 0.00735246\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.09932156 Val Loss: 0.03382184\n",
      "Epoch:   2 Train Loss: 0.04592993 Val Loss: 0.03814710\n",
      "Epoch:   3 Train Loss: 0.03581185 Val Loss: 0.02140713\n",
      "Epoch:   4 Train Loss: 0.02795140 Val Loss: 0.01596896\n",
      "Epoch:   5 Train Loss: 0.01576041 Val Loss: 0.01087018\n",
      "Epoch:   6 Train Loss: 0.01130444 Val Loss: 0.00984992\n",
      "Epoch:   7 Train Loss: 0.01378906 Val Loss: 0.01085356\n",
      "Epoch:   8 Train Loss: 0.01295138 Val Loss: 0.01014116\n",
      "Epoch:   9 Train Loss: 0.01052038 Val Loss: 0.00951849\n",
      "Epoch:  10 Train Loss: 0.00924341 Val Loss: 0.00934603\n",
      "Epoch:  11 Train Loss: 0.01176328 Val Loss: 0.01161107\n",
      "Epoch:  12 Train Loss: 0.01110780 Val Loss: 0.01008037\n",
      "Epoch:  13 Train Loss: 0.01175516 Val Loss: 0.01091277\n",
      "Epoch:  14 Train Loss: 0.00950262 Val Loss: 0.01208617\n",
      "Epoch:  15 Train Loss: 0.01266066 Val Loss: 0.01074641\n",
      "Epoch:  16 Train Loss: 0.00970592 Val Loss: 0.00880529\n",
      "Epoch:  17 Train Loss: 0.01043126 Val Loss: 0.00851445\n",
      "Epoch:  18 Train Loss: 0.00922837 Val Loss: 0.00838254\n",
      "Epoch:  19 Train Loss: 0.00927380 Val Loss: 0.00843346\n",
      "Epoch:  20 Train Loss: 0.01165664 Val Loss: 0.01051227\n",
      "Epoch:  21 Train Loss: 0.01073522 Val Loss: 0.00833891\n",
      "Epoch:  22 Train Loss: 0.00999554 Val Loss: 0.00830227\n",
      "Epoch:  23 Train Loss: 0.00881711 Val Loss: 0.00843564\n",
      "Epoch:  24 Train Loss: 0.00994026 Val Loss: 0.00807487\n",
      "Epoch:  25 Train Loss: 0.00867456 Val Loss: 0.00845702\n",
      "Epoch:  26 Train Loss: 0.00785721 Val Loss: 0.00908989\n",
      "Epoch:  27 Train Loss: 0.00991793 Val Loss: 0.00955580\n",
      "Epoch:  28 Train Loss: 0.00988644 Val Loss: 0.00807591\n",
      "Epoch:  29 Train Loss: 0.00985885 Val Loss: 0.00789248\n",
      "Epoch:  30 Train Loss: 0.00961680 Val Loss: 0.00796640\n",
      "Epoch:  31 Train Loss: 0.00774035 Val Loss: 0.00824363\n",
      "Epoch:  32 Train Loss: 0.01002406 Val Loss: 0.01051794\n",
      "Epoch:  33 Train Loss: 0.01276032 Val Loss: 0.00805036\n",
      "Epoch:  34 Train Loss: 0.00945527 Val Loss: 0.00878556\n",
      "Epoch:  35 Train Loss: 0.00824895 Val Loss: 0.00820787\n",
      "Epoch:  36 Train Loss: 0.00838035 Val Loss: 0.00793206\n",
      "Epoch:  37 Train Loss: 0.00688323 Val Loss: 0.00823102\n",
      "Epoch:  38 Train Loss: 0.00884891 Val Loss: 0.00822287\n",
      "Epoch:  39 Train Loss: 0.00840895 Val Loss: 0.00776023\n",
      "Epoch:  40 Train Loss: 0.00912549 Val Loss: 0.00923672\n",
      "Epoch:  41 Train Loss: 0.01090932 Val Loss: 0.00888375\n",
      "Epoch:  42 Train Loss: 0.00935196 Val Loss: 0.00806790\n",
      "Epoch:  43 Train Loss: 0.00963703 Val Loss: 0.00777422\n",
      "Epoch:  44 Train Loss: 0.01188478 Val Loss: 0.00805941\n",
      "Epoch:  45 Train Loss: 0.00878920 Val Loss: 0.00784205\n",
      "Epoch:  46 Train Loss: 0.00842441 Val Loss: 0.00796058\n",
      "Epoch:  47 Train Loss: 0.00866639 Val Loss: 0.00833693\n",
      "Epoch:  48 Train Loss: 0.00841641 Val Loss: 0.00784806\n",
      "Epoch:  49 Train Loss: 0.00810266 Val Loss: 0.00744574\n",
      "Epoch:  50 Train Loss: 0.00787620 Val Loss: 0.00737783\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.06435261 Val Loss: 0.02879952\n",
      "Epoch:   2 Train Loss: 0.04006296 Val Loss: 0.01982839\n",
      "Epoch:   3 Train Loss: 0.02286544 Val Loss: 0.01290163\n",
      "Epoch:   4 Train Loss: 0.01481150 Val Loss: 0.01351434\n",
      "Epoch:   5 Train Loss: 0.01202810 Val Loss: 0.00961853\n",
      "Epoch:   6 Train Loss: 0.01157741 Val Loss: 0.01075635\n",
      "Epoch:   7 Train Loss: 0.01110247 Val Loss: 0.00888681\n",
      "Epoch:   8 Train Loss: 0.01000279 Val Loss: 0.00982879\n",
      "Epoch:   9 Train Loss: 0.01236341 Val Loss: 0.00957834\n",
      "Epoch:  10 Train Loss: 0.00969971 Val Loss: 0.00868059\n",
      "Epoch:  11 Train Loss: 0.00947556 Val Loss: 0.00857324\n",
      "Epoch:  12 Train Loss: 0.00931964 Val Loss: 0.00980120\n",
      "Epoch:  13 Train Loss: 0.00907156 Val Loss: 0.00833810\n",
      "Epoch:  14 Train Loss: 0.00899014 Val Loss: 0.00962321\n",
      "Epoch:  15 Train Loss: 0.00983423 Val Loss: 0.00804526\n",
      "Epoch:  16 Train Loss: 0.00890738 Val Loss: 0.01149727\n",
      "Epoch:  17 Train Loss: 0.01039285 Val Loss: 0.00797122\n",
      "Epoch:  18 Train Loss: 0.00871687 Val Loss: 0.00934464\n",
      "Epoch:  19 Train Loss: 0.00902141 Val Loss: 0.00800926\n",
      "Epoch:  20 Train Loss: 0.00822894 Val Loss: 0.00819763\n",
      "Epoch:  21 Train Loss: 0.00925637 Val Loss: 0.00834858\n",
      "Epoch:  22 Train Loss: 0.00799157 Val Loss: 0.00815828\n",
      "Epoch:  23 Train Loss: 0.01043762 Val Loss: 0.00801285\n",
      "Epoch:  24 Train Loss: 0.00759463 Val Loss: 0.00785560\n",
      "Epoch:  25 Train Loss: 0.00900790 Val Loss: 0.00848906\n",
      "Epoch:  26 Train Loss: 0.00914622 Val Loss: 0.01026076\n",
      "Epoch:  27 Train Loss: 0.00795180 Val Loss: 0.00828068\n",
      "Epoch:  28 Train Loss: 0.00858975 Val Loss: 0.00765184\n",
      "Epoch:  29 Train Loss: 0.00820594 Val Loss: 0.00888998\n",
      "Epoch:  30 Train Loss: 0.00753617 Val Loss: 0.00948749\n",
      "Epoch:  31 Train Loss: 0.00820761 Val Loss: 0.00758325\n",
      "Epoch:  32 Train Loss: 0.00813086 Val Loss: 0.00760497\n",
      "Epoch:  33 Train Loss: 0.00848772 Val Loss: 0.01087432\n",
      "Epoch:  34 Train Loss: 0.01542821 Val Loss: 0.01207472\n",
      "Epoch:  35 Train Loss: 0.00886315 Val Loss: 0.00802000\n",
      "Epoch:  36 Train Loss: 0.00877286 Val Loss: 0.00972593\n",
      "Epoch:  37 Train Loss: 0.00990557 Val Loss: 0.00900692\n",
      "Epoch:  38 Train Loss: 0.00933993 Val Loss: 0.00907015\n",
      "Epoch:  39 Train Loss: 0.00905706 Val Loss: 0.00798507\n",
      "Epoch:  40 Train Loss: 0.00891658 Val Loss: 0.00780365\n",
      "Epoch:  41 Train Loss: 0.00755886 Val Loss: 0.00786498\n",
      "Epoch:  42 Train Loss: 0.00737291 Val Loss: 0.00781443\n",
      "Epoch:  43 Train Loss: 0.00822049 Val Loss: 0.00763303\n",
      "Epoch:  44 Train Loss: 0.00789366 Val Loss: 0.00752392\n",
      "Epoch:  45 Train Loss: 0.00726908 Val Loss: 0.00749522\n",
      "Epoch:  46 Train Loss: 0.00726659 Val Loss: 0.00736863\n",
      "Epoch:  47 Train Loss: 0.00820812 Val Loss: 0.00756776\n",
      "Epoch:  48 Train Loss: 0.00843861 Val Loss: 0.00777438\n",
      "Epoch:  49 Train Loss: 0.00881799 Val Loss: 0.01011887\n",
      "Epoch:  50 Train Loss: 0.00983053 Val Loss: 0.01333358\n",
      "Epoch:  51 Train Loss: 0.00896930 Val Loss: 0.00768267\n",
      "Epoch:  52 Train Loss: 0.00700144 Val Loss: 0.00782243\n",
      "Epoch:  53 Train Loss: 0.00672568 Val Loss: 0.00736721\n",
      "Epoch:  54 Train Loss: 0.00943819 Val Loss: 0.00741312\n",
      "Epoch:  55 Train Loss: 0.00792881 Val Loss: 0.00776955\n",
      "Epoch:  56 Train Loss: 0.00769809 Val Loss: 0.00900655\n",
      "Epoch:  57 Train Loss: 0.00796837 Val Loss: 0.00701453\n",
      "Epoch:  58 Train Loss: 0.00914656 Val Loss: 0.00699023\n",
      "Epoch:  59 Train Loss: 0.00798608 Val Loss: 0.00709680\n",
      "Epoch:  60 Train Loss: 0.00880843 Val Loss: 0.00987875\n",
      "Epoch:  61 Train Loss: 0.00787549 Val Loss: 0.00745297\n",
      "Epoch:  62 Train Loss: 0.00697541 Val Loss: 0.00677116\n",
      "Epoch:  63 Train Loss: 0.00819272 Val Loss: 0.00670334\n",
      "Epoch:  64 Train Loss: 0.00918500 Val Loss: 0.01484056\n",
      "Epoch:  65 Train Loss: 0.01298524 Val Loss: 0.00883618\n",
      "Epoch:  66 Train Loss: 0.00841740 Val Loss: 0.00822474\n",
      "Epoch:  67 Train Loss: 0.00811248 Val Loss: 0.00777836\n",
      "Epoch:  68 Train Loss: 0.00820083 Val Loss: 0.00735571\n",
      "Epoch:  69 Train Loss: 0.00706547 Val Loss: 0.00724215\n",
      "Epoch:  70 Train Loss: 0.00747772 Val Loss: 0.00722696\n",
      "Epoch:  71 Train Loss: 0.00724236 Val Loss: 0.00753408\n",
      "Epoch:  72 Train Loss: 0.00753250 Val Loss: 0.00701915\n",
      "Epoch:  73 Train Loss: 0.00734991 Val Loss: 0.00695169\n",
      "Epoch:  74 Train Loss: 0.00667261 Val Loss: 0.00709521\n",
      "Epoch:  75 Train Loss: 0.00776065 Val Loss: 0.00698382\n",
      "Epoch:  76 Train Loss: 0.00662302 Val Loss: 0.00681822\n",
      "Epoch:  77 Train Loss: 0.00679473 Val Loss: 0.00673716\n",
      "Epoch:  78 Train Loss: 0.00692533 Val Loss: 0.00838980\n",
      "Epoch:  79 Train Loss: 0.00787984 Val Loss: 0.00726791\n",
      "Epoch:  80 Train Loss: 0.00713680 Val Loss: 0.00721414\n",
      "Epoch:  81 Train Loss: 0.00787399 Val Loss: 0.00668772\n",
      "Epoch:  82 Train Loss: 0.00686731 Val Loss: 0.00652220\n",
      "Epoch:  83 Train Loss: 0.00822896 Val Loss: 0.00849756\n",
      "Epoch:  84 Train Loss: 0.00825796 Val Loss: 0.00685175\n",
      "Epoch:  85 Train Loss: 0.00879682 Val Loss: 0.00782524\n",
      "Epoch:  86 Train Loss: 0.00860710 Val Loss: 0.00629404\n",
      "Epoch:  87 Train Loss: 0.00686469 Val Loss: 0.00649367\n",
      "Epoch:  88 Train Loss: 0.00671826 Val Loss: 0.00780554\n",
      "Epoch:  89 Train Loss: 0.00676364 Val Loss: 0.01239628\n",
      "Epoch:  90 Train Loss: 0.00817849 Val Loss: 0.01177860\n",
      "Epoch:  91 Train Loss: 0.00928849 Val Loss: 0.00696166\n",
      "Epoch:  92 Train Loss: 0.00756277 Val Loss: 0.00747941\n",
      "Epoch:  93 Train Loss: 0.00696289 Val Loss: 0.00672904\n",
      "Epoch:  94 Train Loss: 0.00642700 Val Loss: 0.00646808\n",
      "Epoch:  95 Train Loss: 0.00704992 Val Loss: 0.00710129\n",
      "Epoch:  96 Train Loss: 0.00647242 Val Loss: 0.00654706\n",
      "Epoch:  97 Train Loss: 0.00570300 Val Loss: 0.00626692\n",
      "Epoch:  98 Train Loss: 0.00583276 Val Loss: 0.00607551\n",
      "Epoch:  99 Train Loss: 0.00520277 Val Loss: 0.00964827\n",
      "Epoch: 100 Train Loss: 0.00720609 Val Loss: 0.00975771\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youweicheng/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 Train Loss: 0.14298814 Val Loss: 0.12681666\n",
      "Epoch:   2 Train Loss: 0.10623187 Val Loss: 0.08582983\n",
      "Epoch:   3 Train Loss: 0.08471750 Val Loss: 0.05084714\n",
      "Epoch:   4 Train Loss: 0.05274312 Val Loss: 0.02977285\n",
      "Epoch:   5 Train Loss: 0.05094830 Val Loss: 0.02765530\n",
      "Epoch:   6 Train Loss: 0.04556833 Val Loss: 0.02532146\n",
      "Epoch:   7 Train Loss: 0.04570041 Val Loss: 0.02455909\n",
      "Epoch:   8 Train Loss: 0.03653668 Val Loss: 0.02130357\n",
      "Epoch:   9 Train Loss: 0.02975207 Val Loss: 0.01833079\n",
      "Epoch:  10 Train Loss: 0.02779284 Val Loss: 0.01562217\n",
      "Epoch:  11 Train Loss: 0.02360914 Val Loss: 0.01281761\n",
      "Epoch:  12 Train Loss: 0.01647896 Val Loss: 0.00910890\n",
      "Epoch:  13 Train Loss: 0.01385019 Val Loss: 0.00932774\n",
      "Epoch:  14 Train Loss: 0.01160356 Val Loss: 0.00922962\n",
      "Epoch:  15 Train Loss: 0.01255808 Val Loss: 0.00873395\n",
      "Epoch:  16 Train Loss: 0.01253549 Val Loss: 0.01001057\n",
      "Epoch:  17 Train Loss: 0.01238219 Val Loss: 0.00884910\n",
      "Epoch:  18 Train Loss: 0.01044003 Val Loss: 0.00953605\n",
      "Epoch:  19 Train Loss: 0.01086701 Val Loss: 0.00808836\n",
      "Epoch:  20 Train Loss: 0.01073412 Val Loss: 0.00935769\n",
      "Epoch:  21 Train Loss: 0.01048854 Val Loss: 0.00752762\n",
      "Epoch:  22 Train Loss: 0.01196507 Val Loss: 0.01031439\n",
      "Epoch:  23 Train Loss: 0.01178731 Val Loss: 0.00752184\n",
      "Epoch:  24 Train Loss: 0.00925483 Val Loss: 0.00837763\n",
      "Epoch:  25 Train Loss: 0.00989326 Val Loss: 0.00756927\n",
      "Epoch:  26 Train Loss: 0.00963073 Val Loss: 0.00748951\n",
      "Epoch:  27 Train Loss: 0.01049845 Val Loss: 0.00843982\n",
      "Epoch:  28 Train Loss: 0.00986753 Val Loss: 0.00733212\n",
      "Epoch:  29 Train Loss: 0.00907392 Val Loss: 0.00723869\n",
      "Epoch:  30 Train Loss: 0.01030739 Val Loss: 0.00980319\n",
      "Epoch:  31 Train Loss: 0.01060220 Val Loss: 0.00726487\n",
      "Epoch:  32 Train Loss: 0.00944136 Val Loss: 0.00746260\n",
      "Epoch:  33 Train Loss: 0.00925147 Val Loss: 0.00753133\n",
      "Epoch:  34 Train Loss: 0.01076907 Val Loss: 0.00762657\n",
      "Epoch:  35 Train Loss: 0.01081009 Val Loss: 0.00757409\n",
      "Epoch:  36 Train Loss: 0.00961507 Val Loss: 0.00719488\n",
      "Epoch:  37 Train Loss: 0.00999571 Val Loss: 0.00737049\n",
      "Epoch:  38 Train Loss: 0.00986093 Val Loss: 0.00792883\n",
      "Epoch:  39 Train Loss: 0.00972064 Val Loss: 0.00855417\n",
      "Epoch:  40 Train Loss: 0.01118442 Val Loss: 0.00712049\n",
      "Epoch:  41 Train Loss: 0.01087805 Val Loss: 0.00731418\n",
      "Epoch:  42 Train Loss: 0.00724461 Val Loss: 0.00749540\n",
      "Epoch:  43 Train Loss: 0.00824696 Val Loss: 0.00721497\n",
      "Epoch:  44 Train Loss: 0.00875044 Val Loss: 0.00744803\n",
      "Epoch:  45 Train Loss: 0.00805591 Val Loss: 0.00707034\n",
      "Epoch:  46 Train Loss: 0.00900636 Val Loss: 0.00763272\n",
      "Epoch:  47 Train Loss: 0.01042361 Val Loss: 0.00771691\n",
      "Epoch:  48 Train Loss: 0.00872796 Val Loss: 0.00706792\n",
      "Epoch:  49 Train Loss: 0.00998590 Val Loss: 0.00737455\n",
      "Epoch:  50 Train Loss: 0.00866185 Val Loss: 0.00704706\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.25536754 Val Loss: 0.22223610\n",
      "Epoch:   2 Train Loss: 0.18772870 Val Loss: 0.15074824\n",
      "Epoch:   3 Train Loss: 0.11791001 Val Loss: 0.08152084\n",
      "Epoch:   4 Train Loss: 0.06921692 Val Loss: 0.03427995\n",
      "Epoch:   5 Train Loss: 0.05979368 Val Loss: 0.03374971\n",
      "Epoch:   6 Train Loss: 0.06262151 Val Loss: 0.02951737\n",
      "Epoch:   7 Train Loss: 0.04596388 Val Loss: 0.03076019\n",
      "Epoch:   8 Train Loss: 0.04224019 Val Loss: 0.02999847\n",
      "Epoch:   9 Train Loss: 0.04345886 Val Loss: 0.02667142\n",
      "Epoch:  10 Train Loss: 0.04460839 Val Loss: 0.02390722\n",
      "Epoch:  11 Train Loss: 0.03917604 Val Loss: 0.02193123\n",
      "Epoch:  12 Train Loss: 0.03900121 Val Loss: 0.01986920\n",
      "Epoch:  13 Train Loss: 0.03240257 Val Loss: 0.01837823\n",
      "Epoch:  14 Train Loss: 0.02720474 Val Loss: 0.01575853\n",
      "Epoch:  15 Train Loss: 0.02629485 Val Loss: 0.01296293\n",
      "Epoch:  16 Train Loss: 0.02092107 Val Loss: 0.01112337\n",
      "Epoch:  17 Train Loss: 0.01485020 Val Loss: 0.00918521\n",
      "Epoch:  18 Train Loss: 0.01599211 Val Loss: 0.00798656\n",
      "Epoch:  19 Train Loss: 0.01013170 Val Loss: 0.00801227\n",
      "Epoch:  20 Train Loss: 0.01292536 Val Loss: 0.01314584\n",
      "Epoch:  21 Train Loss: 0.01208541 Val Loss: 0.00769637\n",
      "Epoch:  22 Train Loss: 0.01009539 Val Loss: 0.00810218\n",
      "Epoch:  23 Train Loss: 0.01346029 Val Loss: 0.00800720\n",
      "Epoch:  24 Train Loss: 0.01165675 Val Loss: 0.00753273\n",
      "Epoch:  25 Train Loss: 0.01182859 Val Loss: 0.00779156\n",
      "Epoch:  26 Train Loss: 0.01044005 Val Loss: 0.00805764\n",
      "Epoch:  27 Train Loss: 0.01315069 Val Loss: 0.00749452\n",
      "Epoch:  28 Train Loss: 0.01079812 Val Loss: 0.00877145\n",
      "Epoch:  29 Train Loss: 0.01285560 Val Loss: 0.00762421\n",
      "Epoch:  30 Train Loss: 0.01226527 Val Loss: 0.00748485\n",
      "Epoch:  31 Train Loss: 0.01257047 Val Loss: 0.01096132\n",
      "Epoch:  32 Train Loss: 0.01296819 Val Loss: 0.00781491\n",
      "Epoch:  33 Train Loss: 0.01178022 Val Loss: 0.00846887\n",
      "Epoch:  34 Train Loss: 0.01162567 Val Loss: 0.00765766\n",
      "Epoch:  35 Train Loss: 0.01352764 Val Loss: 0.00882901\n",
      "Epoch:  36 Train Loss: 0.00990379 Val Loss: 0.00742624\n",
      "Epoch:  37 Train Loss: 0.01068695 Val Loss: 0.00931681\n",
      "Epoch:  38 Train Loss: 0.01041125 Val Loss: 0.00790896\n",
      "Epoch:  39 Train Loss: 0.01159809 Val Loss: 0.00776716\n",
      "Epoch:  40 Train Loss: 0.01338101 Val Loss: 0.00803817\n",
      "Epoch:  41 Train Loss: 0.01104093 Val Loss: 0.00760059\n",
      "Epoch:  42 Train Loss: 0.01115961 Val Loss: 0.00839586\n",
      "Epoch:  43 Train Loss: 0.01105883 Val Loss: 0.00785720\n",
      "Epoch:  44 Train Loss: 0.01187909 Val Loss: 0.00805163\n",
      "Epoch:  45 Train Loss: 0.01055470 Val Loss: 0.00733151\n",
      "Epoch:  46 Train Loss: 0.01063136 Val Loss: 0.00790908\n",
      "Epoch:  47 Train Loss: 0.00952787 Val Loss: 0.00778142\n",
      "Epoch:  48 Train Loss: 0.00922059 Val Loss: 0.00848041\n",
      "Epoch:  49 Train Loss: 0.00933909 Val Loss: 0.00729749\n",
      "Epoch:  50 Train Loss: 0.01060253 Val Loss: 0.00784912\n",
      "Epoch:  51 Train Loss: 0.01010612 Val Loss: 0.00774696\n",
      "Epoch:  52 Train Loss: 0.00788416 Val Loss: 0.00730947\n",
      "Epoch:  53 Train Loss: 0.01008742 Val Loss: 0.00851519\n",
      "Epoch:  54 Train Loss: 0.01325768 Val Loss: 0.00741766\n",
      "Epoch:  55 Train Loss: 0.00902213 Val Loss: 0.00726443\n",
      "Epoch:  56 Train Loss: 0.00806020 Val Loss: 0.00854683\n",
      "Epoch:  57 Train Loss: 0.01024956 Val Loss: 0.00736034\n",
      "Epoch:  58 Train Loss: 0.00979965 Val Loss: 0.00762428\n",
      "Epoch:  59 Train Loss: 0.01318237 Val Loss: 0.00790608\n",
      "Epoch:  60 Train Loss: 0.01106918 Val Loss: 0.00743221\n",
      "Epoch:  61 Train Loss: 0.01018615 Val Loss: 0.00772050\n",
      "Epoch:  62 Train Loss: 0.01252919 Val Loss: 0.00756702\n",
      "Epoch:  63 Train Loss: 0.00974319 Val Loss: 0.00729943\n",
      "Epoch:  64 Train Loss: 0.01142554 Val Loss: 0.00757308\n",
      "Epoch:  65 Train Loss: 0.00919238 Val Loss: 0.00725770\n",
      "Epoch:  66 Train Loss: 0.00972209 Val Loss: 0.00747792\n",
      "Epoch:  67 Train Loss: 0.01000676 Val Loss: 0.00740887\n",
      "Epoch:  68 Train Loss: 0.01102814 Val Loss: 0.00791618\n",
      "Epoch:  69 Train Loss: 0.00824759 Val Loss: 0.00739399\n",
      "Epoch:  70 Train Loss: 0.00864456 Val Loss: 0.00726362\n",
      "Epoch:  71 Train Loss: 0.01055393 Val Loss: 0.00791630\n",
      "Epoch:  72 Train Loss: 0.01060277 Val Loss: 0.00731661\n",
      "Epoch:  73 Train Loss: 0.01109546 Val Loss: 0.00792594\n",
      "Epoch:  74 Train Loss: 0.00945764 Val Loss: 0.00831894\n",
      "Epoch:  75 Train Loss: 0.01060942 Val Loss: 0.00734261\n",
      "Epoch:  76 Train Loss: 0.00833231 Val Loss: 0.00919215\n",
      "Epoch:  77 Train Loss: 0.01025102 Val Loss: 0.00723764\n",
      "Epoch:  78 Train Loss: 0.00973881 Val Loss: 0.00750986\n",
      "Epoch:  79 Train Loss: 0.01030203 Val Loss: 0.00787993\n",
      "Epoch:  80 Train Loss: 0.01040929 Val Loss: 0.00722014\n",
      "Epoch:  81 Train Loss: 0.00895435 Val Loss: 0.00732395\n",
      "Epoch:  82 Train Loss: 0.01077547 Val Loss: 0.00790438\n",
      "Epoch:  83 Train Loss: 0.01281142 Val Loss: 0.00737951\n",
      "Epoch:  84 Train Loss: 0.01142326 Val Loss: 0.00723698\n",
      "Epoch:  85 Train Loss: 0.01093333 Val Loss: 0.00878105\n",
      "Epoch:  86 Train Loss: 0.00858623 Val Loss: 0.00735078\n",
      "Epoch:  87 Train Loss: 0.00982556 Val Loss: 0.00805732\n",
      "Epoch:  88 Train Loss: 0.01012019 Val Loss: 0.00733023\n",
      "Epoch:  89 Train Loss: 0.00895817 Val Loss: 0.00726057\n",
      "Epoch:  90 Train Loss: 0.00879519 Val Loss: 0.00748813\n",
      "Epoch:  91 Train Loss: 0.00933928 Val Loss: 0.00719827\n",
      "Epoch:  92 Train Loss: 0.01170607 Val Loss: 0.00880588\n",
      "Epoch:  93 Train Loss: 0.00928656 Val Loss: 0.00740656\n",
      "Epoch:  94 Train Loss: 0.00900657 Val Loss: 0.00827536\n",
      "Epoch:  95 Train Loss: 0.01154569 Val Loss: 0.00769165\n",
      "Epoch:  96 Train Loss: 0.01023298 Val Loss: 0.00735535\n",
      "Epoch:  97 Train Loss: 0.01112607 Val Loss: 0.00745136\n",
      "Epoch:  98 Train Loss: 0.01044461 Val Loss: 0.00758797\n",
      "Epoch:  99 Train Loss: 0.00864279 Val Loss: 0.00736097\n",
      "Epoch: 100 Train Loss: 0.01257896 Val Loss: 0.00725227\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.14990303 Val Loss: 0.04075849\n",
      "Epoch:   2 Train Loss: 0.05019937 Val Loss: 0.03339394\n",
      "Epoch:   3 Train Loss: 0.04141504 Val Loss: 0.02167191\n",
      "Epoch:   4 Train Loss: 0.03047677 Val Loss: 0.01613933\n",
      "Epoch:   5 Train Loss: 0.01524446 Val Loss: 0.01438477\n",
      "Epoch:   6 Train Loss: 0.02294632 Val Loss: 0.00861621\n",
      "Epoch:   7 Train Loss: 0.01253673 Val Loss: 0.01047614\n",
      "Epoch:   8 Train Loss: 0.01467706 Val Loss: 0.00956076\n",
      "Epoch:   9 Train Loss: 0.01170458 Val Loss: 0.00964685\n",
      "Epoch:  10 Train Loss: 0.01425372 Val Loss: 0.00891534\n",
      "Epoch:  11 Train Loss: 0.00975717 Val Loss: 0.00965640\n",
      "Epoch:  12 Train Loss: 0.01242793 Val Loss: 0.00922225\n",
      "Epoch:  13 Train Loss: 0.01094673 Val Loss: 0.01190535\n",
      "Epoch:  14 Train Loss: 0.01342106 Val Loss: 0.00870096\n",
      "Epoch:  15 Train Loss: 0.01350526 Val Loss: 0.00846048\n",
      "Epoch:  16 Train Loss: 0.01417578 Val Loss: 0.01195030\n",
      "Epoch:  17 Train Loss: 0.01200139 Val Loss: 0.01071719\n",
      "Epoch:  18 Train Loss: 0.01563105 Val Loss: 0.01268718\n",
      "Epoch:  19 Train Loss: 0.01242480 Val Loss: 0.01134732\n",
      "Epoch:  20 Train Loss: 0.01075158 Val Loss: 0.00861574\n",
      "Epoch:  21 Train Loss: 0.01049990 Val Loss: 0.00842801\n",
      "Epoch:  22 Train Loss: 0.01252612 Val Loss: 0.00868620\n",
      "Epoch:  23 Train Loss: 0.01049872 Val Loss: 0.00810749\n",
      "Epoch:  24 Train Loss: 0.00966202 Val Loss: 0.00901006\n",
      "Epoch:  25 Train Loss: 0.01337156 Val Loss: 0.01189163\n",
      "Epoch:  26 Train Loss: 0.00952947 Val Loss: 0.00908691\n",
      "Epoch:  27 Train Loss: 0.01078166 Val Loss: 0.00880880\n",
      "Epoch:  28 Train Loss: 0.00801429 Val Loss: 0.00946825\n",
      "Epoch:  29 Train Loss: 0.00966391 Val Loss: 0.00875534\n",
      "Epoch:  30 Train Loss: 0.01238770 Val Loss: 0.00974964\n",
      "Epoch:  31 Train Loss: 0.00903977 Val Loss: 0.00798970\n",
      "Epoch:  32 Train Loss: 0.00883438 Val Loss: 0.00856587\n",
      "Epoch:  33 Train Loss: 0.01003967 Val Loss: 0.00821549\n",
      "Epoch:  34 Train Loss: 0.00746160 Val Loss: 0.00856631\n",
      "Epoch:  35 Train Loss: 0.00798532 Val Loss: 0.00797832\n",
      "Epoch:  36 Train Loss: 0.00825459 Val Loss: 0.00879203\n",
      "Epoch:  37 Train Loss: 0.00828932 Val Loss: 0.00848452\n",
      "Epoch:  38 Train Loss: 0.01087054 Val Loss: 0.00809731\n",
      "Epoch:  39 Train Loss: 0.00941638 Val Loss: 0.00816210\n",
      "Epoch:  40 Train Loss: 0.00876456 Val Loss: 0.00855361\n",
      "Epoch:  41 Train Loss: 0.00858936 Val Loss: 0.00826993\n",
      "Epoch:  42 Train Loss: 0.00923785 Val Loss: 0.00784128\n",
      "Epoch:  43 Train Loss: 0.00769355 Val Loss: 0.00893390\n",
      "Epoch:  44 Train Loss: 0.00820008 Val Loss: 0.00860943\n",
      "Epoch:  45 Train Loss: 0.00914013 Val Loss: 0.00797260\n",
      "Epoch:  46 Train Loss: 0.00939663 Val Loss: 0.01169584\n",
      "Epoch:  47 Train Loss: 0.01025697 Val Loss: 0.01024734\n",
      "Epoch:  48 Train Loss: 0.00831850 Val Loss: 0.00814813\n",
      "Epoch:  49 Train Loss: 0.00717238 Val Loss: 0.00753279\n",
      "Epoch:  50 Train Loss: 0.00758634 Val Loss: 0.00774415\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.12749221 Val Loss: 0.02632129\n",
      "Epoch:   2 Train Loss: 0.03974511 Val Loss: 0.03020809\n",
      "Epoch:   3 Train Loss: 0.03526586 Val Loss: 0.01783482\n",
      "Epoch:   4 Train Loss: 0.02371413 Val Loss: 0.01274711\n",
      "Epoch:   5 Train Loss: 0.01278078 Val Loss: 0.00935235\n",
      "Epoch:   6 Train Loss: 0.01728951 Val Loss: 0.01069390\n",
      "Epoch:   7 Train Loss: 0.01521162 Val Loss: 0.01144234\n",
      "Epoch:   8 Train Loss: 0.01408404 Val Loss: 0.00936336\n",
      "Epoch:   9 Train Loss: 0.01370437 Val Loss: 0.00907497\n",
      "Epoch:  10 Train Loss: 0.01142224 Val Loss: 0.01066852\n",
      "Epoch:  11 Train Loss: 0.01409112 Val Loss: 0.00899276\n",
      "Epoch:  12 Train Loss: 0.01031048 Val Loss: 0.00884648\n",
      "Epoch:  13 Train Loss: 0.01278854 Val Loss: 0.01130586\n",
      "Epoch:  14 Train Loss: 0.01177404 Val Loss: 0.00889698\n",
      "Epoch:  15 Train Loss: 0.00943870 Val Loss: 0.00899183\n",
      "Epoch:  16 Train Loss: 0.01098728 Val Loss: 0.01106711\n",
      "Epoch:  17 Train Loss: 0.01154639 Val Loss: 0.00922424\n",
      "Epoch:  18 Train Loss: 0.01274775 Val Loss: 0.00879852\n",
      "Epoch:  19 Train Loss: 0.00990961 Val Loss: 0.00848267\n",
      "Epoch:  20 Train Loss: 0.00848908 Val Loss: 0.00875519\n",
      "Epoch:  21 Train Loss: 0.01194096 Val Loss: 0.00999301\n",
      "Epoch:  22 Train Loss: 0.00953717 Val Loss: 0.00866827\n",
      "Epoch:  23 Train Loss: 0.01223637 Val Loss: 0.00849709\n",
      "Epoch:  24 Train Loss: 0.01086881 Val Loss: 0.00854737\n",
      "Epoch:  25 Train Loss: 0.00862082 Val Loss: 0.01019234\n",
      "Epoch:  26 Train Loss: 0.00907306 Val Loss: 0.01019356\n",
      "Epoch:  27 Train Loss: 0.01240040 Val Loss: 0.01045638\n",
      "Epoch:  28 Train Loss: 0.01210045 Val Loss: 0.00823345\n",
      "Epoch:  29 Train Loss: 0.01025652 Val Loss: 0.00935224\n",
      "Epoch:  30 Train Loss: 0.01260721 Val Loss: 0.00930487\n",
      "Epoch:  31 Train Loss: 0.00933841 Val Loss: 0.00798544\n",
      "Epoch:  32 Train Loss: 0.01050383 Val Loss: 0.00792963\n",
      "Epoch:  33 Train Loss: 0.01100650 Val Loss: 0.00794082\n",
      "Epoch:  34 Train Loss: 0.00881110 Val Loss: 0.00797232\n",
      "Epoch:  35 Train Loss: 0.00998117 Val Loss: 0.00886246\n",
      "Epoch:  36 Train Loss: 0.00992829 Val Loss: 0.00850370\n",
      "Epoch:  37 Train Loss: 0.00936313 Val Loss: 0.00810197\n",
      "Epoch:  38 Train Loss: 0.00910165 Val Loss: 0.00820539\n",
      "Epoch:  39 Train Loss: 0.00794194 Val Loss: 0.00832124\n",
      "Epoch:  40 Train Loss: 0.01142758 Val Loss: 0.00829263\n",
      "Epoch:  41 Train Loss: 0.00801055 Val Loss: 0.00807658\n",
      "Epoch:  42 Train Loss: 0.00848733 Val Loss: 0.00875373\n",
      "Epoch:  43 Train Loss: 0.00990376 Val Loss: 0.01012487\n",
      "Epoch:  44 Train Loss: 0.00981045 Val Loss: 0.00796849\n",
      "Epoch:  45 Train Loss: 0.00832867 Val Loss: 0.00780971\n",
      "Epoch:  46 Train Loss: 0.00885752 Val Loss: 0.00862139\n",
      "Epoch:  47 Train Loss: 0.00838855 Val Loss: 0.00775522\n",
      "Epoch:  48 Train Loss: 0.01130849 Val Loss: 0.00778638\n",
      "Epoch:  49 Train Loss: 0.01158132 Val Loss: 0.00847626\n",
      "Epoch:  50 Train Loss: 0.01009518 Val Loss: 0.00845938\n",
      "Epoch:  51 Train Loss: 0.00959402 Val Loss: 0.00983168\n",
      "Epoch:  52 Train Loss: 0.00975157 Val Loss: 0.01188618\n",
      "Epoch:  53 Train Loss: 0.01143124 Val Loss: 0.01007281\n",
      "Epoch:  54 Train Loss: 0.01039881 Val Loss: 0.00778403\n",
      "Epoch:  55 Train Loss: 0.00899523 Val Loss: 0.00993360\n",
      "Epoch:  56 Train Loss: 0.01052610 Val Loss: 0.01030770\n",
      "Epoch:  57 Train Loss: 0.00914815 Val Loss: 0.00780159\n",
      "Epoch:  58 Train Loss: 0.00980386 Val Loss: 0.00780863\n",
      "Epoch:  59 Train Loss: 0.00918458 Val Loss: 0.00773341\n",
      "Epoch:  60 Train Loss: 0.00778246 Val Loss: 0.00817242\n",
      "Epoch:  61 Train Loss: 0.00817514 Val Loss: 0.00752918\n",
      "Epoch:  62 Train Loss: 0.00936711 Val Loss: 0.00750533\n",
      "Epoch:  63 Train Loss: 0.00746044 Val Loss: 0.00893620\n",
      "Epoch:  64 Train Loss: 0.00884658 Val Loss: 0.00753426\n",
      "Epoch:  65 Train Loss: 0.00914237 Val Loss: 0.00873928\n",
      "Epoch:  66 Train Loss: 0.00827597 Val Loss: 0.00765198\n",
      "Epoch:  67 Train Loss: 0.00816054 Val Loss: 0.00879582\n",
      "Epoch:  68 Train Loss: 0.00963677 Val Loss: 0.00780946\n",
      "Epoch:  69 Train Loss: 0.00660351 Val Loss: 0.00746985\n",
      "Epoch:  70 Train Loss: 0.00886624 Val Loss: 0.00734842\n",
      "Epoch:  71 Train Loss: 0.00707343 Val Loss: 0.00814256\n",
      "Epoch:  72 Train Loss: 0.00678162 Val Loss: 0.00732658\n",
      "Epoch:  73 Train Loss: 0.00803238 Val Loss: 0.00697213\n",
      "Epoch:  74 Train Loss: 0.00896352 Val Loss: 0.00915195\n",
      "Epoch:  75 Train Loss: 0.00911332 Val Loss: 0.00924976\n",
      "Epoch:  76 Train Loss: 0.01031928 Val Loss: 0.01002681\n",
      "Epoch:  77 Train Loss: 0.00978697 Val Loss: 0.01104261\n",
      "Epoch:  78 Train Loss: 0.01169749 Val Loss: 0.00766514\n",
      "Epoch:  79 Train Loss: 0.00849369 Val Loss: 0.00774528\n",
      "Epoch:  80 Train Loss: 0.01009293 Val Loss: 0.00828248\n",
      "Epoch:  81 Train Loss: 0.00716713 Val Loss: 0.01035456\n",
      "Epoch:  82 Train Loss: 0.00862836 Val Loss: 0.00878450\n",
      "Epoch:  83 Train Loss: 0.00741005 Val Loss: 0.00709911\n",
      "Epoch:  84 Train Loss: 0.00753784 Val Loss: 0.00693633\n",
      "Epoch:  85 Train Loss: 0.00644451 Val Loss: 0.00673182\n",
      "Epoch:  86 Train Loss: 0.00730673 Val Loss: 0.00689003\n",
      "Epoch:  87 Train Loss: 0.00746916 Val Loss: 0.00658836\n",
      "Epoch:  88 Train Loss: 0.00747403 Val Loss: 0.00652341\n",
      "Epoch:  89 Train Loss: 0.00714845 Val Loss: 0.00651897\n",
      "Epoch:  90 Train Loss: 0.00678322 Val Loss: 0.01134490\n",
      "Epoch:  91 Train Loss: 0.00992833 Val Loss: 0.00744967\n",
      "Epoch:  92 Train Loss: 0.00964415 Val Loss: 0.00655440\n",
      "Epoch:  93 Train Loss: 0.00757090 Val Loss: 0.00649372\n",
      "Epoch:  94 Train Loss: 0.00722933 Val Loss: 0.00682755\n",
      "Epoch:  95 Train Loss: 0.00711848 Val Loss: 0.00702363\n",
      "Epoch:  96 Train Loss: 0.00702007 Val Loss: 0.00901140\n",
      "Epoch:  97 Train Loss: 0.00935684 Val Loss: 0.00708956\n",
      "Epoch:  98 Train Loss: 0.00827255 Val Loss: 0.00669228\n",
      "Epoch:  99 Train Loss: 0.00707065 Val Loss: 0.00799354\n",
      "Epoch: 100 Train Loss: 0.00832002 Val Loss: 0.00685918\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.12690491 Val Loss: 0.08405409\n",
      "Epoch:   2 Train Loss: 0.07154536 Val Loss: 0.03663879\n",
      "Epoch:   3 Train Loss: 0.05097713 Val Loss: 0.03355960\n",
      "Epoch:   4 Train Loss: 0.04901813 Val Loss: 0.03017669\n",
      "Epoch:   5 Train Loss: 0.05235610 Val Loss: 0.03293715\n",
      "Epoch:   6 Train Loss: 0.04665347 Val Loss: 0.02913287\n",
      "Epoch:   7 Train Loss: 0.03889664 Val Loss: 0.02476584\n",
      "Epoch:   8 Train Loss: 0.03561058 Val Loss: 0.02120037\n",
      "Epoch:   9 Train Loss: 0.02710581 Val Loss: 0.01662186\n",
      "Epoch:  10 Train Loss: 0.01835306 Val Loss: 0.01190477\n",
      "Epoch:  11 Train Loss: 0.01098678 Val Loss: 0.00854648\n",
      "Epoch:  12 Train Loss: 0.00906274 Val Loss: 0.00953584\n",
      "Epoch:  13 Train Loss: 0.00974165 Val Loss: 0.01128666\n",
      "Epoch:  14 Train Loss: 0.00920361 Val Loss: 0.00792304\n",
      "Epoch:  15 Train Loss: 0.00908127 Val Loss: 0.01022280\n",
      "Epoch:  16 Train Loss: 0.00830912 Val Loss: 0.00786218\n",
      "Epoch:  17 Train Loss: 0.00907150 Val Loss: 0.00986329\n",
      "Epoch:  18 Train Loss: 0.00894636 Val Loss: 0.00791066\n",
      "Epoch:  19 Train Loss: 0.00826717 Val Loss: 0.00876740\n",
      "Epoch:  20 Train Loss: 0.00771516 Val Loss: 0.00792355\n",
      "Epoch:  21 Train Loss: 0.00992859 Val Loss: 0.00777782\n",
      "Epoch:  22 Train Loss: 0.00878969 Val Loss: 0.00761110\n",
      "Epoch:  23 Train Loss: 0.00829505 Val Loss: 0.00764594\n",
      "Epoch:  24 Train Loss: 0.00827950 Val Loss: 0.00790686\n",
      "Epoch:  25 Train Loss: 0.01268353 Val Loss: 0.00803913\n",
      "Epoch:  26 Train Loss: 0.01184492 Val Loss: 0.00876540\n",
      "Epoch:  27 Train Loss: 0.00950509 Val Loss: 0.00807273\n",
      "Epoch:  28 Train Loss: 0.00824043 Val Loss: 0.00946929\n",
      "Epoch:  29 Train Loss: 0.00760738 Val Loss: 0.00746375\n",
      "Epoch:  30 Train Loss: 0.00779623 Val Loss: 0.00761239\n",
      "Epoch:  31 Train Loss: 0.00711090 Val Loss: 0.00747899\n",
      "Epoch:  32 Train Loss: 0.00733587 Val Loss: 0.00750730\n",
      "Epoch:  33 Train Loss: 0.00765185 Val Loss: 0.00741448\n",
      "Epoch:  34 Train Loss: 0.00688995 Val Loss: 0.00743446\n",
      "Epoch:  35 Train Loss: 0.00702415 Val Loss: 0.00785599\n",
      "Epoch:  36 Train Loss: 0.00844020 Val Loss: 0.00741278\n",
      "Epoch:  37 Train Loss: 0.00765365 Val Loss: 0.00752374\n",
      "Epoch:  38 Train Loss: 0.00692040 Val Loss: 0.00743165\n",
      "Epoch:  39 Train Loss: 0.00812974 Val Loss: 0.00736116\n",
      "Epoch:  40 Train Loss: 0.00866995 Val Loss: 0.00864214\n",
      "Epoch:  41 Train Loss: 0.00724852 Val Loss: 0.00776565\n",
      "Epoch:  42 Train Loss: 0.00788924 Val Loss: 0.00739340\n",
      "Epoch:  43 Train Loss: 0.00794759 Val Loss: 0.00731541\n",
      "Epoch:  44 Train Loss: 0.00781865 Val Loss: 0.00745652\n",
      "Epoch:  45 Train Loss: 0.00761377 Val Loss: 0.00730559\n",
      "Epoch:  46 Train Loss: 0.00828494 Val Loss: 0.00748771\n",
      "Epoch:  47 Train Loss: 0.00814020 Val Loss: 0.00736558\n",
      "Epoch:  48 Train Loss: 0.00907540 Val Loss: 0.00726268\n",
      "Epoch:  49 Train Loss: 0.00827411 Val Loss: 0.00734912\n",
      "Epoch:  50 Train Loss: 0.00718562 Val Loss: 0.00732438\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.19791080 Val Loss: 0.15000473\n",
      "Epoch:   2 Train Loss: 0.11005744 Val Loss: 0.06825043\n",
      "Epoch:   3 Train Loss: 0.05807151 Val Loss: 0.03360760\n",
      "Epoch:   4 Train Loss: 0.06178806 Val Loss: 0.03265424\n",
      "Epoch:   5 Train Loss: 0.05082649 Val Loss: 0.03349168\n",
      "Epoch:   6 Train Loss: 0.04712226 Val Loss: 0.03170669\n",
      "Epoch:   7 Train Loss: 0.04049420 Val Loss: 0.02581057\n",
      "Epoch:   8 Train Loss: 0.03576017 Val Loss: 0.02259211\n",
      "Epoch:   9 Train Loss: 0.03062729 Val Loss: 0.01898522\n",
      "Epoch:  10 Train Loss: 0.02230600 Val Loss: 0.01226389\n",
      "Epoch:  11 Train Loss: 0.01386763 Val Loss: 0.01314214\n",
      "Epoch:  12 Train Loss: 0.01077995 Val Loss: 0.00919278\n",
      "Epoch:  13 Train Loss: 0.01158511 Val Loss: 0.01406207\n",
      "Epoch:  14 Train Loss: 0.00977538 Val Loss: 0.00886104\n",
      "Epoch:  15 Train Loss: 0.01025176 Val Loss: 0.01055827\n",
      "Epoch:  16 Train Loss: 0.01053607 Val Loss: 0.00842218\n",
      "Epoch:  17 Train Loss: 0.00856874 Val Loss: 0.00917964\n",
      "Epoch:  18 Train Loss: 0.00967480 Val Loss: 0.00881716\n",
      "Epoch:  19 Train Loss: 0.00848296 Val Loss: 0.00993072\n",
      "Epoch:  20 Train Loss: 0.00900442 Val Loss: 0.00841132\n",
      "Epoch:  21 Train Loss: 0.00969216 Val Loss: 0.00884228\n",
      "Epoch:  22 Train Loss: 0.00831195 Val Loss: 0.00822951\n",
      "Epoch:  23 Train Loss: 0.00949701 Val Loss: 0.00825271\n",
      "Epoch:  24 Train Loss: 0.00954458 Val Loss: 0.00876927\n",
      "Epoch:  25 Train Loss: 0.00911906 Val Loss: 0.00847616\n",
      "Epoch:  26 Train Loss: 0.00940402 Val Loss: 0.00832855\n",
      "Epoch:  27 Train Loss: 0.00803140 Val Loss: 0.00818757\n",
      "Epoch:  28 Train Loss: 0.00765036 Val Loss: 0.00831808\n",
      "Epoch:  29 Train Loss: 0.00962471 Val Loss: 0.00815761\n",
      "Epoch:  30 Train Loss: 0.00972208 Val Loss: 0.00845757\n",
      "Epoch:  31 Train Loss: 0.00852229 Val Loss: 0.00828870\n",
      "Epoch:  32 Train Loss: 0.00784855 Val Loss: 0.00839684\n",
      "Epoch:  33 Train Loss: 0.00864327 Val Loss: 0.00810413\n",
      "Epoch:  34 Train Loss: 0.00839414 Val Loss: 0.00817675\n",
      "Epoch:  35 Train Loss: 0.00782863 Val Loss: 0.00797655\n",
      "Epoch:  36 Train Loss: 0.00969992 Val Loss: 0.00881162\n",
      "Epoch:  37 Train Loss: 0.00949403 Val Loss: 0.00795445\n",
      "Epoch:  38 Train Loss: 0.00670293 Val Loss: 0.00850031\n",
      "Epoch:  39 Train Loss: 0.00924590 Val Loss: 0.00787988\n",
      "Epoch:  40 Train Loss: 0.00790835 Val Loss: 0.00808788\n",
      "Epoch:  41 Train Loss: 0.00809255 Val Loss: 0.00781706\n",
      "Epoch:  42 Train Loss: 0.00739125 Val Loss: 0.00793874\n",
      "Epoch:  43 Train Loss: 0.00848046 Val Loss: 0.00804876\n",
      "Epoch:  44 Train Loss: 0.01147292 Val Loss: 0.00777536\n",
      "Epoch:  45 Train Loss: 0.01013852 Val Loss: 0.00799774\n",
      "Epoch:  46 Train Loss: 0.00834658 Val Loss: 0.00781311\n",
      "Epoch:  47 Train Loss: 0.00778064 Val Loss: 0.00809318\n",
      "Epoch:  48 Train Loss: 0.00951890 Val Loss: 0.00787435\n",
      "Epoch:  49 Train Loss: 0.00761405 Val Loss: 0.00798031\n",
      "Epoch:  50 Train Loss: 0.00895265 Val Loss: 0.00763204\n",
      "Epoch:  51 Train Loss: 0.00726074 Val Loss: 0.00829281\n",
      "Epoch:  52 Train Loss: 0.00894356 Val Loss: 0.00774970\n",
      "Epoch:  53 Train Loss: 0.00791486 Val Loss: 0.00860167\n",
      "Epoch:  54 Train Loss: 0.00922999 Val Loss: 0.00773899\n",
      "Epoch:  55 Train Loss: 0.00944586 Val Loss: 0.00808042\n",
      "Epoch:  56 Train Loss: 0.00867263 Val Loss: 0.00756871\n",
      "Epoch:  57 Train Loss: 0.00874085 Val Loss: 0.00852722\n",
      "Epoch:  58 Train Loss: 0.00838955 Val Loss: 0.00745428\n",
      "Epoch:  59 Train Loss: 0.00823848 Val Loss: 0.00743937\n",
      "Epoch:  60 Train Loss: 0.00786457 Val Loss: 0.00746651\n",
      "Epoch:  61 Train Loss: 0.00757921 Val Loss: 0.00770762\n",
      "Epoch:  62 Train Loss: 0.00760339 Val Loss: 0.00748188\n",
      "Epoch:  63 Train Loss: 0.00798137 Val Loss: 0.00736513\n",
      "Epoch:  64 Train Loss: 0.00785011 Val Loss: 0.00764478\n",
      "Epoch:  65 Train Loss: 0.00752195 Val Loss: 0.00738142\n",
      "Epoch:  66 Train Loss: 0.00862442 Val Loss: 0.00769341\n",
      "Epoch:  67 Train Loss: 0.00715870 Val Loss: 0.00734993\n",
      "Epoch:  68 Train Loss: 0.00712194 Val Loss: 0.00769730\n",
      "Epoch:  69 Train Loss: 0.00890752 Val Loss: 0.00734186\n",
      "Epoch:  70 Train Loss: 0.00850957 Val Loss: 0.00728424\n",
      "Epoch:  71 Train Loss: 0.00680488 Val Loss: 0.00745580\n",
      "Epoch:  72 Train Loss: 0.00836658 Val Loss: 0.00726541\n",
      "Epoch:  73 Train Loss: 0.00869148 Val Loss: 0.00753849\n",
      "Epoch:  74 Train Loss: 0.00768897 Val Loss: 0.00731736\n",
      "Epoch:  75 Train Loss: 0.00724508 Val Loss: 0.00719859\n",
      "Epoch:  76 Train Loss: 0.00646612 Val Loss: 0.00741996\n",
      "Epoch:  77 Train Loss: 0.01041261 Val Loss: 0.00718877\n",
      "Epoch:  78 Train Loss: 0.00768644 Val Loss: 0.00841695\n",
      "Epoch:  79 Train Loss: 0.00871442 Val Loss: 0.00731246\n",
      "Epoch:  80 Train Loss: 0.00931988 Val Loss: 0.00725593\n",
      "Epoch:  81 Train Loss: 0.00756533 Val Loss: 0.00720761\n",
      "Epoch:  82 Train Loss: 0.00925050 Val Loss: 0.00786817\n",
      "Epoch:  83 Train Loss: 0.00894173 Val Loss: 0.00717981\n",
      "Epoch:  84 Train Loss: 0.00765169 Val Loss: 0.00718109\n",
      "Epoch:  85 Train Loss: 0.00811647 Val Loss: 0.00772415\n",
      "Epoch:  86 Train Loss: 0.00744906 Val Loss: 0.00716691\n",
      "Epoch:  87 Train Loss: 0.00831023 Val Loss: 0.00780193\n",
      "Epoch:  88 Train Loss: 0.00776230 Val Loss: 0.00718029\n",
      "Epoch:  89 Train Loss: 0.00757429 Val Loss: 0.00773531\n",
      "Epoch:  90 Train Loss: 0.00837974 Val Loss: 0.00708240\n",
      "Epoch:  91 Train Loss: 0.00764317 Val Loss: 0.00730409\n",
      "Epoch:  92 Train Loss: 0.00763438 Val Loss: 0.00708768\n",
      "Epoch:  93 Train Loss: 0.00737742 Val Loss: 0.00758831\n",
      "Epoch:  94 Train Loss: 0.00839439 Val Loss: 0.00700533\n",
      "Epoch:  95 Train Loss: 0.00693465 Val Loss: 0.00715057\n",
      "Epoch:  96 Train Loss: 0.00807983 Val Loss: 0.00711813\n",
      "Epoch:  97 Train Loss: 0.00853477 Val Loss: 0.00713861\n",
      "Epoch:  98 Train Loss: 0.00688327 Val Loss: 0.00746830\n",
      "Epoch:  99 Train Loss: 0.00715409 Val Loss: 0.00693614\n",
      "Epoch: 100 Train Loss: 0.00692529 Val Loss: 0.00694654\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.08719951 Val Loss: 0.04678481\n",
      "Epoch:   2 Train Loss: 0.05130003 Val Loss: 0.02764274\n",
      "Epoch:   3 Train Loss: 0.04001318 Val Loss: 0.02382104\n",
      "Epoch:   4 Train Loss: 0.01939171 Val Loss: 0.00891854\n",
      "Epoch:   5 Train Loss: 0.01102558 Val Loss: 0.00992083\n",
      "Epoch:   6 Train Loss: 0.01012658 Val Loss: 0.01003288\n",
      "Epoch:   7 Train Loss: 0.00949437 Val Loss: 0.00878377\n",
      "Epoch:   8 Train Loss: 0.00958057 Val Loss: 0.00898096\n",
      "Epoch:   9 Train Loss: 0.01070011 Val Loss: 0.00885794\n",
      "Epoch:  10 Train Loss: 0.00986165 Val Loss: 0.00883020\n",
      "Epoch:  11 Train Loss: 0.01135068 Val Loss: 0.01567031\n",
      "Epoch:  12 Train Loss: 0.01290334 Val Loss: 0.01218467\n",
      "Epoch:  13 Train Loss: 0.01007784 Val Loss: 0.00831223\n",
      "Epoch:  14 Train Loss: 0.00945171 Val Loss: 0.01055294\n",
      "Epoch:  15 Train Loss: 0.00936498 Val Loss: 0.01044450\n",
      "Epoch:  16 Train Loss: 0.00896786 Val Loss: 0.00836399\n",
      "Epoch:  17 Train Loss: 0.00892344 Val Loss: 0.01245321\n",
      "Epoch:  18 Train Loss: 0.01064037 Val Loss: 0.00953723\n",
      "Epoch:  19 Train Loss: 0.00917146 Val Loss: 0.00826989\n",
      "Epoch:  20 Train Loss: 0.00793282 Val Loss: 0.00813114\n",
      "Epoch:  21 Train Loss: 0.00734788 Val Loss: 0.00831448\n",
      "Epoch:  22 Train Loss: 0.00822364 Val Loss: 0.00842425\n",
      "Epoch:  23 Train Loss: 0.00738512 Val Loss: 0.00807388\n",
      "Epoch:  24 Train Loss: 0.00837811 Val Loss: 0.01273583\n",
      "Epoch:  25 Train Loss: 0.01143038 Val Loss: 0.00852282\n",
      "Epoch:  26 Train Loss: 0.01055841 Val Loss: 0.01044221\n",
      "Epoch:  27 Train Loss: 0.01082679 Val Loss: 0.00854459\n",
      "Epoch:  28 Train Loss: 0.00824285 Val Loss: 0.00931651\n",
      "Epoch:  29 Train Loss: 0.00879715 Val Loss: 0.00885935\n",
      "Epoch:  30 Train Loss: 0.00763235 Val Loss: 0.00798957\n",
      "Epoch:  31 Train Loss: 0.00843698 Val Loss: 0.00951940\n",
      "Epoch:  32 Train Loss: 0.00759083 Val Loss: 0.00876723\n",
      "Epoch:  33 Train Loss: 0.00745629 Val Loss: 0.00920431\n",
      "Epoch:  34 Train Loss: 0.01008675 Val Loss: 0.01018007\n",
      "Epoch:  35 Train Loss: 0.01113109 Val Loss: 0.01764421\n",
      "Epoch:  36 Train Loss: 0.01257197 Val Loss: 0.01061215\n",
      "Epoch:  37 Train Loss: 0.01083703 Val Loss: 0.00809597\n",
      "Epoch:  38 Train Loss: 0.00830461 Val Loss: 0.00851757\n",
      "Epoch:  39 Train Loss: 0.00992767 Val Loss: 0.00873943\n",
      "Epoch:  40 Train Loss: 0.00918080 Val Loss: 0.00810576\n",
      "Epoch:  41 Train Loss: 0.00782375 Val Loss: 0.00796573\n",
      "Epoch:  42 Train Loss: 0.00794732 Val Loss: 0.00793763\n",
      "Epoch:  43 Train Loss: 0.00747491 Val Loss: 0.00787816\n",
      "Epoch:  44 Train Loss: 0.00703927 Val Loss: 0.00852626\n",
      "Epoch:  45 Train Loss: 0.00739237 Val Loss: 0.00925581\n",
      "Epoch:  46 Train Loss: 0.00877871 Val Loss: 0.00825136\n",
      "Epoch:  47 Train Loss: 0.00856298 Val Loss: 0.00795895\n",
      "Epoch:  48 Train Loss: 0.00764455 Val Loss: 0.00790455\n",
      "Epoch:  49 Train Loss: 0.00734479 Val Loss: 0.00900688\n",
      "Epoch:  50 Train Loss: 0.00771449 Val Loss: 0.00812024\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.09498579 Val Loss: 0.03933996\n",
      "Epoch:   2 Train Loss: 0.04544765 Val Loss: 0.02375286\n",
      "Epoch:   3 Train Loss: 0.02677383 Val Loss: 0.01069579\n",
      "Epoch:   4 Train Loss: 0.02208836 Val Loss: 0.01736486\n",
      "Epoch:   5 Train Loss: 0.01516281 Val Loss: 0.01167104\n",
      "Epoch:   6 Train Loss: 0.01554944 Val Loss: 0.00953435\n",
      "Epoch:   7 Train Loss: 0.00827363 Val Loss: 0.00989778\n",
      "Epoch:   8 Train Loss: 0.00880099 Val Loss: 0.00957486\n",
      "Epoch:   9 Train Loss: 0.01226245 Val Loss: 0.00943421\n",
      "Epoch:  10 Train Loss: 0.01166378 Val Loss: 0.00910249\n",
      "Epoch:  11 Train Loss: 0.01113697 Val Loss: 0.01066834\n",
      "Epoch:  12 Train Loss: 0.01008192 Val Loss: 0.01237783\n",
      "Epoch:  13 Train Loss: 0.01026404 Val Loss: 0.01037124\n",
      "Epoch:  14 Train Loss: 0.01205494 Val Loss: 0.00838897\n",
      "Epoch:  15 Train Loss: 0.00976513 Val Loss: 0.00929272\n",
      "Epoch:  16 Train Loss: 0.01027102 Val Loss: 0.00901859\n",
      "Epoch:  17 Train Loss: 0.00778409 Val Loss: 0.00811700\n",
      "Epoch:  18 Train Loss: 0.00832791 Val Loss: 0.00811627\n",
      "Epoch:  19 Train Loss: 0.00879956 Val Loss: 0.00830953\n",
      "Epoch:  20 Train Loss: 0.00820185 Val Loss: 0.00818247\n",
      "Epoch:  21 Train Loss: 0.00714941 Val Loss: 0.00846762\n",
      "Epoch:  22 Train Loss: 0.00838066 Val Loss: 0.00792170\n",
      "Epoch:  23 Train Loss: 0.00701404 Val Loss: 0.00787858\n",
      "Epoch:  24 Train Loss: 0.00786297 Val Loss: 0.00835417\n",
      "Epoch:  25 Train Loss: 0.00716723 Val Loss: 0.00789159\n",
      "Epoch:  26 Train Loss: 0.00757846 Val Loss: 0.00777380\n",
      "Epoch:  27 Train Loss: 0.00711237 Val Loss: 0.00795945\n",
      "Epoch:  28 Train Loss: 0.00774026 Val Loss: 0.00770114\n",
      "Epoch:  29 Train Loss: 0.00746904 Val Loss: 0.00759923\n",
      "Epoch:  30 Train Loss: 0.00979354 Val Loss: 0.00746803\n",
      "Epoch:  31 Train Loss: 0.00815430 Val Loss: 0.00921996\n",
      "Epoch:  32 Train Loss: 0.00882450 Val Loss: 0.00876171\n",
      "Epoch:  33 Train Loss: 0.00903178 Val Loss: 0.00857428\n",
      "Epoch:  34 Train Loss: 0.00894983 Val Loss: 0.01674267\n",
      "Epoch:  35 Train Loss: 0.01379828 Val Loss: 0.01113578\n",
      "Epoch:  36 Train Loss: 0.00910768 Val Loss: 0.00854616\n",
      "Epoch:  37 Train Loss: 0.00751988 Val Loss: 0.00863924\n",
      "Epoch:  38 Train Loss: 0.00724235 Val Loss: 0.00981193\n",
      "Epoch:  39 Train Loss: 0.00781628 Val Loss: 0.00819642\n",
      "Epoch:  40 Train Loss: 0.00778483 Val Loss: 0.00793148\n",
      "Epoch:  41 Train Loss: 0.00793644 Val Loss: 0.00769778\n",
      "Epoch:  42 Train Loss: 0.00784319 Val Loss: 0.00964770\n",
      "Epoch:  43 Train Loss: 0.00902468 Val Loss: 0.00787009\n",
      "Epoch:  44 Train Loss: 0.00931002 Val Loss: 0.00837706\n",
      "Epoch:  45 Train Loss: 0.00865301 Val Loss: 0.01101859\n",
      "Epoch:  46 Train Loss: 0.01037896 Val Loss: 0.00797262\n",
      "Epoch:  47 Train Loss: 0.00982406 Val Loss: 0.00756474\n",
      "Epoch:  48 Train Loss: 0.00668672 Val Loss: 0.00761229\n",
      "Epoch:  49 Train Loss: 0.00910659 Val Loss: 0.00852238\n",
      "Epoch:  50 Train Loss: 0.00925139 Val Loss: 0.00755030\n",
      "Epoch:  51 Train Loss: 0.00805282 Val Loss: 0.00723502\n",
      "Epoch:  52 Train Loss: 0.00783744 Val Loss: 0.01003568\n",
      "Epoch:  53 Train Loss: 0.00818323 Val Loss: 0.00837486\n",
      "Epoch:  54 Train Loss: 0.00886342 Val Loss: 0.00811461\n",
      "Epoch:  55 Train Loss: 0.00775187 Val Loss: 0.01021253\n",
      "Epoch:  56 Train Loss: 0.00781961 Val Loss: 0.00779941\n",
      "Epoch:  57 Train Loss: 0.00787401 Val Loss: 0.00851141\n",
      "Epoch:  58 Train Loss: 0.00844597 Val Loss: 0.01091404\n",
      "Epoch:  59 Train Loss: 0.01097659 Val Loss: 0.00867919\n",
      "Epoch:  60 Train Loss: 0.00855994 Val Loss: 0.00799496\n",
      "Epoch:  61 Train Loss: 0.00750872 Val Loss: 0.00885525\n",
      "Epoch:  62 Train Loss: 0.00716006 Val Loss: 0.00757474\n",
      "Epoch:  63 Train Loss: 0.00859167 Val Loss: 0.00753563\n",
      "Epoch:  64 Train Loss: 0.00720878 Val Loss: 0.01039522\n",
      "Epoch:  65 Train Loss: 0.00976418 Val Loss: 0.00783323\n",
      "Epoch:  66 Train Loss: 0.00749736 Val Loss: 0.00780042\n",
      "Epoch:  67 Train Loss: 0.00729971 Val Loss: 0.00751665\n",
      "Epoch:  68 Train Loss: 0.00835393 Val Loss: 0.00744603\n",
      "Epoch:  69 Train Loss: 0.00712943 Val Loss: 0.00739430\n",
      "Epoch:  70 Train Loss: 0.00837512 Val Loss: 0.00731019\n",
      "Epoch:  71 Train Loss: 0.00739079 Val Loss: 0.00688954\n",
      "Epoch:  72 Train Loss: 0.00820123 Val Loss: 0.00700320\n",
      "Epoch:  73 Train Loss: 0.00721336 Val Loss: 0.00721313\n",
      "Epoch:  74 Train Loss: 0.00746358 Val Loss: 0.00752697\n",
      "Epoch:  75 Train Loss: 0.00682064 Val Loss: 0.00688757\n",
      "Epoch:  76 Train Loss: 0.00805699 Val Loss: 0.00725727\n",
      "Epoch:  77 Train Loss: 0.00737781 Val Loss: 0.00991400\n",
      "Epoch:  78 Train Loss: 0.00838403 Val Loss: 0.00938012\n",
      "Epoch:  79 Train Loss: 0.00790463 Val Loss: 0.00809639\n",
      "Epoch:  80 Train Loss: 0.00699711 Val Loss: 0.00768158\n",
      "Epoch:  81 Train Loss: 0.00860797 Val Loss: 0.00884050\n",
      "Epoch:  82 Train Loss: 0.00801598 Val Loss: 0.00773359\n",
      "Epoch:  83 Train Loss: 0.00940562 Val Loss: 0.00744351\n",
      "Epoch:  84 Train Loss: 0.00822506 Val Loss: 0.00704039\n",
      "Epoch:  85 Train Loss: 0.00898343 Val Loss: 0.00722258\n",
      "Epoch:  86 Train Loss: 0.00749786 Val Loss: 0.00774775\n",
      "Epoch:  87 Train Loss: 0.00835346 Val Loss: 0.00678813\n",
      "Epoch:  88 Train Loss: 0.00737764 Val Loss: 0.00684460\n",
      "Epoch:  89 Train Loss: 0.00706282 Val Loss: 0.00873158\n",
      "Epoch:  90 Train Loss: 0.00749279 Val Loss: 0.00727788\n",
      "Epoch:  91 Train Loss: 0.00742070 Val Loss: 0.00848550\n",
      "Epoch:  92 Train Loss: 0.00691320 Val Loss: 0.00870687\n",
      "Epoch:  93 Train Loss: 0.00795156 Val Loss: 0.00775534\n",
      "Epoch:  94 Train Loss: 0.00933582 Val Loss: 0.00881216\n",
      "Epoch:  95 Train Loss: 0.01100169 Val Loss: 0.00808052\n",
      "Epoch:  96 Train Loss: 0.00974098 Val Loss: 0.00970802\n",
      "Epoch:  97 Train Loss: 0.00891390 Val Loss: 0.00725086\n",
      "Epoch:  98 Train Loss: 0.00773657 Val Loss: 0.00733000\n",
      "Epoch:  99 Train Loss: 0.00716349 Val Loss: 0.00924626\n",
      "Epoch: 100 Train Loss: 0.00716623 Val Loss: 0.00771218\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.13176066 Val Loss: 0.08564174\n",
      "Epoch:   2 Train Loss: 0.07651342 Val Loss: 0.03682397\n",
      "Epoch:   3 Train Loss: 0.05022692 Val Loss: 0.03391246\n",
      "Epoch:   4 Train Loss: 0.05481446 Val Loss: 0.02932060\n",
      "Epoch:   5 Train Loss: 0.04436218 Val Loss: 0.02983215\n",
      "Epoch:   6 Train Loss: 0.04238901 Val Loss: 0.02662600\n",
      "Epoch:   7 Train Loss: 0.03900043 Val Loss: 0.02111079\n",
      "Epoch:   8 Train Loss: 0.02630436 Val Loss: 0.01621628\n",
      "Epoch:   9 Train Loss: 0.01778551 Val Loss: 0.01009626\n",
      "Epoch:  10 Train Loss: 0.01223035 Val Loss: 0.00879887\n",
      "Epoch:  11 Train Loss: 0.01201238 Val Loss: 0.01464393\n",
      "Epoch:  12 Train Loss: 0.01477143 Val Loss: 0.00804860\n",
      "Epoch:  13 Train Loss: 0.01056902 Val Loss: 0.00975068\n",
      "Epoch:  14 Train Loss: 0.01104039 Val Loss: 0.00801506\n",
      "Epoch:  15 Train Loss: 0.00953564 Val Loss: 0.00924695\n",
      "Epoch:  16 Train Loss: 0.00802889 Val Loss: 0.00776484\n",
      "Epoch:  17 Train Loss: 0.01145742 Val Loss: 0.00928712\n",
      "Epoch:  18 Train Loss: 0.00840004 Val Loss: 0.00766465\n",
      "Epoch:  19 Train Loss: 0.00894898 Val Loss: 0.00796222\n",
      "Epoch:  20 Train Loss: 0.00952639 Val Loss: 0.00760321\n",
      "Epoch:  21 Train Loss: 0.00983150 Val Loss: 0.00778687\n",
      "Epoch:  22 Train Loss: 0.00864099 Val Loss: 0.00755488\n",
      "Epoch:  23 Train Loss: 0.00930657 Val Loss: 0.00788328\n",
      "Epoch:  24 Train Loss: 0.00904412 Val Loss: 0.00748410\n",
      "Epoch:  25 Train Loss: 0.00799664 Val Loss: 0.00762611\n",
      "Epoch:  26 Train Loss: 0.00884333 Val Loss: 0.00743500\n",
      "Epoch:  27 Train Loss: 0.00856678 Val Loss: 0.00783708\n",
      "Epoch:  28 Train Loss: 0.00912510 Val Loss: 0.00738736\n",
      "Epoch:  29 Train Loss: 0.01046106 Val Loss: 0.00816647\n",
      "Epoch:  30 Train Loss: 0.00972569 Val Loss: 0.00756854\n",
      "Epoch:  31 Train Loss: 0.00851153 Val Loss: 0.00740934\n",
      "Epoch:  32 Train Loss: 0.00923468 Val Loss: 0.00839162\n",
      "Epoch:  33 Train Loss: 0.00866037 Val Loss: 0.00734489\n",
      "Epoch:  34 Train Loss: 0.00981193 Val Loss: 0.00828449\n",
      "Epoch:  35 Train Loss: 0.00949027 Val Loss: 0.00728526\n",
      "Epoch:  36 Train Loss: 0.00794974 Val Loss: 0.00735899\n",
      "Epoch:  37 Train Loss: 0.00846230 Val Loss: 0.00730012\n",
      "Epoch:  38 Train Loss: 0.00837554 Val Loss: 0.00733486\n",
      "Epoch:  39 Train Loss: 0.00827442 Val Loss: 0.00746154\n",
      "Epoch:  40 Train Loss: 0.00980387 Val Loss: 0.00721915\n",
      "Epoch:  41 Train Loss: 0.00814817 Val Loss: 0.00743203\n",
      "Epoch:  42 Train Loss: 0.00907046 Val Loss: 0.00738468\n",
      "Epoch:  43 Train Loss: 0.00938351 Val Loss: 0.00742498\n",
      "Epoch:  44 Train Loss: 0.00795828 Val Loss: 0.00744302\n",
      "Epoch:  45 Train Loss: 0.00810369 Val Loss: 0.00716560\n",
      "Epoch:  46 Train Loss: 0.00727612 Val Loss: 0.00733734\n",
      "Epoch:  47 Train Loss: 0.00858770 Val Loss: 0.00707529\n",
      "Epoch:  48 Train Loss: 0.00793475 Val Loss: 0.00742919\n",
      "Epoch:  49 Train Loss: 0.00907168 Val Loss: 0.00707826\n",
      "Epoch:  50 Train Loss: 0.00865601 Val Loss: 0.00762900\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.20498916 Val Loss: 0.15451079\n",
      "Epoch:   2 Train Loss: 0.11803498 Val Loss: 0.07872616\n",
      "Epoch:   3 Train Loss: 0.06715799 Val Loss: 0.03045474\n",
      "Epoch:   4 Train Loss: 0.04860187 Val Loss: 0.03158987\n",
      "Epoch:   5 Train Loss: 0.04789737 Val Loss: 0.02654556\n",
      "Epoch:   6 Train Loss: 0.04678836 Val Loss: 0.02659563\n",
      "Epoch:   7 Train Loss: 0.03331682 Val Loss: 0.02127618\n",
      "Epoch:   8 Train Loss: 0.03288784 Val Loss: 0.01727415\n",
      "Epoch:   9 Train Loss: 0.02454909 Val Loss: 0.01273214\n",
      "Epoch:  10 Train Loss: 0.01538369 Val Loss: 0.00858495\n",
      "Epoch:  11 Train Loss: 0.00999816 Val Loss: 0.00907920\n",
      "Epoch:  12 Train Loss: 0.01266086 Val Loss: 0.01173289\n",
      "Epoch:  13 Train Loss: 0.01276190 Val Loss: 0.01055406\n",
      "Epoch:  14 Train Loss: 0.00902946 Val Loss: 0.00873728\n",
      "Epoch:  15 Train Loss: 0.01023777 Val Loss: 0.00806819\n",
      "Epoch:  16 Train Loss: 0.01022068 Val Loss: 0.00861472\n",
      "Epoch:  17 Train Loss: 0.01012152 Val Loss: 0.00746673\n",
      "Epoch:  18 Train Loss: 0.00974498 Val Loss: 0.00772271\n",
      "Epoch:  19 Train Loss: 0.00990521 Val Loss: 0.00758971\n",
      "Epoch:  20 Train Loss: 0.00943699 Val Loss: 0.00818101\n",
      "Epoch:  21 Train Loss: 0.00869706 Val Loss: 0.00736303\n",
      "Epoch:  22 Train Loss: 0.01097767 Val Loss: 0.01002323\n",
      "Epoch:  23 Train Loss: 0.01017720 Val Loss: 0.00734774\n",
      "Epoch:  24 Train Loss: 0.01099919 Val Loss: 0.00730517\n",
      "Epoch:  25 Train Loss: 0.00813489 Val Loss: 0.00756214\n",
      "Epoch:  26 Train Loss: 0.00833819 Val Loss: 0.00738235\n",
      "Epoch:  27 Train Loss: 0.00716980 Val Loss: 0.00742416\n",
      "Epoch:  28 Train Loss: 0.00849682 Val Loss: 0.00759695\n",
      "Epoch:  29 Train Loss: 0.01034428 Val Loss: 0.00748885\n",
      "Epoch:  30 Train Loss: 0.00861223 Val Loss: 0.00786262\n",
      "Epoch:  31 Train Loss: 0.00885027 Val Loss: 0.00722869\n",
      "Epoch:  32 Train Loss: 0.00926227 Val Loss: 0.00734204\n",
      "Epoch:  33 Train Loss: 0.00766116 Val Loss: 0.00721335\n",
      "Epoch:  34 Train Loss: 0.00804506 Val Loss: 0.00732313\n",
      "Epoch:  35 Train Loss: 0.00996745 Val Loss: 0.00729244\n",
      "Epoch:  36 Train Loss: 0.00822752 Val Loss: 0.00762395\n",
      "Epoch:  37 Train Loss: 0.00981959 Val Loss: 0.00717389\n",
      "Epoch:  38 Train Loss: 0.00887069 Val Loss: 0.00725078\n",
      "Epoch:  39 Train Loss: 0.00931031 Val Loss: 0.00736078\n",
      "Epoch:  40 Train Loss: 0.00901397 Val Loss: 0.00718162\n",
      "Epoch:  41 Train Loss: 0.00868315 Val Loss: 0.00758302\n",
      "Epoch:  42 Train Loss: 0.00957083 Val Loss: 0.00762123\n",
      "Epoch:  43 Train Loss: 0.00788185 Val Loss: 0.00796702\n",
      "Epoch:  44 Train Loss: 0.00921802 Val Loss: 0.00754360\n",
      "Epoch:  45 Train Loss: 0.00968238 Val Loss: 0.00729511\n",
      "Epoch:  46 Train Loss: 0.00748481 Val Loss: 0.00729836\n",
      "Epoch:  47 Train Loss: 0.00892340 Val Loss: 0.00746903\n",
      "Epoch:  48 Train Loss: 0.00828799 Val Loss: 0.00719417\n",
      "Epoch:  49 Train Loss: 0.00941130 Val Loss: 0.00770970\n",
      "Epoch:  50 Train Loss: 0.00724334 Val Loss: 0.00716679\n",
      "Epoch:  51 Train Loss: 0.00837635 Val Loss: 0.00714873\n",
      "Epoch:  52 Train Loss: 0.00962875 Val Loss: 0.00713693\n",
      "Epoch:  53 Train Loss: 0.00911992 Val Loss: 0.00727730\n",
      "Epoch:  54 Train Loss: 0.00940383 Val Loss: 0.00712655\n",
      "Epoch:  55 Train Loss: 0.00850709 Val Loss: 0.00861143\n",
      "Epoch:  56 Train Loss: 0.00997313 Val Loss: 0.00711299\n",
      "Epoch:  57 Train Loss: 0.01014317 Val Loss: 0.00713053\n",
      "Epoch:  58 Train Loss: 0.00907192 Val Loss: 0.00745551\n",
      "Epoch:  59 Train Loss: 0.00812226 Val Loss: 0.00735975\n",
      "Epoch:  60 Train Loss: 0.00811988 Val Loss: 0.00777469\n",
      "Epoch:  61 Train Loss: 0.00836189 Val Loss: 0.00712919\n",
      "Epoch:  62 Train Loss: 0.00869524 Val Loss: 0.00704898\n",
      "Epoch:  63 Train Loss: 0.00817439 Val Loss: 0.00717960\n",
      "Epoch:  64 Train Loss: 0.00731407 Val Loss: 0.00704635\n",
      "Epoch:  65 Train Loss: 0.00745820 Val Loss: 0.00707472\n",
      "Epoch:  66 Train Loss: 0.00750836 Val Loss: 0.00724788\n",
      "Epoch:  67 Train Loss: 0.00723838 Val Loss: 0.00704950\n",
      "Epoch:  68 Train Loss: 0.00839925 Val Loss: 0.00715322\n",
      "Epoch:  69 Train Loss: 0.00983783 Val Loss: 0.00817844\n",
      "Epoch:  70 Train Loss: 0.00972092 Val Loss: 0.00701655\n",
      "Epoch:  71 Train Loss: 0.00891574 Val Loss: 0.00734213\n",
      "Epoch:  72 Train Loss: 0.00938507 Val Loss: 0.00703689\n",
      "Epoch:  73 Train Loss: 0.00864070 Val Loss: 0.00701185\n",
      "Epoch:  74 Train Loss: 0.00835497 Val Loss: 0.00704859\n",
      "Epoch:  75 Train Loss: 0.00780214 Val Loss: 0.00702922\n",
      "Epoch:  76 Train Loss: 0.00832371 Val Loss: 0.00722953\n",
      "Epoch:  77 Train Loss: 0.00738464 Val Loss: 0.00697945\n",
      "Epoch:  78 Train Loss: 0.01009036 Val Loss: 0.00718222\n",
      "Epoch:  79 Train Loss: 0.01073600 Val Loss: 0.00696871\n",
      "Epoch:  80 Train Loss: 0.00854777 Val Loss: 0.00696458\n",
      "Epoch:  81 Train Loss: 0.00718946 Val Loss: 0.00703948\n",
      "Epoch:  82 Train Loss: 0.00733228 Val Loss: 0.00698177\n",
      "Epoch:  83 Train Loss: 0.00821334 Val Loss: 0.00712136\n",
      "Epoch:  84 Train Loss: 0.00779130 Val Loss: 0.00709289\n",
      "Epoch:  85 Train Loss: 0.00738415 Val Loss: 0.00697250\n",
      "Epoch:  86 Train Loss: 0.00792543 Val Loss: 0.00708720\n",
      "Epoch:  87 Train Loss: 0.00795826 Val Loss: 0.00699535\n",
      "Epoch:  88 Train Loss: 0.00831711 Val Loss: 0.00693044\n",
      "Epoch:  89 Train Loss: 0.00734126 Val Loss: 0.00700653\n",
      "Epoch:  90 Train Loss: 0.00758040 Val Loss: 0.00704930\n",
      "Epoch:  91 Train Loss: 0.00839619 Val Loss: 0.00754926\n",
      "Epoch:  92 Train Loss: 0.01050016 Val Loss: 0.00696507\n",
      "Epoch:  93 Train Loss: 0.00725602 Val Loss: 0.00697046\n",
      "Epoch:  94 Train Loss: 0.00765865 Val Loss: 0.00702937\n",
      "Epoch:  95 Train Loss: 0.01062697 Val Loss: 0.00711207\n",
      "Epoch:  96 Train Loss: 0.00925991 Val Loss: 0.00713734\n",
      "Epoch:  97 Train Loss: 0.00840287 Val Loss: 0.00763284\n",
      "Epoch:  98 Train Loss: 0.00753069 Val Loss: 0.00819951\n",
      "Epoch:  99 Train Loss: 0.00820647 Val Loss: 0.00701800\n",
      "Epoch: 100 Train Loss: 0.00771788 Val Loss: 0.00726994\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.08226890 Val Loss: 0.03145038\n",
      "Epoch:   2 Train Loss: 0.04740947 Val Loss: 0.04334490\n",
      "Epoch:   3 Train Loss: 0.03684036 Val Loss: 0.00999803\n",
      "Epoch:   4 Train Loss: 0.01348805 Val Loss: 0.00822726\n",
      "Epoch:   5 Train Loss: 0.01059591 Val Loss: 0.01120132\n",
      "Epoch:   6 Train Loss: 0.01181079 Val Loss: 0.00828347\n",
      "Epoch:   7 Train Loss: 0.00950054 Val Loss: 0.00801169\n",
      "Epoch:   8 Train Loss: 0.00813556 Val Loss: 0.00778961\n",
      "Epoch:   9 Train Loss: 0.01171470 Val Loss: 0.01329452\n",
      "Epoch:  10 Train Loss: 0.00916925 Val Loss: 0.00786052\n",
      "Epoch:  11 Train Loss: 0.00977283 Val Loss: 0.00769765\n",
      "Epoch:  12 Train Loss: 0.00847350 Val Loss: 0.01046938\n",
      "Epoch:  13 Train Loss: 0.00918920 Val Loss: 0.01022247\n",
      "Epoch:  14 Train Loss: 0.00896440 Val Loss: 0.01268993\n",
      "Epoch:  15 Train Loss: 0.01159207 Val Loss: 0.00804720\n",
      "Epoch:  16 Train Loss: 0.01372408 Val Loss: 0.01275193\n",
      "Epoch:  17 Train Loss: 0.00853028 Val Loss: 0.00885175\n",
      "Epoch:  18 Train Loss: 0.00977308 Val Loss: 0.00770032\n",
      "Epoch:  19 Train Loss: 0.00841744 Val Loss: 0.01306890\n",
      "Epoch:  20 Train Loss: 0.01011202 Val Loss: 0.01026850\n",
      "Epoch:  21 Train Loss: 0.00978697 Val Loss: 0.00768935\n",
      "Epoch:  22 Train Loss: 0.01077632 Val Loss: 0.00996398\n",
      "Epoch:  23 Train Loss: 0.00991793 Val Loss: 0.00808905\n",
      "Epoch:  24 Train Loss: 0.00889221 Val Loss: 0.00779445\n",
      "Epoch:  25 Train Loss: 0.00871474 Val Loss: 0.00779696\n",
      "Epoch:  26 Train Loss: 0.00761514 Val Loss: 0.00880178\n",
      "Epoch:  27 Train Loss: 0.00827633 Val Loss: 0.00816942\n",
      "Epoch:  28 Train Loss: 0.00868499 Val Loss: 0.00764059\n",
      "Epoch:  29 Train Loss: 0.00768398 Val Loss: 0.00856798\n",
      "Epoch:  30 Train Loss: 0.00876043 Val Loss: 0.00820978\n",
      "Epoch:  31 Train Loss: 0.00977562 Val Loss: 0.00837166\n",
      "Epoch:  32 Train Loss: 0.01029780 Val Loss: 0.00865106\n",
      "Epoch:  33 Train Loss: 0.01142615 Val Loss: 0.01552405\n",
      "Epoch:  34 Train Loss: 0.01302670 Val Loss: 0.00885633\n",
      "Epoch:  35 Train Loss: 0.01133869 Val Loss: 0.00834578\n",
      "Epoch:  36 Train Loss: 0.01003507 Val Loss: 0.00954414\n",
      "Epoch:  37 Train Loss: 0.00894253 Val Loss: 0.01222829\n",
      "Epoch:  38 Train Loss: 0.01278247 Val Loss: 0.00857712\n",
      "Epoch:  39 Train Loss: 0.00905250 Val Loss: 0.00782820\n",
      "Epoch:  40 Train Loss: 0.00767445 Val Loss: 0.00958473\n",
      "Epoch:  41 Train Loss: 0.01054088 Val Loss: 0.00871167\n",
      "Epoch:  42 Train Loss: 0.00908903 Val Loss: 0.00755782\n",
      "Epoch:  43 Train Loss: 0.00789347 Val Loss: 0.00756182\n",
      "Epoch:  44 Train Loss: 0.00881183 Val Loss: 0.00799545\n",
      "Epoch:  45 Train Loss: 0.00834444 Val Loss: 0.00813216\n",
      "Epoch:  46 Train Loss: 0.00856728 Val Loss: 0.00767892\n",
      "Epoch:  47 Train Loss: 0.00847000 Val Loss: 0.00876391\n",
      "Epoch:  48 Train Loss: 0.00955749 Val Loss: 0.00779522\n",
      "Epoch:  49 Train Loss: 0.00823760 Val Loss: 0.00793660\n",
      "Epoch:  50 Train Loss: 0.00810060 Val Loss: 0.01078301\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.10381742 Val Loss: 0.06641684\n",
      "Epoch:   2 Train Loss: 0.07258059 Val Loss: 0.05015687\n",
      "Epoch:   3 Train Loss: 0.05434043 Val Loss: 0.03135843\n",
      "Epoch:   4 Train Loss: 0.03847616 Val Loss: 0.02425744\n",
      "Epoch:   5 Train Loss: 0.02774798 Val Loss: 0.01839952\n",
      "Epoch:   6 Train Loss: 0.01777881 Val Loss: 0.01010866\n",
      "Epoch:   7 Train Loss: 0.01137151 Val Loss: 0.01296949\n",
      "Epoch:   8 Train Loss: 0.01127414 Val Loss: 0.01174391\n",
      "Epoch:   9 Train Loss: 0.01220946 Val Loss: 0.01054705\n",
      "Epoch:  10 Train Loss: 0.01160466 Val Loss: 0.00953530\n",
      "Epoch:  11 Train Loss: 0.01063854 Val Loss: 0.00849071\n",
      "Epoch:  12 Train Loss: 0.00987022 Val Loss: 0.00819195\n",
      "Epoch:  13 Train Loss: 0.00935143 Val Loss: 0.00929587\n",
      "Epoch:  14 Train Loss: 0.00955342 Val Loss: 0.00921065\n",
      "Epoch:  15 Train Loss: 0.01188975 Val Loss: 0.01048115\n",
      "Epoch:  16 Train Loss: 0.00902219 Val Loss: 0.00796860\n",
      "Epoch:  17 Train Loss: 0.00863102 Val Loss: 0.00798486\n",
      "Epoch:  18 Train Loss: 0.00834148 Val Loss: 0.00796889\n",
      "Epoch:  19 Train Loss: 0.00913137 Val Loss: 0.00800776\n",
      "Epoch:  20 Train Loss: 0.00920204 Val Loss: 0.00923160\n",
      "Epoch:  21 Train Loss: 0.01105958 Val Loss: 0.00793312\n",
      "Epoch:  22 Train Loss: 0.00877712 Val Loss: 0.00778807\n",
      "Epoch:  23 Train Loss: 0.00889253 Val Loss: 0.00783391\n",
      "Epoch:  24 Train Loss: 0.00903656 Val Loss: 0.00839550\n",
      "Epoch:  25 Train Loss: 0.00906941 Val Loss: 0.00983347\n",
      "Epoch:  26 Train Loss: 0.01179606 Val Loss: 0.00852248\n",
      "Epoch:  27 Train Loss: 0.00889722 Val Loss: 0.00827825\n",
      "Epoch:  28 Train Loss: 0.00956061 Val Loss: 0.00816742\n",
      "Epoch:  29 Train Loss: 0.00968903 Val Loss: 0.00768287\n",
      "Epoch:  30 Train Loss: 0.00889417 Val Loss: 0.00908821\n",
      "Epoch:  31 Train Loss: 0.00787797 Val Loss: 0.00769232\n",
      "Epoch:  32 Train Loss: 0.00855047 Val Loss: 0.00785111\n",
      "Epoch:  33 Train Loss: 0.00884193 Val Loss: 0.00887629\n",
      "Epoch:  34 Train Loss: 0.00863483 Val Loss: 0.00758801\n",
      "Epoch:  35 Train Loss: 0.00900918 Val Loss: 0.00753630\n",
      "Epoch:  36 Train Loss: 0.00811600 Val Loss: 0.00779571\n",
      "Epoch:  37 Train Loss: 0.00941999 Val Loss: 0.00824966\n",
      "Epoch:  38 Train Loss: 0.01065816 Val Loss: 0.00931545\n",
      "Epoch:  39 Train Loss: 0.01034982 Val Loss: 0.00890350\n",
      "Epoch:  40 Train Loss: 0.01097910 Val Loss: 0.00807878\n",
      "Epoch:  41 Train Loss: 0.00916319 Val Loss: 0.00975471\n",
      "Epoch:  42 Train Loss: 0.01041705 Val Loss: 0.01030738\n",
      "Epoch:  43 Train Loss: 0.00849671 Val Loss: 0.00753434\n",
      "Epoch:  44 Train Loss: 0.00809324 Val Loss: 0.00772862\n",
      "Epoch:  45 Train Loss: 0.00730233 Val Loss: 0.00753426\n",
      "Epoch:  46 Train Loss: 0.00790616 Val Loss: 0.00818080\n",
      "Epoch:  47 Train Loss: 0.00911446 Val Loss: 0.00818646\n",
      "Epoch:  48 Train Loss: 0.00770273 Val Loss: 0.00840551\n",
      "Epoch:  49 Train Loss: 0.00883018 Val Loss: 0.00762571\n",
      "Epoch:  50 Train Loss: 0.00857812 Val Loss: 0.00947754\n",
      "Epoch:  51 Train Loss: 0.00922293 Val Loss: 0.00739904\n",
      "Epoch:  52 Train Loss: 0.00729489 Val Loss: 0.00764999\n",
      "Epoch:  53 Train Loss: 0.00663250 Val Loss: 0.00737118\n",
      "Epoch:  54 Train Loss: 0.00786700 Val Loss: 0.00726724\n",
      "Epoch:  55 Train Loss: 0.00770165 Val Loss: 0.00813784\n",
      "Epoch:  56 Train Loss: 0.00843260 Val Loss: 0.00716870\n",
      "Epoch:  57 Train Loss: 0.00730057 Val Loss: 0.00740865\n",
      "Epoch:  58 Train Loss: 0.00727277 Val Loss: 0.00738995\n",
      "Epoch:  59 Train Loss: 0.00832919 Val Loss: 0.00810197\n",
      "Epoch:  60 Train Loss: 0.00876177 Val Loss: 0.00761027\n",
      "Epoch:  61 Train Loss: 0.00757095 Val Loss: 0.00790782\n",
      "Epoch:  62 Train Loss: 0.00852286 Val Loss: 0.00762695\n",
      "Epoch:  63 Train Loss: 0.00727626 Val Loss: 0.00852592\n",
      "Epoch:  64 Train Loss: 0.01104715 Val Loss: 0.00956692\n",
      "Epoch:  65 Train Loss: 0.00875521 Val Loss: 0.00818379\n",
      "Epoch:  66 Train Loss: 0.00891666 Val Loss: 0.00783729\n",
      "Epoch:  67 Train Loss: 0.00667494 Val Loss: 0.00714471\n",
      "Epoch:  68 Train Loss: 0.00740501 Val Loss: 0.00823633\n",
      "Epoch:  69 Train Loss: 0.01009171 Val Loss: 0.00687093\n",
      "Epoch:  70 Train Loss: 0.00971924 Val Loss: 0.00694776\n",
      "Epoch:  71 Train Loss: 0.00696924 Val Loss: 0.01001161\n",
      "Epoch:  72 Train Loss: 0.00782482 Val Loss: 0.00797779\n",
      "Epoch:  73 Train Loss: 0.00898618 Val Loss: 0.00877396\n",
      "Epoch:  74 Train Loss: 0.00716679 Val Loss: 0.00946269\n",
      "Epoch:  75 Train Loss: 0.00762728 Val Loss: 0.00908478\n",
      "Epoch:  76 Train Loss: 0.00692961 Val Loss: 0.00713918\n",
      "Epoch:  77 Train Loss: 0.00750292 Val Loss: 0.00888403\n",
      "Epoch:  78 Train Loss: 0.00826267 Val Loss: 0.00765162\n",
      "Epoch:  79 Train Loss: 0.00881065 Val Loss: 0.00740987\n",
      "Epoch:  80 Train Loss: 0.00739315 Val Loss: 0.00813471\n",
      "Epoch:  81 Train Loss: 0.00741195 Val Loss: 0.00735711\n",
      "Epoch:  82 Train Loss: 0.00859092 Val Loss: 0.00715045\n",
      "Epoch:  83 Train Loss: 0.00636212 Val Loss: 0.00660213\n",
      "Epoch:  84 Train Loss: 0.00703307 Val Loss: 0.00675528\n",
      "Epoch:  85 Train Loss: 0.00774019 Val Loss: 0.00843923\n",
      "Epoch:  86 Train Loss: 0.00807287 Val Loss: 0.00766607\n",
      "Epoch:  87 Train Loss: 0.01010338 Val Loss: 0.01125043\n",
      "Epoch:  88 Train Loss: 0.00995227 Val Loss: 0.00791326\n",
      "Epoch:  89 Train Loss: 0.00706297 Val Loss: 0.00739312\n",
      "Epoch:  90 Train Loss: 0.00747865 Val Loss: 0.00689045\n",
      "Epoch:  91 Train Loss: 0.00757450 Val Loss: 0.00680708\n",
      "Epoch:  92 Train Loss: 0.00861813 Val Loss: 0.01182934\n",
      "Epoch:  93 Train Loss: 0.00841988 Val Loss: 0.00794361\n",
      "Epoch:  94 Train Loss: 0.00756758 Val Loss: 0.00893674\n",
      "Epoch:  95 Train Loss: 0.00852878 Val Loss: 0.01090312\n",
      "Epoch:  96 Train Loss: 0.00985572 Val Loss: 0.00796996\n",
      "Epoch:  97 Train Loss: 0.00979926 Val Loss: 0.00807066\n",
      "Epoch:  98 Train Loss: 0.00719936 Val Loss: 0.00882817\n",
      "Epoch:  99 Train Loss: 0.00877288 Val Loss: 0.00886909\n",
      "Epoch: 100 Train Loss: 0.00903343 Val Loss: 0.00767035\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.13949259 Val Loss: 0.09351515\n",
      "Epoch:   2 Train Loss: 0.07478478 Val Loss: 0.04434142\n",
      "Epoch:   3 Train Loss: 0.05876769 Val Loss: 0.03195749\n",
      "Epoch:   4 Train Loss: 0.05257948 Val Loss: 0.03071450\n",
      "Epoch:   5 Train Loss: 0.05371303 Val Loss: 0.02936632\n",
      "Epoch:   6 Train Loss: 0.04843433 Val Loss: 0.02961410\n",
      "Epoch:   7 Train Loss: 0.04403779 Val Loss: 0.02586330\n",
      "Epoch:   8 Train Loss: 0.03886824 Val Loss: 0.02082396\n",
      "Epoch:   9 Train Loss: 0.03046350 Val Loss: 0.01561692\n",
      "Epoch:  10 Train Loss: 0.02247027 Val Loss: 0.01039577\n",
      "Epoch:  11 Train Loss: 0.01241343 Val Loss: 0.00817004\n",
      "Epoch:  12 Train Loss: 0.01169570 Val Loss: 0.01617941\n",
      "Epoch:  13 Train Loss: 0.01635487 Val Loss: 0.00771440\n",
      "Epoch:  14 Train Loss: 0.01146591 Val Loss: 0.00868734\n",
      "Epoch:  15 Train Loss: 0.01450116 Val Loss: 0.00922162\n",
      "Epoch:  16 Train Loss: 0.01238414 Val Loss: 0.00750947\n",
      "Epoch:  17 Train Loss: 0.00957261 Val Loss: 0.01040043\n",
      "Epoch:  18 Train Loss: 0.01143134 Val Loss: 0.00747044\n",
      "Epoch:  19 Train Loss: 0.01148854 Val Loss: 0.00881735\n",
      "Epoch:  20 Train Loss: 0.01231487 Val Loss: 0.00781767\n",
      "Epoch:  21 Train Loss: 0.01235393 Val Loss: 0.00738025\n",
      "Epoch:  22 Train Loss: 0.01001379 Val Loss: 0.00880616\n",
      "Epoch:  23 Train Loss: 0.00832751 Val Loss: 0.00725441\n",
      "Epoch:  24 Train Loss: 0.00788359 Val Loss: 0.00750802\n",
      "Epoch:  25 Train Loss: 0.01129253 Val Loss: 0.00749752\n",
      "Epoch:  26 Train Loss: 0.00882670 Val Loss: 0.00721024\n",
      "Epoch:  27 Train Loss: 0.00951562 Val Loss: 0.00827169\n",
      "Epoch:  28 Train Loss: 0.01064369 Val Loss: 0.00737973\n",
      "Epoch:  29 Train Loss: 0.01018130 Val Loss: 0.00721671\n",
      "Epoch:  30 Train Loss: 0.01002126 Val Loss: 0.00716052\n",
      "Epoch:  31 Train Loss: 0.01089865 Val Loss: 0.00829580\n",
      "Epoch:  32 Train Loss: 0.00958563 Val Loss: 0.00772583\n",
      "Epoch:  33 Train Loss: 0.01021171 Val Loss: 0.00727236\n",
      "Epoch:  34 Train Loss: 0.00917614 Val Loss: 0.00764288\n",
      "Epoch:  35 Train Loss: 0.00846262 Val Loss: 0.00710324\n",
      "Epoch:  36 Train Loss: 0.00814948 Val Loss: 0.00786675\n",
      "Epoch:  37 Train Loss: 0.00887418 Val Loss: 0.00749663\n",
      "Epoch:  38 Train Loss: 0.00986348 Val Loss: 0.00722588\n",
      "Epoch:  39 Train Loss: 0.01061254 Val Loss: 0.00729625\n",
      "Epoch:  40 Train Loss: 0.00835111 Val Loss: 0.00707787\n",
      "Epoch:  41 Train Loss: 0.00833540 Val Loss: 0.00747369\n",
      "Epoch:  42 Train Loss: 0.01015823 Val Loss: 0.00745147\n",
      "Epoch:  43 Train Loss: 0.00770473 Val Loss: 0.00705684\n",
      "Epoch:  44 Train Loss: 0.00801623 Val Loss: 0.00720571\n",
      "Epoch:  45 Train Loss: 0.00937471 Val Loss: 0.00712791\n",
      "Epoch:  46 Train Loss: 0.01076285 Val Loss: 0.00718792\n",
      "Epoch:  47 Train Loss: 0.01002331 Val Loss: 0.00704325\n",
      "Epoch:  48 Train Loss: 0.00873556 Val Loss: 0.00760307\n",
      "Epoch:  49 Train Loss: 0.00798835 Val Loss: 0.00722327\n",
      "Epoch:  50 Train Loss: 0.00719092 Val Loss: 0.00774126\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.27250316 Val Loss: 0.24178768\n",
      "Epoch:   2 Train Loss: 0.19415800 Val Loss: 0.13875286\n",
      "Epoch:   3 Train Loss: 0.10761459 Val Loss: 0.04885691\n",
      "Epoch:   4 Train Loss: 0.06833814 Val Loss: 0.03822670\n",
      "Epoch:   5 Train Loss: 0.07084547 Val Loss: 0.03211724\n",
      "Epoch:   6 Train Loss: 0.05496443 Val Loss: 0.03666456\n",
      "Epoch:   7 Train Loss: 0.05471582 Val Loss: 0.03618416\n",
      "Epoch:   8 Train Loss: 0.05432452 Val Loss: 0.02827509\n",
      "Epoch:   9 Train Loss: 0.04516734 Val Loss: 0.02481402\n",
      "Epoch:  10 Train Loss: 0.03837539 Val Loss: 0.02271105\n",
      "Epoch:  11 Train Loss: 0.03538682 Val Loss: 0.02138196\n",
      "Epoch:  12 Train Loss: 0.02660063 Val Loss: 0.01227154\n",
      "Epoch:  13 Train Loss: 0.01741442 Val Loss: 0.00923368\n",
      "Epoch:  14 Train Loss: 0.01475114 Val Loss: 0.00978633\n",
      "Epoch:  15 Train Loss: 0.01638787 Val Loss: 0.01030485\n",
      "Epoch:  16 Train Loss: 0.01698712 Val Loss: 0.00951676\n",
      "Epoch:  17 Train Loss: 0.01214786 Val Loss: 0.00874041\n",
      "Epoch:  18 Train Loss: 0.01123743 Val Loss: 0.00773923\n",
      "Epoch:  19 Train Loss: 0.01500665 Val Loss: 0.00928960\n",
      "Epoch:  20 Train Loss: 0.01109586 Val Loss: 0.00851457\n",
      "Epoch:  21 Train Loss: 0.01278147 Val Loss: 0.00741094\n",
      "Epoch:  22 Train Loss: 0.01296929 Val Loss: 0.01153969\n",
      "Epoch:  23 Train Loss: 0.01308050 Val Loss: 0.00749219\n",
      "Epoch:  24 Train Loss: 0.01002379 Val Loss: 0.00848315\n",
      "Epoch:  25 Train Loss: 0.01241606 Val Loss: 0.00751988\n",
      "Epoch:  26 Train Loss: 0.01431501 Val Loss: 0.00791148\n",
      "Epoch:  27 Train Loss: 0.01155818 Val Loss: 0.00968795\n",
      "Epoch:  28 Train Loss: 0.01228817 Val Loss: 0.00779605\n",
      "Epoch:  29 Train Loss: 0.01249325 Val Loss: 0.01184853\n",
      "Epoch:  30 Train Loss: 0.01300624 Val Loss: 0.00725176\n",
      "Epoch:  31 Train Loss: 0.01166323 Val Loss: 0.00722989\n",
      "Epoch:  32 Train Loss: 0.01281506 Val Loss: 0.00760420\n",
      "Epoch:  33 Train Loss: 0.00999405 Val Loss: 0.00724140\n",
      "Epoch:  34 Train Loss: 0.01274083 Val Loss: 0.00857436\n",
      "Epoch:  35 Train Loss: 0.00988213 Val Loss: 0.00748582\n",
      "Epoch:  36 Train Loss: 0.00992449 Val Loss: 0.00927506\n",
      "Epoch:  37 Train Loss: 0.00940897 Val Loss: 0.00719840\n",
      "Epoch:  38 Train Loss: 0.01303318 Val Loss: 0.00940686\n",
      "Epoch:  39 Train Loss: 0.01205737 Val Loss: 0.00746052\n",
      "Epoch:  40 Train Loss: 0.01286781 Val Loss: 0.00723068\n",
      "Epoch:  41 Train Loss: 0.01489088 Val Loss: 0.01093221\n",
      "Epoch:  42 Train Loss: 0.01154314 Val Loss: 0.00758765\n",
      "Epoch:  43 Train Loss: 0.01143778 Val Loss: 0.00830736\n",
      "Epoch:  44 Train Loss: 0.01292581 Val Loss: 0.00716024\n",
      "Epoch:  45 Train Loss: 0.01172354 Val Loss: 0.00736484\n",
      "Epoch:  46 Train Loss: 0.01057016 Val Loss: 0.00760482\n",
      "Epoch:  47 Train Loss: 0.01071087 Val Loss: 0.00719234\n",
      "Epoch:  48 Train Loss: 0.01209227 Val Loss: 0.00719843\n",
      "Epoch:  49 Train Loss: 0.01008791 Val Loss: 0.00722192\n",
      "Epoch:  50 Train Loss: 0.01037984 Val Loss: 0.00729673\n",
      "Epoch:  51 Train Loss: 0.01011404 Val Loss: 0.00742024\n",
      "Epoch:  52 Train Loss: 0.00908961 Val Loss: 0.00770682\n",
      "Epoch:  53 Train Loss: 0.00897997 Val Loss: 0.00737608\n",
      "Epoch:  54 Train Loss: 0.00944028 Val Loss: 0.00720483\n",
      "Epoch:  55 Train Loss: 0.01010346 Val Loss: 0.00728039\n",
      "Epoch:  56 Train Loss: 0.01022795 Val Loss: 0.00714588\n",
      "Epoch:  57 Train Loss: 0.00981280 Val Loss: 0.00713120\n",
      "Epoch:  58 Train Loss: 0.01180371 Val Loss: 0.00717349\n",
      "Epoch:  59 Train Loss: 0.01004905 Val Loss: 0.00714799\n",
      "Epoch:  60 Train Loss: 0.01147823 Val Loss: 0.00746939\n",
      "Epoch:  61 Train Loss: 0.01212501 Val Loss: 0.00766528\n",
      "Epoch:  62 Train Loss: 0.01049408 Val Loss: 0.00755514\n",
      "Epoch:  63 Train Loss: 0.00957812 Val Loss: 0.00725929\n",
      "Epoch:  64 Train Loss: 0.01045785 Val Loss: 0.00934609\n",
      "Epoch:  65 Train Loss: 0.00926057 Val Loss: 0.00785321\n",
      "Epoch:  66 Train Loss: 0.00986659 Val Loss: 0.00816757\n",
      "Epoch:  67 Train Loss: 0.01064487 Val Loss: 0.00709064\n",
      "Epoch:  68 Train Loss: 0.00978734 Val Loss: 0.00711697\n",
      "Epoch:  69 Train Loss: 0.00910918 Val Loss: 0.00713943\n",
      "Epoch:  70 Train Loss: 0.01121136 Val Loss: 0.00825463\n",
      "Epoch:  71 Train Loss: 0.01071465 Val Loss: 0.00710182\n",
      "Epoch:  72 Train Loss: 0.00984569 Val Loss: 0.00745135\n",
      "Epoch:  73 Train Loss: 0.01167224 Val Loss: 0.00737896\n",
      "Epoch:  74 Train Loss: 0.01029653 Val Loss: 0.00706936\n",
      "Epoch:  75 Train Loss: 0.01128628 Val Loss: 0.00748226\n",
      "Epoch:  76 Train Loss: 0.00948224 Val Loss: 0.00723788\n",
      "Epoch:  77 Train Loss: 0.00942514 Val Loss: 0.00743061\n",
      "Epoch:  78 Train Loss: 0.01045752 Val Loss: 0.00725147\n",
      "Epoch:  79 Train Loss: 0.00945676 Val Loss: 0.00716972\n",
      "Epoch:  80 Train Loss: 0.01071857 Val Loss: 0.00755014\n",
      "Epoch:  81 Train Loss: 0.00924557 Val Loss: 0.00722700\n",
      "Epoch:  82 Train Loss: 0.01065360 Val Loss: 0.00863816\n",
      "Epoch:  83 Train Loss: 0.01084954 Val Loss: 0.00704611\n",
      "Epoch:  84 Train Loss: 0.00916469 Val Loss: 0.00750278\n",
      "Epoch:  85 Train Loss: 0.01036878 Val Loss: 0.00711286\n",
      "Epoch:  86 Train Loss: 0.00958366 Val Loss: 0.00704656\n",
      "Epoch:  87 Train Loss: 0.00948148 Val Loss: 0.00822390\n",
      "Epoch:  88 Train Loss: 0.00969314 Val Loss: 0.00739441\n",
      "Epoch:  89 Train Loss: 0.00895315 Val Loss: 0.00748684\n",
      "Epoch:  90 Train Loss: 0.00983914 Val Loss: 0.00744713\n",
      "Epoch:  91 Train Loss: 0.01143069 Val Loss: 0.00761715\n",
      "Epoch:  92 Train Loss: 0.00865889 Val Loss: 0.00748511\n",
      "Epoch:  93 Train Loss: 0.01055441 Val Loss: 0.00783016\n",
      "Epoch:  94 Train Loss: 0.00917029 Val Loss: 0.00703431\n",
      "Epoch:  95 Train Loss: 0.01125519 Val Loss: 0.00788516\n",
      "Epoch:  96 Train Loss: 0.01101752 Val Loss: 0.00700414\n",
      "Epoch:  97 Train Loss: 0.01261051 Val Loss: 0.00699283\n",
      "Epoch:  98 Train Loss: 0.01038337 Val Loss: 0.00870669\n",
      "Epoch:  99 Train Loss: 0.01072740 Val Loss: 0.00882624\n",
      "Epoch: 100 Train Loss: 0.00975402 Val Loss: 0.00917029\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.06916574 Val Loss: 0.06200259\n",
      "Epoch:   2 Train Loss: 0.05708591 Val Loss: 0.02506668\n",
      "Epoch:   3 Train Loss: 0.03573518 Val Loss: 0.01803620\n",
      "Epoch:   4 Train Loss: 0.01628286 Val Loss: 0.02270469\n",
      "Epoch:   5 Train Loss: 0.02286103 Val Loss: 0.01471384\n",
      "Epoch:   6 Train Loss: 0.01549445 Val Loss: 0.00944358\n",
      "Epoch:   7 Train Loss: 0.01125168 Val Loss: 0.01058479\n",
      "Epoch:   8 Train Loss: 0.01332456 Val Loss: 0.01168074\n",
      "Epoch:   9 Train Loss: 0.01124998 Val Loss: 0.01106760\n",
      "Epoch:  10 Train Loss: 0.01263674 Val Loss: 0.00941967\n",
      "Epoch:  11 Train Loss: 0.00924313 Val Loss: 0.00932449\n",
      "Epoch:  12 Train Loss: 0.00900778 Val Loss: 0.00838210\n",
      "Epoch:  13 Train Loss: 0.01000244 Val Loss: 0.00972266\n",
      "Epoch:  14 Train Loss: 0.01025222 Val Loss: 0.00881935\n",
      "Epoch:  15 Train Loss: 0.01027961 Val Loss: 0.00863814\n",
      "Epoch:  16 Train Loss: 0.01164473 Val Loss: 0.00975727\n",
      "Epoch:  17 Train Loss: 0.01041406 Val Loss: 0.01163753\n",
      "Epoch:  18 Train Loss: 0.01178200 Val Loss: 0.01162240\n",
      "Epoch:  19 Train Loss: 0.01059875 Val Loss: 0.00914356\n",
      "Epoch:  20 Train Loss: 0.01020403 Val Loss: 0.01017059\n",
      "Epoch:  21 Train Loss: 0.01178197 Val Loss: 0.00855007\n",
      "Epoch:  22 Train Loss: 0.01060712 Val Loss: 0.00812823\n",
      "Epoch:  23 Train Loss: 0.00928949 Val Loss: 0.00920528\n",
      "Epoch:  24 Train Loss: 0.01115636 Val Loss: 0.00861699\n",
      "Epoch:  25 Train Loss: 0.01161383 Val Loss: 0.00998000\n",
      "Epoch:  26 Train Loss: 0.00941406 Val Loss: 0.00783634\n",
      "Epoch:  27 Train Loss: 0.00830422 Val Loss: 0.00851687\n",
      "Epoch:  28 Train Loss: 0.00813203 Val Loss: 0.00820616\n",
      "Epoch:  29 Train Loss: 0.00887290 Val Loss: 0.00765552\n",
      "Epoch:  30 Train Loss: 0.01012033 Val Loss: 0.00866025\n",
      "Epoch:  31 Train Loss: 0.01043064 Val Loss: 0.00787808\n",
      "Epoch:  32 Train Loss: 0.00829902 Val Loss: 0.01293682\n",
      "Epoch:  33 Train Loss: 0.01066580 Val Loss: 0.00961052\n",
      "Epoch:  34 Train Loss: 0.01021687 Val Loss: 0.00799053\n",
      "Epoch:  35 Train Loss: 0.00786225 Val Loss: 0.00795314\n",
      "Epoch:  36 Train Loss: 0.00821176 Val Loss: 0.01115865\n",
      "Epoch:  37 Train Loss: 0.00919360 Val Loss: 0.00752156\n",
      "Epoch:  38 Train Loss: 0.01014243 Val Loss: 0.00867590\n",
      "Epoch:  39 Train Loss: 0.01345760 Val Loss: 0.00877567\n",
      "Epoch:  40 Train Loss: 0.00872858 Val Loss: 0.00929029\n",
      "Epoch:  41 Train Loss: 0.00780384 Val Loss: 0.00804137\n",
      "Epoch:  42 Train Loss: 0.00929926 Val Loss: 0.00979355\n",
      "Epoch:  43 Train Loss: 0.00976397 Val Loss: 0.00965945\n",
      "Epoch:  44 Train Loss: 0.00887543 Val Loss: 0.00887658\n",
      "Epoch:  45 Train Loss: 0.00904556 Val Loss: 0.00835624\n",
      "Epoch:  46 Train Loss: 0.00762139 Val Loss: 0.00772459\n",
      "Epoch:  47 Train Loss: 0.00855804 Val Loss: 0.00792539\n",
      "Epoch:  48 Train Loss: 0.00831409 Val Loss: 0.00817247\n",
      "Epoch:  49 Train Loss: 0.00975059 Val Loss: 0.00803635\n",
      "Epoch:  50 Train Loss: 0.00897178 Val Loss: 0.00767832\n",
      "\n",
      "Training with parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.08050788 Val Loss: 0.05483429\n",
      "Epoch:   2 Train Loss: 0.05145038 Val Loss: 0.03053631\n",
      "Epoch:   3 Train Loss: 0.03250672 Val Loss: 0.01275571\n",
      "Epoch:   4 Train Loss: 0.01402927 Val Loss: 0.01158166\n",
      "Epoch:   5 Train Loss: 0.01139928 Val Loss: 0.00938652\n",
      "Epoch:   6 Train Loss: 0.01516325 Val Loss: 0.00846187\n",
      "Epoch:   7 Train Loss: 0.01055644 Val Loss: 0.01378077\n",
      "Epoch:   8 Train Loss: 0.01316157 Val Loss: 0.00852761\n",
      "Epoch:   9 Train Loss: 0.01050602 Val Loss: 0.00979368\n",
      "Epoch:  10 Train Loss: 0.00940576 Val Loss: 0.00913570\n",
      "Epoch:  11 Train Loss: 0.01105969 Val Loss: 0.00938294\n",
      "Epoch:  12 Train Loss: 0.01121810 Val Loss: 0.00808506\n",
      "Epoch:  13 Train Loss: 0.01143178 Val Loss: 0.01130492\n",
      "Epoch:  14 Train Loss: 0.01052397 Val Loss: 0.00832513\n",
      "Epoch:  15 Train Loss: 0.01115672 Val Loss: 0.00808353\n",
      "Epoch:  16 Train Loss: 0.01103670 Val Loss: 0.00922365\n",
      "Epoch:  17 Train Loss: 0.00873415 Val Loss: 0.00813134\n",
      "Epoch:  18 Train Loss: 0.01022965 Val Loss: 0.00937725\n",
      "Epoch:  19 Train Loss: 0.01068402 Val Loss: 0.01152398\n",
      "Epoch:  20 Train Loss: 0.01018839 Val Loss: 0.00913302\n",
      "Epoch:  21 Train Loss: 0.01072895 Val Loss: 0.00794190\n",
      "Epoch:  22 Train Loss: 0.01154698 Val Loss: 0.00857374\n",
      "Epoch:  23 Train Loss: 0.00995976 Val Loss: 0.00802767\n",
      "Epoch:  24 Train Loss: 0.00913505 Val Loss: 0.00829428\n",
      "Epoch:  25 Train Loss: 0.00966243 Val Loss: 0.01198079\n",
      "Epoch:  26 Train Loss: 0.01040769 Val Loss: 0.00847114\n",
      "Epoch:  27 Train Loss: 0.00861815 Val Loss: 0.00773156\n",
      "Epoch:  28 Train Loss: 0.00799508 Val Loss: 0.00744185\n",
      "Epoch:  29 Train Loss: 0.00920646 Val Loss: 0.00806650\n",
      "Epoch:  30 Train Loss: 0.00930308 Val Loss: 0.00741498\n",
      "Epoch:  31 Train Loss: 0.00849622 Val Loss: 0.00774837\n",
      "Epoch:  32 Train Loss: 0.00965229 Val Loss: 0.00783210\n",
      "Epoch:  33 Train Loss: 0.00898164 Val Loss: 0.01298281\n",
      "Epoch:  34 Train Loss: 0.01388612 Val Loss: 0.00753951\n",
      "Epoch:  35 Train Loss: 0.01022095 Val Loss: 0.00941578\n",
      "Epoch:  36 Train Loss: 0.01006378 Val Loss: 0.00797021\n",
      "Epoch:  37 Train Loss: 0.00974162 Val Loss: 0.00849907\n",
      "Epoch:  38 Train Loss: 0.01008648 Val Loss: 0.00796945\n",
      "Epoch:  39 Train Loss: 0.00878629 Val Loss: 0.00983276\n",
      "Epoch:  40 Train Loss: 0.00935759 Val Loss: 0.00755846\n",
      "Epoch:  41 Train Loss: 0.01021091 Val Loss: 0.01397508\n",
      "Epoch:  42 Train Loss: 0.01093520 Val Loss: 0.00783079\n",
      "Epoch:  43 Train Loss: 0.00797949 Val Loss: 0.00791241\n",
      "Epoch:  44 Train Loss: 0.00951474 Val Loss: 0.00770595\n",
      "Epoch:  45 Train Loss: 0.00732906 Val Loss: 0.00811594\n",
      "Epoch:  46 Train Loss: 0.00991603 Val Loss: 0.01097034\n",
      "Epoch:  47 Train Loss: 0.00994093 Val Loss: 0.01038723\n",
      "Epoch:  48 Train Loss: 0.01103790 Val Loss: 0.01110769\n",
      "Epoch:  49 Train Loss: 0.01016505 Val Loss: 0.00838419\n",
      "Epoch:  50 Train Loss: 0.01039232 Val Loss: 0.00882641\n",
      "Epoch:  51 Train Loss: 0.01117700 Val Loss: 0.00886178\n",
      "Epoch:  52 Train Loss: 0.00815130 Val Loss: 0.00771422\n",
      "Epoch:  53 Train Loss: 0.00922155 Val Loss: 0.00764605\n",
      "Epoch:  54 Train Loss: 0.00787001 Val Loss: 0.00761641\n",
      "Epoch:  55 Train Loss: 0.00867168 Val Loss: 0.00835747\n",
      "Epoch:  56 Train Loss: 0.00744404 Val Loss: 0.00771367\n",
      "Epoch:  57 Train Loss: 0.00883196 Val Loss: 0.00777575\n",
      "Epoch:  58 Train Loss: 0.00969545 Val Loss: 0.01045840\n",
      "Epoch:  59 Train Loss: 0.01507973 Val Loss: 0.00814352\n",
      "Epoch:  60 Train Loss: 0.00933409 Val Loss: 0.01005327\n",
      "Epoch:  61 Train Loss: 0.01020958 Val Loss: 0.00993047\n",
      "Epoch:  62 Train Loss: 0.00866462 Val Loss: 0.00839968\n",
      "Epoch:  63 Train Loss: 0.00914660 Val Loss: 0.01144362\n",
      "Epoch:  64 Train Loss: 0.01067515 Val Loss: 0.00758214\n",
      "Epoch:  65 Train Loss: 0.00749159 Val Loss: 0.00884244\n",
      "Epoch:  66 Train Loss: 0.00901952 Val Loss: 0.00759902\n",
      "Epoch:  67 Train Loss: 0.00894892 Val Loss: 0.00916573\n",
      "Epoch:  68 Train Loss: 0.00780953 Val Loss: 0.00763435\n",
      "Epoch:  69 Train Loss: 0.00846830 Val Loss: 0.00881032\n",
      "Epoch:  70 Train Loss: 0.00804958 Val Loss: 0.00739709\n",
      "Epoch:  71 Train Loss: 0.00895219 Val Loss: 0.00748360\n",
      "Epoch:  72 Train Loss: 0.00764911 Val Loss: 0.00784767\n",
      "Epoch:  73 Train Loss: 0.00777431 Val Loss: 0.00770138\n",
      "Epoch:  74 Train Loss: 0.00701195 Val Loss: 0.00763702\n",
      "Epoch:  75 Train Loss: 0.00729264 Val Loss: 0.00745632\n",
      "Epoch:  76 Train Loss: 0.00795084 Val Loss: 0.00757818\n",
      "Epoch:  77 Train Loss: 0.00738405 Val Loss: 0.00745299\n",
      "Epoch:  78 Train Loss: 0.00957880 Val Loss: 0.00960511\n",
      "Epoch:  79 Train Loss: 0.00939497 Val Loss: 0.00808517\n",
      "Epoch:  80 Train Loss: 0.00823437 Val Loss: 0.00935404\n",
      "Epoch:  81 Train Loss: 0.00929923 Val Loss: 0.00822569\n",
      "Epoch:  82 Train Loss: 0.00837911 Val Loss: 0.00844000\n",
      "Epoch:  83 Train Loss: 0.00945010 Val Loss: 0.00751065\n",
      "Epoch:  84 Train Loss: 0.00751516 Val Loss: 0.00726137\n",
      "Epoch:  85 Train Loss: 0.00826789 Val Loss: 0.00795186\n",
      "Epoch:  86 Train Loss: 0.00811089 Val Loss: 0.00706850\n",
      "Epoch:  87 Train Loss: 0.00755892 Val Loss: 0.00742067\n",
      "Epoch:  88 Train Loss: 0.00941172 Val Loss: 0.00699254\n",
      "Epoch:  89 Train Loss: 0.00838603 Val Loss: 0.00758694\n",
      "Epoch:  90 Train Loss: 0.00738751 Val Loss: 0.00675222\n",
      "Epoch:  91 Train Loss: 0.00741159 Val Loss: 0.00852038\n",
      "Epoch:  92 Train Loss: 0.00791840 Val Loss: 0.00774884\n",
      "Epoch:  93 Train Loss: 0.00776379 Val Loss: 0.01027179\n",
      "Epoch:  94 Train Loss: 0.00792039 Val Loss: 0.00693242\n",
      "Epoch:  95 Train Loss: 0.00759984 Val Loss: 0.01059079\n",
      "Epoch:  96 Train Loss: 0.00805980 Val Loss: 0.00786230\n",
      "Epoch:  97 Train Loss: 0.00829223 Val Loss: 0.00828282\n",
      "Epoch:  98 Train Loss: 0.00849211 Val Loss: 0.00800081\n",
      "Epoch:  99 Train Loss: 0.00752436 Val Loss: 0.00794193\n",
      "Epoch: 100 Train Loss: 0.00857183 Val Loss: 0.00728247\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.21348075 Val Loss: 0.16992220\n",
      "Epoch:   2 Train Loss: 0.12424573 Val Loss: 0.09019552\n",
      "Epoch:   3 Train Loss: 0.06927724 Val Loss: 0.02730513\n",
      "Epoch:   4 Train Loss: 0.04452709 Val Loss: 0.03064987\n",
      "Epoch:   5 Train Loss: 0.03971210 Val Loss: 0.02395083\n",
      "Epoch:   6 Train Loss: 0.03423305 Val Loss: 0.02419376\n",
      "Epoch:   7 Train Loss: 0.03072213 Val Loss: 0.01943479\n",
      "Epoch:   8 Train Loss: 0.02598092 Val Loss: 0.01680803\n",
      "Epoch:   9 Train Loss: 0.02243129 Val Loss: 0.01422014\n",
      "Epoch:  10 Train Loss: 0.01594563 Val Loss: 0.01120651\n",
      "Epoch:  11 Train Loss: 0.01250975 Val Loss: 0.00873172\n",
      "Epoch:  12 Train Loss: 0.00914367 Val Loss: 0.00887424\n",
      "Epoch:  13 Train Loss: 0.00994029 Val Loss: 0.00797856\n",
      "Epoch:  14 Train Loss: 0.01136384 Val Loss: 0.01318366\n",
      "Epoch:  15 Train Loss: 0.01056153 Val Loss: 0.00786338\n",
      "Epoch:  16 Train Loss: 0.00938596 Val Loss: 0.00909671\n",
      "Epoch:  17 Train Loss: 0.01000036 Val Loss: 0.00965365\n",
      "Epoch:  18 Train Loss: 0.00964169 Val Loss: 0.00813174\n",
      "Epoch:  19 Train Loss: 0.00857589 Val Loss: 0.00873278\n",
      "Epoch:  20 Train Loss: 0.01031993 Val Loss: 0.00855568\n",
      "Epoch:  21 Train Loss: 0.00844762 Val Loss: 0.00812471\n",
      "Epoch:  22 Train Loss: 0.00874025 Val Loss: 0.00899198\n",
      "Epoch:  23 Train Loss: 0.00884519 Val Loss: 0.00793862\n",
      "Epoch:  24 Train Loss: 0.00857053 Val Loss: 0.00844902\n",
      "Epoch:  25 Train Loss: 0.00898042 Val Loss: 0.00745421\n",
      "Epoch:  26 Train Loss: 0.00869391 Val Loss: 0.00950801\n",
      "Epoch:  27 Train Loss: 0.00808140 Val Loss: 0.00818074\n",
      "Epoch:  28 Train Loss: 0.00726817 Val Loss: 0.00771520\n",
      "Epoch:  29 Train Loss: 0.00762091 Val Loss: 0.00736091\n",
      "Epoch:  30 Train Loss: 0.00792951 Val Loss: 0.00849370\n",
      "Epoch:  31 Train Loss: 0.00814115 Val Loss: 0.00715004\n",
      "Epoch:  32 Train Loss: 0.00771421 Val Loss: 0.00762268\n",
      "Epoch:  33 Train Loss: 0.00661313 Val Loss: 0.00714135\n",
      "Epoch:  34 Train Loss: 0.00855156 Val Loss: 0.00724187\n",
      "Epoch:  35 Train Loss: 0.00782997 Val Loss: 0.00815409\n",
      "Epoch:  36 Train Loss: 0.00856169 Val Loss: 0.00746987\n",
      "Epoch:  37 Train Loss: 0.00680829 Val Loss: 0.00721178\n",
      "Epoch:  38 Train Loss: 0.00955967 Val Loss: 0.00718420\n",
      "Epoch:  39 Train Loss: 0.00838141 Val Loss: 0.00714241\n",
      "Epoch:  40 Train Loss: 0.00771061 Val Loss: 0.00740392\n",
      "Epoch:  41 Train Loss: 0.00800370 Val Loss: 0.00692529\n",
      "Epoch:  42 Train Loss: 0.00758285 Val Loss: 0.00735935\n",
      "Epoch:  43 Train Loss: 0.00942537 Val Loss: 0.00753381\n",
      "Epoch:  44 Train Loss: 0.00831851 Val Loss: 0.00709829\n",
      "Epoch:  45 Train Loss: 0.00740713 Val Loss: 0.00693529\n",
      "Epoch:  46 Train Loss: 0.00724991 Val Loss: 0.00731432\n",
      "Epoch:  47 Train Loss: 0.00842753 Val Loss: 0.00679328\n",
      "Epoch:  48 Train Loss: 0.00714989 Val Loss: 0.00702401\n",
      "Epoch:  49 Train Loss: 0.00704046 Val Loss: 0.00695774\n",
      "Epoch:  50 Train Loss: 0.00672413 Val Loss: 0.00702967\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.18136377 Val Loss: 0.13670386\n",
      "Epoch:   2 Train Loss: 0.10331096 Val Loss: 0.06566178\n",
      "Epoch:   3 Train Loss: 0.05999632 Val Loss: 0.03507218\n",
      "Epoch:   4 Train Loss: 0.05276907 Val Loss: 0.03146634\n",
      "Epoch:   5 Train Loss: 0.04910387 Val Loss: 0.03107021\n",
      "Epoch:   6 Train Loss: 0.04549960 Val Loss: 0.02802016\n",
      "Epoch:   7 Train Loss: 0.04003271 Val Loss: 0.02421387\n",
      "Epoch:   8 Train Loss: 0.03412055 Val Loss: 0.02174608\n",
      "Epoch:   9 Train Loss: 0.02643249 Val Loss: 0.01607186\n",
      "Epoch:  10 Train Loss: 0.01836515 Val Loss: 0.01075825\n",
      "Epoch:  11 Train Loss: 0.01102255 Val Loss: 0.01089562\n",
      "Epoch:  12 Train Loss: 0.01044165 Val Loss: 0.02139679\n",
      "Epoch:  13 Train Loss: 0.01776368 Val Loss: 0.00952351\n",
      "Epoch:  14 Train Loss: 0.01282234 Val Loss: 0.00930286\n",
      "Epoch:  15 Train Loss: 0.00923813 Val Loss: 0.00823833\n",
      "Epoch:  16 Train Loss: 0.01031841 Val Loss: 0.00867262\n",
      "Epoch:  17 Train Loss: 0.01202128 Val Loss: 0.01055352\n",
      "Epoch:  18 Train Loss: 0.01135623 Val Loss: 0.00983454\n",
      "Epoch:  19 Train Loss: 0.00999004 Val Loss: 0.00800144\n",
      "Epoch:  20 Train Loss: 0.01024519 Val Loss: 0.00875265\n",
      "Epoch:  21 Train Loss: 0.01056240 Val Loss: 0.00746772\n",
      "Epoch:  22 Train Loss: 0.00783351 Val Loss: 0.00991286\n",
      "Epoch:  23 Train Loss: 0.00721096 Val Loss: 0.00734106\n",
      "Epoch:  24 Train Loss: 0.00841037 Val Loss: 0.01008658\n",
      "Epoch:  25 Train Loss: 0.00853304 Val Loss: 0.00722028\n",
      "Epoch:  26 Train Loss: 0.00940440 Val Loss: 0.00899151\n",
      "Epoch:  27 Train Loss: 0.00806886 Val Loss: 0.00771505\n",
      "Epoch:  28 Train Loss: 0.00825967 Val Loss: 0.00706640\n",
      "Epoch:  29 Train Loss: 0.00756948 Val Loss: 0.00791295\n",
      "Epoch:  30 Train Loss: 0.00758119 Val Loss: 0.00702257\n",
      "Epoch:  31 Train Loss: 0.00843274 Val Loss: 0.00796369\n",
      "Epoch:  32 Train Loss: 0.00822540 Val Loss: 0.00697162\n",
      "Epoch:  33 Train Loss: 0.00763610 Val Loss: 0.00744611\n",
      "Epoch:  34 Train Loss: 0.00794486 Val Loss: 0.00705263\n",
      "Epoch:  35 Train Loss: 0.00688117 Val Loss: 0.00695407\n",
      "Epoch:  36 Train Loss: 0.00827016 Val Loss: 0.00698286\n",
      "Epoch:  37 Train Loss: 0.00757670 Val Loss: 0.00745161\n",
      "Epoch:  38 Train Loss: 0.00842945 Val Loss: 0.00690061\n",
      "Epoch:  39 Train Loss: 0.00803179 Val Loss: 0.00704017\n",
      "Epoch:  40 Train Loss: 0.00836783 Val Loss: 0.00747511\n",
      "Epoch:  41 Train Loss: 0.00668054 Val Loss: 0.00684315\n",
      "Epoch:  42 Train Loss: 0.00732383 Val Loss: 0.00700890\n",
      "Epoch:  43 Train Loss: 0.00660721 Val Loss: 0.00680553\n",
      "Epoch:  44 Train Loss: 0.00776877 Val Loss: 0.00687394\n",
      "Epoch:  45 Train Loss: 0.00786679 Val Loss: 0.00711877\n",
      "Epoch:  46 Train Loss: 0.00789373 Val Loss: 0.00678846\n",
      "Epoch:  47 Train Loss: 0.00738490 Val Loss: 0.00687958\n",
      "Epoch:  48 Train Loss: 0.00765749 Val Loss: 0.00682466\n",
      "Epoch:  49 Train Loss: 0.00668574 Val Loss: 0.00673925\n",
      "Epoch:  50 Train Loss: 0.00736394 Val Loss: 0.00684590\n",
      "Epoch:  51 Train Loss: 0.00688829 Val Loss: 0.00680827\n",
      "Epoch:  52 Train Loss: 0.00865750 Val Loss: 0.00691249\n",
      "Epoch:  53 Train Loss: 0.00691638 Val Loss: 0.00691234\n",
      "Epoch:  54 Train Loss: 0.00699658 Val Loss: 0.00724390\n",
      "Epoch:  55 Train Loss: 0.00755095 Val Loss: 0.00694762\n",
      "Epoch:  56 Train Loss: 0.00737567 Val Loss: 0.00682588\n",
      "Epoch:  57 Train Loss: 0.00665744 Val Loss: 0.00673505\n",
      "Epoch:  58 Train Loss: 0.00664511 Val Loss: 0.00683849\n",
      "Epoch:  59 Train Loss: 0.00678072 Val Loss: 0.00675170\n",
      "Epoch:  60 Train Loss: 0.00646631 Val Loss: 0.00690575\n",
      "Epoch:  61 Train Loss: 0.00716814 Val Loss: 0.00681661\n",
      "Epoch:  62 Train Loss: 0.00790543 Val Loss: 0.00686407\n",
      "Epoch:  63 Train Loss: 0.00702189 Val Loss: 0.00668433\n",
      "Epoch:  64 Train Loss: 0.00789566 Val Loss: 0.00669560\n",
      "Epoch:  65 Train Loss: 0.00714393 Val Loss: 0.00678418\n",
      "Epoch:  66 Train Loss: 0.00723688 Val Loss: 0.00674009\n",
      "Epoch:  67 Train Loss: 0.00685458 Val Loss: 0.00679550\n",
      "Epoch:  68 Train Loss: 0.00689676 Val Loss: 0.00675010\n",
      "Epoch:  69 Train Loss: 0.00706735 Val Loss: 0.00750646\n",
      "Epoch:  70 Train Loss: 0.00794138 Val Loss: 0.00668216\n",
      "Epoch:  71 Train Loss: 0.00735140 Val Loss: 0.00718953\n",
      "Epoch:  72 Train Loss: 0.00720025 Val Loss: 0.00676497\n",
      "Epoch:  73 Train Loss: 0.00818271 Val Loss: 0.00790113\n",
      "Epoch:  74 Train Loss: 0.00857176 Val Loss: 0.00694658\n",
      "Epoch:  75 Train Loss: 0.00820508 Val Loss: 0.00711343\n",
      "Epoch:  76 Train Loss: 0.00696532 Val Loss: 0.00665467\n",
      "Epoch:  77 Train Loss: 0.00792618 Val Loss: 0.00737908\n",
      "Epoch:  78 Train Loss: 0.00784022 Val Loss: 0.00665435\n",
      "Epoch:  79 Train Loss: 0.00762437 Val Loss: 0.00706145\n",
      "Epoch:  80 Train Loss: 0.00849879 Val Loss: 0.00661227\n",
      "Epoch:  81 Train Loss: 0.00888141 Val Loss: 0.00716349\n",
      "Epoch:  82 Train Loss: 0.00783028 Val Loss: 0.00662484\n",
      "Epoch:  83 Train Loss: 0.00754464 Val Loss: 0.00730605\n",
      "Epoch:  84 Train Loss: 0.00774555 Val Loss: 0.00677565\n",
      "Epoch:  85 Train Loss: 0.00760457 Val Loss: 0.00802159\n",
      "Epoch:  86 Train Loss: 0.00724210 Val Loss: 0.00660384\n",
      "Epoch:  87 Train Loss: 0.00697330 Val Loss: 0.00659289\n",
      "Epoch:  88 Train Loss: 0.00703189 Val Loss: 0.00684477\n",
      "Epoch:  89 Train Loss: 0.00660906 Val Loss: 0.00661467\n",
      "Epoch:  90 Train Loss: 0.00627007 Val Loss: 0.00658398\n",
      "Epoch:  91 Train Loss: 0.00735127 Val Loss: 0.00661410\n",
      "Epoch:  92 Train Loss: 0.00635749 Val Loss: 0.00659054\n",
      "Epoch:  93 Train Loss: 0.00673541 Val Loss: 0.00660612\n",
      "Epoch:  94 Train Loss: 0.00744858 Val Loss: 0.00667928\n",
      "Epoch:  95 Train Loss: 0.00711662 Val Loss: 0.00655674\n",
      "Epoch:  96 Train Loss: 0.00694518 Val Loss: 0.00679861\n",
      "Epoch:  97 Train Loss: 0.00794174 Val Loss: 0.00669862\n",
      "Epoch:  98 Train Loss: 0.00758234 Val Loss: 0.00653427\n",
      "Epoch:  99 Train Loss: 0.00591111 Val Loss: 0.00674189\n",
      "Epoch: 100 Train Loss: 0.00650678 Val Loss: 0.00653981\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.09037333 Val Loss: 0.06168890\n",
      "Epoch:   2 Train Loss: 0.05210485 Val Loss: 0.02224601\n",
      "Epoch:   3 Train Loss: 0.03296861 Val Loss: 0.01825878\n",
      "Epoch:   4 Train Loss: 0.02094914 Val Loss: 0.01546695\n",
      "Epoch:   5 Train Loss: 0.01490604 Val Loss: 0.00965277\n",
      "Epoch:   6 Train Loss: 0.01123588 Val Loss: 0.00962842\n",
      "Epoch:   7 Train Loss: 0.00948623 Val Loss: 0.01010648\n",
      "Epoch:   8 Train Loss: 0.01168384 Val Loss: 0.01144079\n",
      "Epoch:   9 Train Loss: 0.00965295 Val Loss: 0.00898252\n",
      "Epoch:  10 Train Loss: 0.00902143 Val Loss: 0.00886217\n",
      "Epoch:  11 Train Loss: 0.01071102 Val Loss: 0.00922303\n",
      "Epoch:  12 Train Loss: 0.00860635 Val Loss: 0.00874326\n",
      "Epoch:  13 Train Loss: 0.00840140 Val Loss: 0.00874221\n",
      "Epoch:  14 Train Loss: 0.01028779 Val Loss: 0.01501826\n",
      "Epoch:  15 Train Loss: 0.01041875 Val Loss: 0.01235940\n",
      "Epoch:  16 Train Loss: 0.00993171 Val Loss: 0.00885185\n",
      "Epoch:  17 Train Loss: 0.00880583 Val Loss: 0.01019312\n",
      "Epoch:  18 Train Loss: 0.00972147 Val Loss: 0.00964309\n",
      "Epoch:  19 Train Loss: 0.00904259 Val Loss: 0.00832685\n",
      "Epoch:  20 Train Loss: 0.00820869 Val Loss: 0.00818042\n",
      "Epoch:  21 Train Loss: 0.00770327 Val Loss: 0.00954154\n",
      "Epoch:  22 Train Loss: 0.00813702 Val Loss: 0.01121607\n",
      "Epoch:  23 Train Loss: 0.01017178 Val Loss: 0.00816561\n",
      "Epoch:  24 Train Loss: 0.01029112 Val Loss: 0.01087704\n",
      "Epoch:  25 Train Loss: 0.00944337 Val Loss: 0.01254472\n",
      "Epoch:  26 Train Loss: 0.01013049 Val Loss: 0.01080393\n",
      "Epoch:  27 Train Loss: 0.00736179 Val Loss: 0.00829805\n",
      "Epoch:  28 Train Loss: 0.00909159 Val Loss: 0.00896623\n",
      "Epoch:  29 Train Loss: 0.00821518 Val Loss: 0.01060543\n",
      "Epoch:  30 Train Loss: 0.00922061 Val Loss: 0.00792272\n",
      "Epoch:  31 Train Loss: 0.00888668 Val Loss: 0.00787909\n",
      "Epoch:  32 Train Loss: 0.00813758 Val Loss: 0.00813555\n",
      "Epoch:  33 Train Loss: 0.00898601 Val Loss: 0.00817900\n",
      "Epoch:  34 Train Loss: 0.00891849 Val Loss: 0.01084408\n",
      "Epoch:  35 Train Loss: 0.00871142 Val Loss: 0.00885047\n",
      "Epoch:  36 Train Loss: 0.00963120 Val Loss: 0.00839937\n",
      "Epoch:  37 Train Loss: 0.00795380 Val Loss: 0.00793020\n",
      "Epoch:  38 Train Loss: 0.00698479 Val Loss: 0.00784995\n",
      "Epoch:  39 Train Loss: 0.00730802 Val Loss: 0.00835344\n",
      "Epoch:  40 Train Loss: 0.00720478 Val Loss: 0.00779315\n",
      "Epoch:  41 Train Loss: 0.00737449 Val Loss: 0.00933340\n",
      "Epoch:  42 Train Loss: 0.00834679 Val Loss: 0.00767595\n",
      "Epoch:  43 Train Loss: 0.00843162 Val Loss: 0.00750888\n",
      "Epoch:  44 Train Loss: 0.00604462 Val Loss: 0.00751728\n",
      "Epoch:  45 Train Loss: 0.00696069 Val Loss: 0.00824618\n",
      "Epoch:  46 Train Loss: 0.00695236 Val Loss: 0.00743976\n",
      "Epoch:  47 Train Loss: 0.00649510 Val Loss: 0.00734675\n",
      "Epoch:  48 Train Loss: 0.00756458 Val Loss: 0.00784972\n",
      "Epoch:  49 Train Loss: 0.00704988 Val Loss: 0.00737436\n",
      "Epoch:  50 Train Loss: 0.00707174 Val Loss: 0.00849355\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.12036014 Val Loss: 0.07567485\n",
      "Epoch:   2 Train Loss: 0.06102838 Val Loss: 0.02497633\n",
      "Epoch:   3 Train Loss: 0.03857606 Val Loss: 0.02279035\n",
      "Epoch:   4 Train Loss: 0.02703086 Val Loss: 0.02048702\n",
      "Epoch:   5 Train Loss: 0.02276470 Val Loss: 0.01395890\n",
      "Epoch:   6 Train Loss: 0.01506306 Val Loss: 0.01101132\n",
      "Epoch:   7 Train Loss: 0.01158790 Val Loss: 0.01029266\n",
      "Epoch:   8 Train Loss: 0.01022939 Val Loss: 0.01024322\n",
      "Epoch:   9 Train Loss: 0.01133818 Val Loss: 0.01080457\n",
      "Epoch:  10 Train Loss: 0.01275712 Val Loss: 0.01059501\n",
      "Epoch:  11 Train Loss: 0.01087567 Val Loss: 0.01032517\n",
      "Epoch:  12 Train Loss: 0.00981090 Val Loss: 0.01056301\n",
      "Epoch:  13 Train Loss: 0.01138694 Val Loss: 0.00931318\n",
      "Epoch:  14 Train Loss: 0.01044318 Val Loss: 0.00902154\n",
      "Epoch:  15 Train Loss: 0.01120274 Val Loss: 0.00907717\n",
      "Epoch:  16 Train Loss: 0.00823737 Val Loss: 0.00864647\n",
      "Epoch:  17 Train Loss: 0.00868329 Val Loss: 0.01005970\n",
      "Epoch:  18 Train Loss: 0.00947299 Val Loss: 0.01061366\n",
      "Epoch:  19 Train Loss: 0.00974313 Val Loss: 0.00832812\n",
      "Epoch:  20 Train Loss: 0.00847035 Val Loss: 0.00811146\n",
      "Epoch:  21 Train Loss: 0.00743714 Val Loss: 0.00829888\n",
      "Epoch:  22 Train Loss: 0.00820152 Val Loss: 0.00813127\n",
      "Epoch:  23 Train Loss: 0.00718769 Val Loss: 0.00911635\n",
      "Epoch:  24 Train Loss: 0.00768773 Val Loss: 0.00809161\n",
      "Epoch:  25 Train Loss: 0.00904198 Val Loss: 0.00927497\n",
      "Epoch:  26 Train Loss: 0.00869333 Val Loss: 0.02199267\n",
      "Epoch:  27 Train Loss: 0.01495771 Val Loss: 0.00888952\n",
      "Epoch:  28 Train Loss: 0.01120199 Val Loss: 0.01015796\n",
      "Epoch:  29 Train Loss: 0.00924244 Val Loss: 0.00838363\n",
      "Epoch:  30 Train Loss: 0.00765822 Val Loss: 0.00813763\n",
      "Epoch:  31 Train Loss: 0.00766200 Val Loss: 0.00820662\n",
      "Epoch:  32 Train Loss: 0.00865588 Val Loss: 0.00852002\n",
      "Epoch:  33 Train Loss: 0.00878430 Val Loss: 0.00817621\n",
      "Epoch:  34 Train Loss: 0.00786827 Val Loss: 0.00844240\n",
      "Epoch:  35 Train Loss: 0.00733781 Val Loss: 0.00786665\n",
      "Epoch:  36 Train Loss: 0.00717842 Val Loss: 0.00777522\n",
      "Epoch:  37 Train Loss: 0.00741925 Val Loss: 0.00792100\n",
      "Epoch:  38 Train Loss: 0.00810351 Val Loss: 0.00791874\n",
      "Epoch:  39 Train Loss: 0.00773064 Val Loss: 0.00782897\n",
      "Epoch:  40 Train Loss: 0.00737116 Val Loss: 0.00850848\n",
      "Epoch:  41 Train Loss: 0.00821107 Val Loss: 0.01000533\n",
      "Epoch:  42 Train Loss: 0.00913242 Val Loss: 0.00727276\n",
      "Epoch:  43 Train Loss: 0.00708623 Val Loss: 0.00727257\n",
      "Epoch:  44 Train Loss: 0.00804200 Val Loss: 0.00771829\n",
      "Epoch:  45 Train Loss: 0.00904764 Val Loss: 0.00920548\n",
      "Epoch:  46 Train Loss: 0.00913666 Val Loss: 0.00741381\n",
      "Epoch:  47 Train Loss: 0.00816241 Val Loss: 0.00981595\n",
      "Epoch:  48 Train Loss: 0.00864132 Val Loss: 0.00819698\n",
      "Epoch:  49 Train Loss: 0.00668538 Val Loss: 0.00833628\n",
      "Epoch:  50 Train Loss: 0.00813470 Val Loss: 0.00691989\n",
      "Epoch:  51 Train Loss: 0.00765480 Val Loss: 0.00750303\n",
      "Epoch:  52 Train Loss: 0.00927036 Val Loss: 0.00834408\n",
      "Epoch:  53 Train Loss: 0.00747036 Val Loss: 0.00751606\n",
      "Epoch:  54 Train Loss: 0.00687255 Val Loss: 0.01035176\n",
      "Epoch:  55 Train Loss: 0.00767313 Val Loss: 0.00960077\n",
      "Epoch:  56 Train Loss: 0.00895076 Val Loss: 0.00742165\n",
      "Epoch:  57 Train Loss: 0.00763814 Val Loss: 0.00773107\n",
      "Epoch:  58 Train Loss: 0.00732576 Val Loss: 0.00844750\n",
      "Epoch:  59 Train Loss: 0.00674075 Val Loss: 0.00828520\n",
      "Epoch:  60 Train Loss: 0.00863540 Val Loss: 0.00806995\n",
      "Epoch:  61 Train Loss: 0.00911904 Val Loss: 0.01049119\n",
      "Epoch:  62 Train Loss: 0.01316882 Val Loss: 0.00732814\n",
      "Epoch:  63 Train Loss: 0.00797319 Val Loss: 0.00923478\n",
      "Epoch:  64 Train Loss: 0.00688681 Val Loss: 0.00939844\n",
      "Epoch:  65 Train Loss: 0.00823372 Val Loss: 0.01084954\n",
      "Epoch:  66 Train Loss: 0.01038677 Val Loss: 0.00679925\n",
      "Epoch:  67 Train Loss: 0.00799012 Val Loss: 0.00755802\n",
      "Epoch:  68 Train Loss: 0.00856129 Val Loss: 0.00792264\n",
      "Epoch:  69 Train Loss: 0.00931874 Val Loss: 0.00935978\n",
      "Epoch:  70 Train Loss: 0.00746652 Val Loss: 0.00892746\n",
      "Epoch:  71 Train Loss: 0.00856756 Val Loss: 0.00785314\n",
      "Epoch:  72 Train Loss: 0.00722219 Val Loss: 0.00807660\n",
      "Epoch:  73 Train Loss: 0.00815477 Val Loss: 0.00733600\n",
      "Epoch:  74 Train Loss: 0.00683842 Val Loss: 0.00761880\n",
      "Epoch:  75 Train Loss: 0.00716873 Val Loss: 0.00670563\n",
      "Epoch:  76 Train Loss: 0.00669265 Val Loss: 0.00819338\n",
      "Epoch:  77 Train Loss: 0.00733875 Val Loss: 0.00664724\n",
      "Epoch:  78 Train Loss: 0.00831648 Val Loss: 0.00665967\n",
      "Epoch:  79 Train Loss: 0.00665463 Val Loss: 0.00850794\n",
      "Epoch:  80 Train Loss: 0.00710351 Val Loss: 0.00726609\n",
      "Epoch:  81 Train Loss: 0.00827821 Val Loss: 0.00774180\n",
      "Epoch:  82 Train Loss: 0.00725127 Val Loss: 0.00641886\n",
      "Epoch:  83 Train Loss: 0.00543529 Val Loss: 0.00861750\n",
      "Epoch:  84 Train Loss: 0.00909223 Val Loss: 0.00687905\n",
      "Epoch:  85 Train Loss: 0.00659519 Val Loss: 0.00611199\n",
      "Epoch:  86 Train Loss: 0.00666277 Val Loss: 0.00803115\n",
      "Epoch:  87 Train Loss: 0.00739250 Val Loss: 0.00932219\n",
      "Epoch:  88 Train Loss: 0.00843903 Val Loss: 0.00746626\n",
      "Epoch:  89 Train Loss: 0.00781771 Val Loss: 0.00659544\n",
      "Epoch:  90 Train Loss: 0.00587026 Val Loss: 0.00653843\n",
      "Epoch:  91 Train Loss: 0.00624889 Val Loss: 0.00600223\n",
      "Epoch:  92 Train Loss: 0.00665912 Val Loss: 0.00695267\n",
      "Epoch:  93 Train Loss: 0.00726531 Val Loss: 0.00730816\n",
      "Epoch:  94 Train Loss: 0.00702281 Val Loss: 0.00609636\n",
      "Epoch:  95 Train Loss: 0.00579206 Val Loss: 0.00679519\n",
      "Epoch:  96 Train Loss: 0.00543673 Val Loss: 0.00613892\n",
      "Epoch:  97 Train Loss: 0.00497027 Val Loss: 0.00488979\n",
      "Epoch:  98 Train Loss: 0.00482325 Val Loss: 0.00468621\n",
      "Epoch:  99 Train Loss: 0.00436402 Val Loss: 0.00584354\n",
      "Epoch: 100 Train Loss: 0.00478769 Val Loss: 0.00769877\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.21141417 Val Loss: 0.15835913\n",
      "Epoch:   2 Train Loss: 0.12175367 Val Loss: 0.06409957\n",
      "Epoch:   3 Train Loss: 0.05588913 Val Loss: 0.03100367\n",
      "Epoch:   4 Train Loss: 0.04866207 Val Loss: 0.02571432\n",
      "Epoch:   5 Train Loss: 0.04318377 Val Loss: 0.02704956\n",
      "Epoch:   6 Train Loss: 0.03734374 Val Loss: 0.02362644\n",
      "Epoch:   7 Train Loss: 0.03278040 Val Loss: 0.02093101\n",
      "Epoch:   8 Train Loss: 0.02869653 Val Loss: 0.01860068\n",
      "Epoch:   9 Train Loss: 0.02583843 Val Loss: 0.01626244\n",
      "Epoch:  10 Train Loss: 0.02166469 Val Loss: 0.01315441\n",
      "Epoch:  11 Train Loss: 0.01494922 Val Loss: 0.01035057\n",
      "Epoch:  12 Train Loss: 0.01146289 Val Loss: 0.00904450\n",
      "Epoch:  13 Train Loss: 0.00992091 Val Loss: 0.00871716\n",
      "Epoch:  14 Train Loss: 0.01061317 Val Loss: 0.00797796\n",
      "Epoch:  15 Train Loss: 0.01059262 Val Loss: 0.00881914\n",
      "Epoch:  16 Train Loss: 0.00931536 Val Loss: 0.00902386\n",
      "Epoch:  17 Train Loss: 0.00916658 Val Loss: 0.00761529\n",
      "Epoch:  18 Train Loss: 0.01037252 Val Loss: 0.01167775\n",
      "Epoch:  19 Train Loss: 0.01263928 Val Loss: 0.00817926\n",
      "Epoch:  20 Train Loss: 0.01031464 Val Loss: 0.00802482\n",
      "Epoch:  21 Train Loss: 0.00945825 Val Loss: 0.00869585\n",
      "Epoch:  22 Train Loss: 0.00953736 Val Loss: 0.00782021\n",
      "Epoch:  23 Train Loss: 0.00828374 Val Loss: 0.00978917\n",
      "Epoch:  24 Train Loss: 0.00908304 Val Loss: 0.00739527\n",
      "Epoch:  25 Train Loss: 0.00901029 Val Loss: 0.00938361\n",
      "Epoch:  26 Train Loss: 0.01024494 Val Loss: 0.00737920\n",
      "Epoch:  27 Train Loss: 0.00855105 Val Loss: 0.01175462\n",
      "Epoch:  28 Train Loss: 0.01081847 Val Loss: 0.00730281\n",
      "Epoch:  29 Train Loss: 0.00878091 Val Loss: 0.00920986\n",
      "Epoch:  30 Train Loss: 0.00973769 Val Loss: 0.00785276\n",
      "Epoch:  31 Train Loss: 0.01210482 Val Loss: 0.00964526\n",
      "Epoch:  32 Train Loss: 0.00900912 Val Loss: 0.00711623\n",
      "Epoch:  33 Train Loss: 0.00851007 Val Loss: 0.00787848\n",
      "Epoch:  34 Train Loss: 0.00884490 Val Loss: 0.00726696\n",
      "Epoch:  35 Train Loss: 0.00777308 Val Loss: 0.00809415\n",
      "Epoch:  36 Train Loss: 0.00743146 Val Loss: 0.00703938\n",
      "Epoch:  37 Train Loss: 0.00871806 Val Loss: 0.00769637\n",
      "Epoch:  38 Train Loss: 0.00713925 Val Loss: 0.00736607\n",
      "Epoch:  39 Train Loss: 0.00695064 Val Loss: 0.00699588\n",
      "Epoch:  40 Train Loss: 0.00912265 Val Loss: 0.00787748\n",
      "Epoch:  41 Train Loss: 0.00708193 Val Loss: 0.00700546\n",
      "Epoch:  42 Train Loss: 0.00871419 Val Loss: 0.00758307\n",
      "Epoch:  43 Train Loss: 0.00962995 Val Loss: 0.00718560\n",
      "Epoch:  44 Train Loss: 0.00921192 Val Loss: 0.00694013\n",
      "Epoch:  45 Train Loss: 0.00829109 Val Loss: 0.00737757\n",
      "Epoch:  46 Train Loss: 0.00812103 Val Loss: 0.00702017\n",
      "Epoch:  47 Train Loss: 0.00744893 Val Loss: 0.00716361\n",
      "Epoch:  48 Train Loss: 0.00703194 Val Loss: 0.00691537\n",
      "Epoch:  49 Train Loss: 0.00793393 Val Loss: 0.00736510\n",
      "Epoch:  50 Train Loss: 0.00830043 Val Loss: 0.00688146\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.26467457 Val Loss: 0.21723092\n",
      "Epoch:   2 Train Loss: 0.16126193 Val Loss: 0.12476551\n",
      "Epoch:   3 Train Loss: 0.08598193 Val Loss: 0.03788027\n",
      "Epoch:   4 Train Loss: 0.04940641 Val Loss: 0.03206928\n",
      "Epoch:   5 Train Loss: 0.04000129 Val Loss: 0.02702050\n",
      "Epoch:   6 Train Loss: 0.03955573 Val Loss: 0.02737487\n",
      "Epoch:   7 Train Loss: 0.03449498 Val Loss: 0.02012310\n",
      "Epoch:   8 Train Loss: 0.02819958 Val Loss: 0.01774812\n",
      "Epoch:   9 Train Loss: 0.02398599 Val Loss: 0.01684116\n",
      "Epoch:  10 Train Loss: 0.02184755 Val Loss: 0.01208330\n",
      "Epoch:  11 Train Loss: 0.01607198 Val Loss: 0.00966659\n",
      "Epoch:  12 Train Loss: 0.01196540 Val Loss: 0.00939503\n",
      "Epoch:  13 Train Loss: 0.00954466 Val Loss: 0.00880051\n",
      "Epoch:  14 Train Loss: 0.01063643 Val Loss: 0.00799467\n",
      "Epoch:  15 Train Loss: 0.00822164 Val Loss: 0.00866799\n",
      "Epoch:  16 Train Loss: 0.01058454 Val Loss: 0.00762645\n",
      "Epoch:  17 Train Loss: 0.00884459 Val Loss: 0.00752899\n",
      "Epoch:  18 Train Loss: 0.00893957 Val Loss: 0.00887009\n",
      "Epoch:  19 Train Loss: 0.00794834 Val Loss: 0.00733624\n",
      "Epoch:  20 Train Loss: 0.00903582 Val Loss: 0.00824382\n",
      "Epoch:  21 Train Loss: 0.00970229 Val Loss: 0.00726927\n",
      "Epoch:  22 Train Loss: 0.00746860 Val Loss: 0.00758147\n",
      "Epoch:  23 Train Loss: 0.00856408 Val Loss: 0.01033184\n",
      "Epoch:  24 Train Loss: 0.01114072 Val Loss: 0.00724363\n",
      "Epoch:  25 Train Loss: 0.01057040 Val Loss: 0.00910624\n",
      "Epoch:  26 Train Loss: 0.00948525 Val Loss: 0.00714368\n",
      "Epoch:  27 Train Loss: 0.00991957 Val Loss: 0.01016550\n",
      "Epoch:  28 Train Loss: 0.00795652 Val Loss: 0.00715192\n",
      "Epoch:  29 Train Loss: 0.00969848 Val Loss: 0.00943680\n",
      "Epoch:  30 Train Loss: 0.00999281 Val Loss: 0.00713487\n",
      "Epoch:  31 Train Loss: 0.00889291 Val Loss: 0.00784654\n",
      "Epoch:  32 Train Loss: 0.00807354 Val Loss: 0.00710134\n",
      "Epoch:  33 Train Loss: 0.01025017 Val Loss: 0.00724960\n",
      "Epoch:  34 Train Loss: 0.00956192 Val Loss: 0.00710817\n",
      "Epoch:  35 Train Loss: 0.00916160 Val Loss: 0.00720767\n",
      "Epoch:  36 Train Loss: 0.00954142 Val Loss: 0.00887827\n",
      "Epoch:  37 Train Loss: 0.01035617 Val Loss: 0.00699926\n",
      "Epoch:  38 Train Loss: 0.00784514 Val Loss: 0.00741705\n",
      "Epoch:  39 Train Loss: 0.00875546 Val Loss: 0.00699530\n",
      "Epoch:  40 Train Loss: 0.00835632 Val Loss: 0.00775782\n",
      "Epoch:  41 Train Loss: 0.00935792 Val Loss: 0.00736721\n",
      "Epoch:  42 Train Loss: 0.01119479 Val Loss: 0.00724503\n",
      "Epoch:  43 Train Loss: 0.01028375 Val Loss: 0.00754073\n",
      "Epoch:  44 Train Loss: 0.01000019 Val Loss: 0.00786694\n",
      "Epoch:  45 Train Loss: 0.00863596 Val Loss: 0.00721513\n",
      "Epoch:  46 Train Loss: 0.00730010 Val Loss: 0.00715323\n",
      "Epoch:  47 Train Loss: 0.00777514 Val Loss: 0.00685684\n",
      "Epoch:  48 Train Loss: 0.00760773 Val Loss: 0.00736882\n",
      "Epoch:  49 Train Loss: 0.00676293 Val Loss: 0.00690096\n",
      "Epoch:  50 Train Loss: 0.00666679 Val Loss: 0.00692397\n",
      "Epoch:  51 Train Loss: 0.00916049 Val Loss: 0.00718586\n",
      "Epoch:  52 Train Loss: 0.00778198 Val Loss: 0.00709887\n",
      "Epoch:  53 Train Loss: 0.00749131 Val Loss: 0.00707665\n",
      "Epoch:  54 Train Loss: 0.00706748 Val Loss: 0.00683934\n",
      "Epoch:  55 Train Loss: 0.00805473 Val Loss: 0.00680995\n",
      "Epoch:  56 Train Loss: 0.00940301 Val Loss: 0.00687452\n",
      "Epoch:  57 Train Loss: 0.00947147 Val Loss: 0.00729823\n",
      "Epoch:  58 Train Loss: 0.00995910 Val Loss: 0.00681750\n",
      "Epoch:  59 Train Loss: 0.00772279 Val Loss: 0.00687517\n",
      "Epoch:  60 Train Loss: 0.00776847 Val Loss: 0.00702226\n",
      "Epoch:  61 Train Loss: 0.00852879 Val Loss: 0.00712201\n",
      "Epoch:  62 Train Loss: 0.00827570 Val Loss: 0.00689441\n",
      "Epoch:  63 Train Loss: 0.00888392 Val Loss: 0.00796051\n",
      "Epoch:  64 Train Loss: 0.00873272 Val Loss: 0.00679811\n",
      "Epoch:  65 Train Loss: 0.00853646 Val Loss: 0.00715580\n",
      "Epoch:  66 Train Loss: 0.00713990 Val Loss: 0.00680348\n",
      "Epoch:  67 Train Loss: 0.00908483 Val Loss: 0.00700435\n",
      "Epoch:  68 Train Loss: 0.00767484 Val Loss: 0.00673685\n",
      "Epoch:  69 Train Loss: 0.00761741 Val Loss: 0.00692461\n",
      "Epoch:  70 Train Loss: 0.00830079 Val Loss: 0.00676208\n",
      "Epoch:  71 Train Loss: 0.00711560 Val Loss: 0.00727968\n",
      "Epoch:  72 Train Loss: 0.00845090 Val Loss: 0.00682954\n",
      "Epoch:  73 Train Loss: 0.00755870 Val Loss: 0.00692220\n",
      "Epoch:  74 Train Loss: 0.00833281 Val Loss: 0.00724301\n",
      "Epoch:  75 Train Loss: 0.00743247 Val Loss: 0.00671095\n",
      "Epoch:  76 Train Loss: 0.00741341 Val Loss: 0.00684820\n",
      "Epoch:  77 Train Loss: 0.00718704 Val Loss: 0.00690114\n",
      "Epoch:  78 Train Loss: 0.00753539 Val Loss: 0.00673270\n",
      "Epoch:  79 Train Loss: 0.00659051 Val Loss: 0.00672061\n",
      "Epoch:  80 Train Loss: 0.00896376 Val Loss: 0.00678311\n",
      "Epoch:  81 Train Loss: 0.00682910 Val Loss: 0.00670472\n",
      "Epoch:  82 Train Loss: 0.00771335 Val Loss: 0.00707309\n",
      "Epoch:  83 Train Loss: 0.00717790 Val Loss: 0.00682151\n",
      "Epoch:  84 Train Loss: 0.00840751 Val Loss: 0.00844631\n",
      "Epoch:  85 Train Loss: 0.00837860 Val Loss: 0.00682721\n",
      "Epoch:  86 Train Loss: 0.00717685 Val Loss: 0.00709036\n",
      "Epoch:  87 Train Loss: 0.00745216 Val Loss: 0.00667034\n",
      "Epoch:  88 Train Loss: 0.00710779 Val Loss: 0.00669675\n",
      "Epoch:  89 Train Loss: 0.00782272 Val Loss: 0.00730257\n",
      "Epoch:  90 Train Loss: 0.00915201 Val Loss: 0.00672607\n",
      "Epoch:  91 Train Loss: 0.00876551 Val Loss: 0.00739638\n",
      "Epoch:  92 Train Loss: 0.00741607 Val Loss: 0.00708915\n",
      "Epoch:  93 Train Loss: 0.00983360 Val Loss: 0.00710610\n",
      "Epoch:  94 Train Loss: 0.00725770 Val Loss: 0.00666600\n",
      "Epoch:  95 Train Loss: 0.00729863 Val Loss: 0.00823634\n",
      "Epoch:  96 Train Loss: 0.00840397 Val Loss: 0.00680591\n",
      "Epoch:  97 Train Loss: 0.00853784 Val Loss: 0.00710295\n",
      "Epoch:  98 Train Loss: 0.00773009 Val Loss: 0.00667505\n",
      "Epoch:  99 Train Loss: 0.00747048 Val Loss: 0.00661482\n",
      "Epoch: 100 Train Loss: 0.00745470 Val Loss: 0.00660988\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.10651456 Val Loss: 0.06180392\n",
      "Epoch:   2 Train Loss: 0.06213979 Val Loss: 0.02803951\n",
      "Epoch:   3 Train Loss: 0.03687377 Val Loss: 0.02353756\n",
      "Epoch:   4 Train Loss: 0.02844509 Val Loss: 0.02176502\n",
      "Epoch:   5 Train Loss: 0.02120273 Val Loss: 0.01313707\n",
      "Epoch:   6 Train Loss: 0.01245071 Val Loss: 0.01034548\n",
      "Epoch:   7 Train Loss: 0.01296572 Val Loss: 0.01004042\n",
      "Epoch:   8 Train Loss: 0.01072026 Val Loss: 0.01205847\n",
      "Epoch:   9 Train Loss: 0.01182748 Val Loss: 0.01047103\n",
      "Epoch:  10 Train Loss: 0.01095493 Val Loss: 0.01029146\n",
      "Epoch:  11 Train Loss: 0.00982429 Val Loss: 0.01162980\n",
      "Epoch:  12 Train Loss: 0.01159315 Val Loss: 0.01010031\n",
      "Epoch:  13 Train Loss: 0.01248650 Val Loss: 0.00996780\n",
      "Epoch:  14 Train Loss: 0.00966471 Val Loss: 0.00913681\n",
      "Epoch:  15 Train Loss: 0.01047656 Val Loss: 0.00866380\n",
      "Epoch:  16 Train Loss: 0.00891240 Val Loss: 0.00854869\n",
      "Epoch:  17 Train Loss: 0.00921161 Val Loss: 0.00842642\n",
      "Epoch:  18 Train Loss: 0.00845160 Val Loss: 0.00935497\n",
      "Epoch:  19 Train Loss: 0.00836306 Val Loss: 0.00828607\n",
      "Epoch:  20 Train Loss: 0.00894009 Val Loss: 0.01367753\n",
      "Epoch:  21 Train Loss: 0.01042879 Val Loss: 0.00810393\n",
      "Epoch:  22 Train Loss: 0.00979712 Val Loss: 0.01311893\n",
      "Epoch:  23 Train Loss: 0.00992150 Val Loss: 0.00887300\n",
      "Epoch:  24 Train Loss: 0.00891194 Val Loss: 0.00815459\n",
      "Epoch:  25 Train Loss: 0.00786367 Val Loss: 0.00945096\n",
      "Epoch:  26 Train Loss: 0.00860835 Val Loss: 0.00803411\n",
      "Epoch:  27 Train Loss: 0.00835814 Val Loss: 0.00798894\n",
      "Epoch:  28 Train Loss: 0.01033140 Val Loss: 0.00888311\n",
      "Epoch:  29 Train Loss: 0.00944642 Val Loss: 0.01090456\n",
      "Epoch:  30 Train Loss: 0.00951974 Val Loss: 0.00819210\n",
      "Epoch:  31 Train Loss: 0.00765674 Val Loss: 0.00828345\n",
      "Epoch:  32 Train Loss: 0.00927896 Val Loss: 0.00872573\n",
      "Epoch:  33 Train Loss: 0.00830948 Val Loss: 0.00774176\n",
      "Epoch:  34 Train Loss: 0.00846961 Val Loss: 0.00897866\n",
      "Epoch:  35 Train Loss: 0.00915219 Val Loss: 0.00850990\n",
      "Epoch:  36 Train Loss: 0.00979054 Val Loss: 0.00779577\n",
      "Epoch:  37 Train Loss: 0.00990364 Val Loss: 0.00814180\n",
      "Epoch:  38 Train Loss: 0.00867502 Val Loss: 0.00980404\n",
      "Epoch:  39 Train Loss: 0.00928561 Val Loss: 0.01283473\n",
      "Epoch:  40 Train Loss: 0.01212670 Val Loss: 0.01098520\n",
      "Epoch:  41 Train Loss: 0.01066944 Val Loss: 0.01431706\n",
      "Epoch:  42 Train Loss: 0.01127933 Val Loss: 0.00850120\n",
      "Epoch:  43 Train Loss: 0.00966640 Val Loss: 0.00794176\n",
      "Epoch:  44 Train Loss: 0.00888480 Val Loss: 0.00885532\n",
      "Epoch:  45 Train Loss: 0.00918886 Val Loss: 0.00860943\n",
      "Epoch:  46 Train Loss: 0.00801036 Val Loss: 0.00891834\n",
      "Epoch:  47 Train Loss: 0.00933806 Val Loss: 0.01251372\n",
      "Epoch:  48 Train Loss: 0.01387625 Val Loss: 0.01420889\n",
      "Epoch:  49 Train Loss: 0.01086721 Val Loss: 0.00984898\n",
      "Epoch:  50 Train Loss: 0.01047555 Val Loss: 0.00977610\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.08048257 Val Loss: 0.04353231\n",
      "Epoch:   2 Train Loss: 0.04028064 Val Loss: 0.02339679\n",
      "Epoch:   3 Train Loss: 0.03453018 Val Loss: 0.01644869\n",
      "Epoch:   4 Train Loss: 0.01645557 Val Loss: 0.01417854\n",
      "Epoch:   5 Train Loss: 0.01417971 Val Loss: 0.01004915\n",
      "Epoch:   6 Train Loss: 0.01298699 Val Loss: 0.01050398\n",
      "Epoch:   7 Train Loss: 0.01065046 Val Loss: 0.00914878\n",
      "Epoch:   8 Train Loss: 0.00941607 Val Loss: 0.00953345\n",
      "Epoch:   9 Train Loss: 0.00992004 Val Loss: 0.00954138\n",
      "Epoch:  10 Train Loss: 0.00910246 Val Loss: 0.00944272\n",
      "Epoch:  11 Train Loss: 0.01057214 Val Loss: 0.00891056\n",
      "Epoch:  12 Train Loss: 0.01021622 Val Loss: 0.00871288\n",
      "Epoch:  13 Train Loss: 0.00953938 Val Loss: 0.00876781\n",
      "Epoch:  14 Train Loss: 0.01093875 Val Loss: 0.00962387\n",
      "Epoch:  15 Train Loss: 0.01020969 Val Loss: 0.01243014\n",
      "Epoch:  16 Train Loss: 0.01087308 Val Loss: 0.01630635\n",
      "Epoch:  17 Train Loss: 0.01312558 Val Loss: 0.00928307\n",
      "Epoch:  18 Train Loss: 0.00938814 Val Loss: 0.00843387\n",
      "Epoch:  19 Train Loss: 0.01019019 Val Loss: 0.00843369\n",
      "Epoch:  20 Train Loss: 0.00958080 Val Loss: 0.01145633\n",
      "Epoch:  21 Train Loss: 0.01095273 Val Loss: 0.01448688\n",
      "Epoch:  22 Train Loss: 0.01340380 Val Loss: 0.00927397\n",
      "Epoch:  23 Train Loss: 0.00969568 Val Loss: 0.00885713\n",
      "Epoch:  24 Train Loss: 0.00912529 Val Loss: 0.00836165\n",
      "Epoch:  25 Train Loss: 0.00947589 Val Loss: 0.00813319\n",
      "Epoch:  26 Train Loss: 0.00924637 Val Loss: 0.01102380\n",
      "Epoch:  27 Train Loss: 0.00896598 Val Loss: 0.00952916\n",
      "Epoch:  28 Train Loss: 0.00978680 Val Loss: 0.00891836\n",
      "Epoch:  29 Train Loss: 0.00828170 Val Loss: 0.00830763\n",
      "Epoch:  30 Train Loss: 0.00786797 Val Loss: 0.00819705\n",
      "Epoch:  31 Train Loss: 0.00923579 Val Loss: 0.00785584\n",
      "Epoch:  32 Train Loss: 0.00945022 Val Loss: 0.00776265\n",
      "Epoch:  33 Train Loss: 0.00848860 Val Loss: 0.01138690\n",
      "Epoch:  34 Train Loss: 0.01449950 Val Loss: 0.01116728\n",
      "Epoch:  35 Train Loss: 0.01137239 Val Loss: 0.01169492\n",
      "Epoch:  36 Train Loss: 0.01225710 Val Loss: 0.01010058\n",
      "Epoch:  37 Train Loss: 0.01039915 Val Loss: 0.00854178\n",
      "Epoch:  38 Train Loss: 0.01087602 Val Loss: 0.00845737\n",
      "Epoch:  39 Train Loss: 0.00842238 Val Loss: 0.00953848\n",
      "Epoch:  40 Train Loss: 0.00880986 Val Loss: 0.00953033\n",
      "Epoch:  41 Train Loss: 0.00860943 Val Loss: 0.00804355\n",
      "Epoch:  42 Train Loss: 0.01003900 Val Loss: 0.00966756\n",
      "Epoch:  43 Train Loss: 0.01242163 Val Loss: 0.00914622\n",
      "Epoch:  44 Train Loss: 0.00965465 Val Loss: 0.00829666\n",
      "Epoch:  45 Train Loss: 0.00784830 Val Loss: 0.00803997\n",
      "Epoch:  46 Train Loss: 0.00917278 Val Loss: 0.00785170\n",
      "Epoch:  47 Train Loss: 0.01007329 Val Loss: 0.00825599\n",
      "Epoch:  48 Train Loss: 0.00902450 Val Loss: 0.00927221\n",
      "Epoch:  49 Train Loss: 0.00839913 Val Loss: 0.00864637\n",
      "Epoch:  50 Train Loss: 0.00881113 Val Loss: 0.00967309\n",
      "Epoch:  51 Train Loss: 0.00989852 Val Loss: 0.00780623\n",
      "Epoch:  52 Train Loss: 0.00965899 Val Loss: 0.00837217\n",
      "Epoch:  53 Train Loss: 0.00809522 Val Loss: 0.00811252\n",
      "Epoch:  54 Train Loss: 0.00685399 Val Loss: 0.00885773\n",
      "Epoch:  55 Train Loss: 0.00907009 Val Loss: 0.00770094\n",
      "Epoch:  56 Train Loss: 0.00734485 Val Loss: 0.00764070\n",
      "Epoch:  57 Train Loss: 0.00737316 Val Loss: 0.00902085\n",
      "Epoch:  58 Train Loss: 0.00773203 Val Loss: 0.00850949\n",
      "Epoch:  59 Train Loss: 0.00869719 Val Loss: 0.00758305\n",
      "Epoch:  60 Train Loss: 0.00887590 Val Loss: 0.00929783\n",
      "Epoch:  61 Train Loss: 0.00754370 Val Loss: 0.00890483\n",
      "Epoch:  62 Train Loss: 0.00888613 Val Loss: 0.00753284\n",
      "Epoch:  63 Train Loss: 0.01080078 Val Loss: 0.00750175\n",
      "Epoch:  64 Train Loss: 0.00812174 Val Loss: 0.00724090\n",
      "Epoch:  65 Train Loss: 0.00765734 Val Loss: 0.00762436\n",
      "Epoch:  66 Train Loss: 0.00783016 Val Loss: 0.00761784\n",
      "Epoch:  67 Train Loss: 0.00770554 Val Loss: 0.00777799\n",
      "Epoch:  68 Train Loss: 0.00883580 Val Loss: 0.00751687\n",
      "Epoch:  69 Train Loss: 0.00721599 Val Loss: 0.01054851\n",
      "Epoch:  70 Train Loss: 0.00811967 Val Loss: 0.00811421\n",
      "Epoch:  71 Train Loss: 0.00856177 Val Loss: 0.00894701\n",
      "Epoch:  72 Train Loss: 0.00958279 Val Loss: 0.01033449\n",
      "Epoch:  73 Train Loss: 0.00841304 Val Loss: 0.00811292\n",
      "Epoch:  74 Train Loss: 0.00645225 Val Loss: 0.00789703\n",
      "Epoch:  75 Train Loss: 0.00803442 Val Loss: 0.00740636\n",
      "Epoch:  76 Train Loss: 0.01085353 Val Loss: 0.00918165\n",
      "Epoch:  77 Train Loss: 0.00728304 Val Loss: 0.00827779\n",
      "Epoch:  78 Train Loss: 0.00841480 Val Loss: 0.00823525\n",
      "Epoch:  79 Train Loss: 0.00653813 Val Loss: 0.00728113\n",
      "Epoch:  80 Train Loss: 0.00642029 Val Loss: 0.00703956\n",
      "Epoch:  81 Train Loss: 0.00787247 Val Loss: 0.00814067\n",
      "Epoch:  82 Train Loss: 0.00747564 Val Loss: 0.00851201\n",
      "Epoch:  83 Train Loss: 0.00720744 Val Loss: 0.00730008\n",
      "Epoch:  84 Train Loss: 0.00736134 Val Loss: 0.00731920\n",
      "Epoch:  85 Train Loss: 0.00828417 Val Loss: 0.00805727\n",
      "Epoch:  86 Train Loss: 0.00771473 Val Loss: 0.00794297\n",
      "Epoch:  87 Train Loss: 0.00823194 Val Loss: 0.00734247\n",
      "Epoch:  88 Train Loss: 0.00835362 Val Loss: 0.00793050\n",
      "Epoch:  89 Train Loss: 0.00883153 Val Loss: 0.00688257\n",
      "Epoch:  90 Train Loss: 0.00672843 Val Loss: 0.00794620\n",
      "Epoch:  91 Train Loss: 0.00730521 Val Loss: 0.00686312\n",
      "Epoch:  92 Train Loss: 0.00675674 Val Loss: 0.00790804\n",
      "Epoch:  93 Train Loss: 0.00825652 Val Loss: 0.00756018\n",
      "Epoch:  94 Train Loss: 0.00815332 Val Loss: 0.00874816\n",
      "Epoch:  95 Train Loss: 0.00651561 Val Loss: 0.00749001\n",
      "Epoch:  96 Train Loss: 0.00691591 Val Loss: 0.00728290\n",
      "Epoch:  97 Train Loss: 0.00632194 Val Loss: 0.00730863\n",
      "Epoch:  98 Train Loss: 0.00798060 Val Loss: 0.00844411\n",
      "Epoch:  99 Train Loss: 0.00794686 Val Loss: 0.00656173\n",
      "Epoch: 100 Train Loss: 0.00677190 Val Loss: 0.00651268\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.22145990 Val Loss: 0.18296783\n",
      "Epoch:   2 Train Loss: 0.15053208 Val Loss: 0.09333929\n",
      "Epoch:   3 Train Loss: 0.06666277 Val Loss: 0.02690938\n",
      "Epoch:   4 Train Loss: 0.04940307 Val Loss: 0.02676696\n",
      "Epoch:   5 Train Loss: 0.04153148 Val Loss: 0.02810261\n",
      "Epoch:   6 Train Loss: 0.04058154 Val Loss: 0.02585950\n",
      "Epoch:   7 Train Loss: 0.03618455 Val Loss: 0.01985044\n",
      "Epoch:   8 Train Loss: 0.02836615 Val Loss: 0.01693456\n",
      "Epoch:   9 Train Loss: 0.02373767 Val Loss: 0.01509012\n",
      "Epoch:  10 Train Loss: 0.01877070 Val Loss: 0.00995838\n",
      "Epoch:  11 Train Loss: 0.01397527 Val Loss: 0.00918396\n",
      "Epoch:  12 Train Loss: 0.00977217 Val Loss: 0.00837414\n",
      "Epoch:  13 Train Loss: 0.00853659 Val Loss: 0.00880939\n",
      "Epoch:  14 Train Loss: 0.01380487 Val Loss: 0.01249766\n",
      "Epoch:  15 Train Loss: 0.01039599 Val Loss: 0.00980296\n",
      "Epoch:  16 Train Loss: 0.01076229 Val Loss: 0.00794311\n",
      "Epoch:  17 Train Loss: 0.01000768 Val Loss: 0.01035043\n",
      "Epoch:  18 Train Loss: 0.01134846 Val Loss: 0.00791005\n",
      "Epoch:  19 Train Loss: 0.00816051 Val Loss: 0.00789865\n",
      "Epoch:  20 Train Loss: 0.00850660 Val Loss: 0.00852162\n",
      "Epoch:  21 Train Loss: 0.01006784 Val Loss: 0.00889253\n",
      "Epoch:  22 Train Loss: 0.01167428 Val Loss: 0.00731314\n",
      "Epoch:  23 Train Loss: 0.00986462 Val Loss: 0.00847561\n",
      "Epoch:  24 Train Loss: 0.00878302 Val Loss: 0.00737872\n",
      "Epoch:  25 Train Loss: 0.01136575 Val Loss: 0.01082235\n",
      "Epoch:  26 Train Loss: 0.01030430 Val Loss: 0.00757344\n",
      "Epoch:  27 Train Loss: 0.01093749 Val Loss: 0.00850388\n",
      "Epoch:  28 Train Loss: 0.00767169 Val Loss: 0.00785223\n",
      "Epoch:  29 Train Loss: 0.00770493 Val Loss: 0.00803421\n",
      "Epoch:  30 Train Loss: 0.00937181 Val Loss: 0.00726511\n",
      "Epoch:  31 Train Loss: 0.01132874 Val Loss: 0.00809556\n",
      "Epoch:  32 Train Loss: 0.00946288 Val Loss: 0.00849039\n",
      "Epoch:  33 Train Loss: 0.00873338 Val Loss: 0.00730950\n",
      "Epoch:  34 Train Loss: 0.01119898 Val Loss: 0.00941076\n",
      "Epoch:  35 Train Loss: 0.01071226 Val Loss: 0.00721389\n",
      "Epoch:  36 Train Loss: 0.01031503 Val Loss: 0.00701427\n",
      "Epoch:  37 Train Loss: 0.00866155 Val Loss: 0.00713890\n",
      "Epoch:  38 Train Loss: 0.00922997 Val Loss: 0.00807598\n",
      "Epoch:  39 Train Loss: 0.01017425 Val Loss: 0.00794181\n",
      "Epoch:  40 Train Loss: 0.00955540 Val Loss: 0.00736065\n",
      "Epoch:  41 Train Loss: 0.00867914 Val Loss: 0.00698041\n",
      "Epoch:  42 Train Loss: 0.00829977 Val Loss: 0.00777322\n",
      "Epoch:  43 Train Loss: 0.00931026 Val Loss: 0.00682815\n",
      "Epoch:  44 Train Loss: 0.00922585 Val Loss: 0.00870129\n",
      "Epoch:  45 Train Loss: 0.00904378 Val Loss: 0.00685996\n",
      "Epoch:  46 Train Loss: 0.00973254 Val Loss: 0.00820618\n",
      "Epoch:  47 Train Loss: 0.00888331 Val Loss: 0.00677737\n",
      "Epoch:  48 Train Loss: 0.01003442 Val Loss: 0.00792265\n",
      "Epoch:  49 Train Loss: 0.00831450 Val Loss: 0.00683883\n",
      "Epoch:  50 Train Loss: 0.00924623 Val Loss: 0.00795066\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.16626190 Val Loss: 0.13229048\n",
      "Epoch:   2 Train Loss: 0.10833529 Val Loss: 0.06341043\n",
      "Epoch:   3 Train Loss: 0.06187783 Val Loss: 0.03175350\n",
      "Epoch:   4 Train Loss: 0.05255928 Val Loss: 0.02777110\n",
      "Epoch:   5 Train Loss: 0.04797470 Val Loss: 0.02995778\n",
      "Epoch:   6 Train Loss: 0.03909627 Val Loss: 0.02501834\n",
      "Epoch:   7 Train Loss: 0.03414585 Val Loss: 0.02016045\n",
      "Epoch:   8 Train Loss: 0.02896990 Val Loss: 0.01613439\n",
      "Epoch:   9 Train Loss: 0.01931584 Val Loss: 0.01090053\n",
      "Epoch:  10 Train Loss: 0.01293577 Val Loss: 0.01104222\n",
      "Epoch:  11 Train Loss: 0.01317018 Val Loss: 0.00908057\n",
      "Epoch:  12 Train Loss: 0.01179823 Val Loss: 0.00872217\n",
      "Epoch:  13 Train Loss: 0.01228635 Val Loss: 0.01203016\n",
      "Epoch:  14 Train Loss: 0.01064834 Val Loss: 0.00781149\n",
      "Epoch:  15 Train Loss: 0.01122046 Val Loss: 0.01035843\n",
      "Epoch:  16 Train Loss: 0.01014304 Val Loss: 0.00759634\n",
      "Epoch:  17 Train Loss: 0.01058464 Val Loss: 0.01053965\n",
      "Epoch:  18 Train Loss: 0.01142545 Val Loss: 0.00740147\n",
      "Epoch:  19 Train Loss: 0.00870326 Val Loss: 0.01005884\n",
      "Epoch:  20 Train Loss: 0.01057975 Val Loss: 0.00787227\n",
      "Epoch:  21 Train Loss: 0.00955076 Val Loss: 0.00794372\n",
      "Epoch:  22 Train Loss: 0.01037967 Val Loss: 0.00942028\n",
      "Epoch:  23 Train Loss: 0.00807711 Val Loss: 0.00733037\n",
      "Epoch:  24 Train Loss: 0.00995936 Val Loss: 0.00995798\n",
      "Epoch:  25 Train Loss: 0.01270589 Val Loss: 0.00812594\n",
      "Epoch:  26 Train Loss: 0.01004959 Val Loss: 0.00847558\n",
      "Epoch:  27 Train Loss: 0.00847477 Val Loss: 0.00714578\n",
      "Epoch:  28 Train Loss: 0.00872156 Val Loss: 0.00906011\n",
      "Epoch:  29 Train Loss: 0.00892913 Val Loss: 0.00694360\n",
      "Epoch:  30 Train Loss: 0.00860838 Val Loss: 0.00794115\n",
      "Epoch:  31 Train Loss: 0.01031696 Val Loss: 0.00699967\n",
      "Epoch:  32 Train Loss: 0.00937071 Val Loss: 0.00731226\n",
      "Epoch:  33 Train Loss: 0.00826348 Val Loss: 0.00686245\n",
      "Epoch:  34 Train Loss: 0.00900761 Val Loss: 0.00711995\n",
      "Epoch:  35 Train Loss: 0.00735981 Val Loss: 0.00680176\n",
      "Epoch:  36 Train Loss: 0.00761759 Val Loss: 0.00722760\n",
      "Epoch:  37 Train Loss: 0.01053301 Val Loss: 0.00718013\n",
      "Epoch:  38 Train Loss: 0.00870871 Val Loss: 0.00679168\n",
      "Epoch:  39 Train Loss: 0.00883108 Val Loss: 0.00714931\n",
      "Epoch:  40 Train Loss: 0.00950775 Val Loss: 0.00693137\n",
      "Epoch:  41 Train Loss: 0.00872118 Val Loss: 0.00712830\n",
      "Epoch:  42 Train Loss: 0.00678313 Val Loss: 0.00678095\n",
      "Epoch:  43 Train Loss: 0.00767210 Val Loss: 0.00676316\n",
      "Epoch:  44 Train Loss: 0.00912718 Val Loss: 0.00706324\n",
      "Epoch:  45 Train Loss: 0.00957425 Val Loss: 0.00685191\n",
      "Epoch:  46 Train Loss: 0.01000930 Val Loss: 0.00704254\n",
      "Epoch:  47 Train Loss: 0.00889731 Val Loss: 0.00676614\n",
      "Epoch:  48 Train Loss: 0.00864096 Val Loss: 0.00676029\n",
      "Epoch:  49 Train Loss: 0.00710109 Val Loss: 0.00715146\n",
      "Epoch:  50 Train Loss: 0.00809825 Val Loss: 0.00703909\n",
      "Epoch:  51 Train Loss: 0.00827813 Val Loss: 0.00681056\n",
      "Epoch:  52 Train Loss: 0.00731769 Val Loss: 0.00677571\n",
      "Epoch:  53 Train Loss: 0.00769822 Val Loss: 0.00729857\n",
      "Epoch:  54 Train Loss: 0.00784853 Val Loss: 0.00674106\n",
      "Epoch:  55 Train Loss: 0.00746401 Val Loss: 0.00678681\n",
      "Epoch:  56 Train Loss: 0.00672452 Val Loss: 0.00674580\n",
      "Epoch:  57 Train Loss: 0.00760222 Val Loss: 0.00826564\n",
      "Epoch:  58 Train Loss: 0.00769062 Val Loss: 0.00677604\n",
      "Epoch:  59 Train Loss: 0.00806250 Val Loss: 0.00736936\n",
      "Epoch:  60 Train Loss: 0.00809055 Val Loss: 0.00667947\n",
      "Epoch:  61 Train Loss: 0.00872040 Val Loss: 0.00678446\n",
      "Epoch:  62 Train Loss: 0.00749398 Val Loss: 0.00699817\n",
      "Epoch:  63 Train Loss: 0.00735225 Val Loss: 0.00669471\n",
      "Epoch:  64 Train Loss: 0.00682085 Val Loss: 0.00700730\n",
      "Epoch:  65 Train Loss: 0.00800493 Val Loss: 0.00664461\n",
      "Epoch:  66 Train Loss: 0.00700202 Val Loss: 0.00664101\n",
      "Epoch:  67 Train Loss: 0.00742546 Val Loss: 0.00694979\n",
      "Epoch:  68 Train Loss: 0.00641541 Val Loss: 0.00670770\n",
      "Epoch:  69 Train Loss: 0.00707803 Val Loss: 0.00722353\n",
      "Epoch:  70 Train Loss: 0.00918649 Val Loss: 0.00678292\n",
      "Epoch:  71 Train Loss: 0.00789600 Val Loss: 0.00728259\n",
      "Epoch:  72 Train Loss: 0.00876763 Val Loss: 0.00673009\n",
      "Epoch:  73 Train Loss: 0.00695335 Val Loss: 0.00669204\n",
      "Epoch:  74 Train Loss: 0.00701785 Val Loss: 0.00688188\n",
      "Epoch:  75 Train Loss: 0.00930099 Val Loss: 0.00674662\n",
      "Epoch:  76 Train Loss: 0.00996252 Val Loss: 0.00739915\n",
      "Epoch:  77 Train Loss: 0.00808262 Val Loss: 0.00678713\n",
      "Epoch:  78 Train Loss: 0.00700889 Val Loss: 0.00724310\n",
      "Epoch:  79 Train Loss: 0.00851999 Val Loss: 0.00674807\n",
      "Epoch:  80 Train Loss: 0.00787852 Val Loss: 0.00681667\n",
      "Epoch:  81 Train Loss: 0.00745433 Val Loss: 0.00738207\n",
      "Epoch:  82 Train Loss: 0.00776183 Val Loss: 0.00709269\n",
      "Epoch:  83 Train Loss: 0.00810095 Val Loss: 0.00833363\n",
      "Epoch:  84 Train Loss: 0.00907110 Val Loss: 0.00674538\n",
      "Epoch:  85 Train Loss: 0.00841123 Val Loss: 0.00744323\n",
      "Epoch:  86 Train Loss: 0.00753541 Val Loss: 0.00702470\n",
      "Epoch:  87 Train Loss: 0.00878725 Val Loss: 0.00810478\n",
      "Epoch:  88 Train Loss: 0.00828080 Val Loss: 0.00663154\n",
      "Epoch:  89 Train Loss: 0.00856056 Val Loss: 0.00663809\n",
      "Epoch:  90 Train Loss: 0.00775508 Val Loss: 0.00656398\n",
      "Epoch:  91 Train Loss: 0.00744269 Val Loss: 0.00660595\n",
      "Epoch:  92 Train Loss: 0.00720152 Val Loss: 0.00772287\n",
      "Epoch:  93 Train Loss: 0.00866352 Val Loss: 0.00671131\n",
      "Epoch:  94 Train Loss: 0.00687980 Val Loss: 0.00674429\n",
      "Epoch:  95 Train Loss: 0.00717986 Val Loss: 0.00656138\n",
      "Epoch:  96 Train Loss: 0.00706134 Val Loss: 0.00663520\n",
      "Epoch:  97 Train Loss: 0.00799593 Val Loss: 0.00694710\n",
      "Epoch:  98 Train Loss: 0.00743036 Val Loss: 0.00667904\n",
      "Epoch:  99 Train Loss: 0.00708529 Val Loss: 0.00721762\n",
      "Epoch: 100 Train Loss: 0.00960186 Val Loss: 0.00669655\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.08010420 Val Loss: 0.04428787\n",
      "Epoch:   2 Train Loss: 0.04414748 Val Loss: 0.02319207\n",
      "Epoch:   3 Train Loss: 0.02941242 Val Loss: 0.01877952\n",
      "Epoch:   4 Train Loss: 0.02182419 Val Loss: 0.01183266\n",
      "Epoch:   5 Train Loss: 0.01427509 Val Loss: 0.00922231\n",
      "Epoch:   6 Train Loss: 0.01241812 Val Loss: 0.01115811\n",
      "Epoch:   7 Train Loss: 0.00992235 Val Loss: 0.00899507\n",
      "Epoch:   8 Train Loss: 0.00944762 Val Loss: 0.00901273\n",
      "Epoch:   9 Train Loss: 0.01013291 Val Loss: 0.00904467\n",
      "Epoch:  10 Train Loss: 0.01352827 Val Loss: 0.00866915\n",
      "Epoch:  11 Train Loss: 0.01008847 Val Loss: 0.00955799\n",
      "Epoch:  12 Train Loss: 0.01096764 Val Loss: 0.00858156\n",
      "Epoch:  13 Train Loss: 0.00983640 Val Loss: 0.00885950\n",
      "Epoch:  14 Train Loss: 0.01004583 Val Loss: 0.00864173\n",
      "Epoch:  15 Train Loss: 0.00945088 Val Loss: 0.00836664\n",
      "Epoch:  16 Train Loss: 0.00999096 Val Loss: 0.00857844\n",
      "Epoch:  17 Train Loss: 0.00946243 Val Loss: 0.00855969\n",
      "Epoch:  18 Train Loss: 0.00978859 Val Loss: 0.01190040\n",
      "Epoch:  19 Train Loss: 0.01120103 Val Loss: 0.00919377\n",
      "Epoch:  20 Train Loss: 0.01136394 Val Loss: 0.00917520\n",
      "Epoch:  21 Train Loss: 0.00991055 Val Loss: 0.00992959\n",
      "Epoch:  22 Train Loss: 0.00896261 Val Loss: 0.00837011\n",
      "Epoch:  23 Train Loss: 0.00893406 Val Loss: 0.00965120\n",
      "Epoch:  24 Train Loss: 0.01023569 Val Loss: 0.00867659\n",
      "Epoch:  25 Train Loss: 0.01114660 Val Loss: 0.01054601\n",
      "Epoch:  26 Train Loss: 0.01208675 Val Loss: 0.00915823\n",
      "Epoch:  27 Train Loss: 0.01365710 Val Loss: 0.01151737\n",
      "Epoch:  28 Train Loss: 0.01278838 Val Loss: 0.00956200\n",
      "Epoch:  29 Train Loss: 0.01465824 Val Loss: 0.01049640\n",
      "Epoch:  30 Train Loss: 0.01167353 Val Loss: 0.01009476\n",
      "Epoch:  31 Train Loss: 0.01101522 Val Loss: 0.01088184\n",
      "Epoch:  32 Train Loss: 0.01152492 Val Loss: 0.00906253\n",
      "Epoch:  33 Train Loss: 0.01041990 Val Loss: 0.00948375\n",
      "Epoch:  34 Train Loss: 0.00860664 Val Loss: 0.01023800\n",
      "Epoch:  35 Train Loss: 0.01067268 Val Loss: 0.01202191\n",
      "Epoch:  36 Train Loss: 0.01131246 Val Loss: 0.00921734\n",
      "Epoch:  37 Train Loss: 0.00970323 Val Loss: 0.00906994\n",
      "Epoch:  38 Train Loss: 0.01103572 Val Loss: 0.01349515\n",
      "Epoch:  39 Train Loss: 0.01150072 Val Loss: 0.01375134\n",
      "Epoch:  40 Train Loss: 0.01175649 Val Loss: 0.00915432\n",
      "Epoch:  41 Train Loss: 0.00874713 Val Loss: 0.00825689\n",
      "Epoch:  42 Train Loss: 0.00803193 Val Loss: 0.00864562\n",
      "Epoch:  43 Train Loss: 0.00820153 Val Loss: 0.00818130\n",
      "Epoch:  44 Train Loss: 0.00900830 Val Loss: 0.00845371\n",
      "Epoch:  45 Train Loss: 0.00823201 Val Loss: 0.00905708\n",
      "Epoch:  46 Train Loss: 0.00826759 Val Loss: 0.00828484\n",
      "Epoch:  47 Train Loss: 0.00791795 Val Loss: 0.00880705\n",
      "Epoch:  48 Train Loss: 0.00916439 Val Loss: 0.00797441\n",
      "Epoch:  49 Train Loss: 0.00827725 Val Loss: 0.00865283\n",
      "Epoch:  50 Train Loss: 0.00860844 Val Loss: 0.00969812\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.11360369 Val Loss: 0.05339987\n",
      "Epoch:   2 Train Loss: 0.04517803 Val Loss: 0.02317731\n",
      "Epoch:   3 Train Loss: 0.03232871 Val Loss: 0.01871458\n",
      "Epoch:   4 Train Loss: 0.02557656 Val Loss: 0.01516402\n",
      "Epoch:   5 Train Loss: 0.01408085 Val Loss: 0.01029803\n",
      "Epoch:   6 Train Loss: 0.01353460 Val Loss: 0.00923659\n",
      "Epoch:   7 Train Loss: 0.01207876 Val Loss: 0.01242047\n",
      "Epoch:   8 Train Loss: 0.01293888 Val Loss: 0.01099412\n",
      "Epoch:   9 Train Loss: 0.01099736 Val Loss: 0.01075879\n",
      "Epoch:  10 Train Loss: 0.01454339 Val Loss: 0.00935845\n",
      "Epoch:  11 Train Loss: 0.01220733 Val Loss: 0.00981745\n",
      "Epoch:  12 Train Loss: 0.01137352 Val Loss: 0.00960931\n",
      "Epoch:  13 Train Loss: 0.00933566 Val Loss: 0.00911176\n",
      "Epoch:  14 Train Loss: 0.01031914 Val Loss: 0.00881901\n",
      "Epoch:  15 Train Loss: 0.00986078 Val Loss: 0.00875429\n",
      "Epoch:  16 Train Loss: 0.00979318 Val Loss: 0.00853094\n",
      "Epoch:  17 Train Loss: 0.00980969 Val Loss: 0.00914386\n",
      "Epoch:  18 Train Loss: 0.00966426 Val Loss: 0.01006093\n",
      "Epoch:  19 Train Loss: 0.01186776 Val Loss: 0.01083947\n",
      "Epoch:  20 Train Loss: 0.01414906 Val Loss: 0.01034918\n",
      "Epoch:  21 Train Loss: 0.01128716 Val Loss: 0.00849020\n",
      "Epoch:  22 Train Loss: 0.01095825 Val Loss: 0.01017431\n",
      "Epoch:  23 Train Loss: 0.01035039 Val Loss: 0.00993399\n",
      "Epoch:  24 Train Loss: 0.00893183 Val Loss: 0.01201700\n",
      "Epoch:  25 Train Loss: 0.01006915 Val Loss: 0.00829117\n",
      "Epoch:  26 Train Loss: 0.00801050 Val Loss: 0.00871398\n",
      "Epoch:  27 Train Loss: 0.00957588 Val Loss: 0.00834671\n",
      "Epoch:  28 Train Loss: 0.00781120 Val Loss: 0.00818380\n",
      "Epoch:  29 Train Loss: 0.00840615 Val Loss: 0.00809125\n",
      "Epoch:  30 Train Loss: 0.01128428 Val Loss: 0.00859767\n",
      "Epoch:  31 Train Loss: 0.00971100 Val Loss: 0.00846228\n",
      "Epoch:  32 Train Loss: 0.01017542 Val Loss: 0.00848005\n",
      "Epoch:  33 Train Loss: 0.00995528 Val Loss: 0.00787553\n",
      "Epoch:  34 Train Loss: 0.01019515 Val Loss: 0.01215881\n",
      "Epoch:  35 Train Loss: 0.01080327 Val Loss: 0.01214192\n",
      "Epoch:  36 Train Loss: 0.01187595 Val Loss: 0.01061812\n",
      "Epoch:  37 Train Loss: 0.01265644 Val Loss: 0.01310687\n",
      "Epoch:  38 Train Loss: 0.01052280 Val Loss: 0.00901268\n",
      "Epoch:  39 Train Loss: 0.01015877 Val Loss: 0.01112502\n",
      "Epoch:  40 Train Loss: 0.01305159 Val Loss: 0.01074049\n",
      "Epoch:  41 Train Loss: 0.01098276 Val Loss: 0.01046472\n",
      "Epoch:  42 Train Loss: 0.01172457 Val Loss: 0.01130373\n",
      "Epoch:  43 Train Loss: 0.01077667 Val Loss: 0.01209888\n",
      "Epoch:  44 Train Loss: 0.01102114 Val Loss: 0.00811169\n",
      "Epoch:  45 Train Loss: 0.01193687 Val Loss: 0.00857466\n",
      "Epoch:  46 Train Loss: 0.01012632 Val Loss: 0.01500445\n",
      "Epoch:  47 Train Loss: 0.01302068 Val Loss: 0.01156284\n",
      "Epoch:  48 Train Loss: 0.01230995 Val Loss: 0.01212224\n",
      "Epoch:  49 Train Loss: 0.00910073 Val Loss: 0.00887908\n",
      "Epoch:  50 Train Loss: 0.00845996 Val Loss: 0.00909286\n",
      "Epoch:  51 Train Loss: 0.00912161 Val Loss: 0.00842443\n",
      "Epoch:  52 Train Loss: 0.00898041 Val Loss: 0.01014938\n",
      "Epoch:  53 Train Loss: 0.01041621 Val Loss: 0.00890763\n",
      "Epoch:  54 Train Loss: 0.00907196 Val Loss: 0.00913397\n",
      "Epoch:  55 Train Loss: 0.00909015 Val Loss: 0.00786912\n",
      "Epoch:  56 Train Loss: 0.00981843 Val Loss: 0.00786525\n",
      "Epoch:  57 Train Loss: 0.00875209 Val Loss: 0.00787633\n",
      "Epoch:  58 Train Loss: 0.00710371 Val Loss: 0.00769538\n",
      "Epoch:  59 Train Loss: 0.00833538 Val Loss: 0.00862231\n",
      "Epoch:  60 Train Loss: 0.00795504 Val Loss: 0.00787222\n",
      "Epoch:  61 Train Loss: 0.00898824 Val Loss: 0.00825785\n",
      "Epoch:  62 Train Loss: 0.01068319 Val Loss: 0.00933282\n",
      "Epoch:  63 Train Loss: 0.00763978 Val Loss: 0.00751981\n",
      "Epoch:  64 Train Loss: 0.00941326 Val Loss: 0.00884508\n",
      "Epoch:  65 Train Loss: 0.00970905 Val Loss: 0.00766651\n",
      "Epoch:  66 Train Loss: 0.00825287 Val Loss: 0.00898045\n",
      "Epoch:  67 Train Loss: 0.00782914 Val Loss: 0.00744307\n",
      "Epoch:  68 Train Loss: 0.00692929 Val Loss: 0.00737746\n",
      "Epoch:  69 Train Loss: 0.00832781 Val Loss: 0.00903175\n",
      "Epoch:  70 Train Loss: 0.01009945 Val Loss: 0.00771352\n",
      "Epoch:  71 Train Loss: 0.00794756 Val Loss: 0.00768701\n",
      "Epoch:  72 Train Loss: 0.00749194 Val Loss: 0.00786817\n",
      "Epoch:  73 Train Loss: 0.00809139 Val Loss: 0.00846509\n",
      "Epoch:  74 Train Loss: 0.00742952 Val Loss: 0.00812429\n",
      "Epoch:  75 Train Loss: 0.00957941 Val Loss: 0.00938660\n",
      "Epoch:  76 Train Loss: 0.00690438 Val Loss: 0.00760191\n",
      "Epoch:  77 Train Loss: 0.00745849 Val Loss: 0.00832207\n",
      "Epoch:  78 Train Loss: 0.00881029 Val Loss: 0.00716761\n",
      "Epoch:  79 Train Loss: 0.00916936 Val Loss: 0.00744472\n",
      "Epoch:  80 Train Loss: 0.01197736 Val Loss: 0.01132964\n",
      "Epoch:  81 Train Loss: 0.01140441 Val Loss: 0.00911803\n",
      "Epoch:  82 Train Loss: 0.00961741 Val Loss: 0.00731968\n",
      "Epoch:  83 Train Loss: 0.00854885 Val Loss: 0.00765884\n",
      "Epoch:  84 Train Loss: 0.00755896 Val Loss: 0.00929531\n",
      "Epoch:  85 Train Loss: 0.00846675 Val Loss: 0.00782997\n",
      "Epoch:  86 Train Loss: 0.00763068 Val Loss: 0.00762395\n",
      "Epoch:  87 Train Loss: 0.00642881 Val Loss: 0.00718498\n",
      "Epoch:  88 Train Loss: 0.00823751 Val Loss: 0.00887045\n",
      "Epoch:  89 Train Loss: 0.00848600 Val Loss: 0.00875912\n",
      "Epoch:  90 Train Loss: 0.00808837 Val Loss: 0.01029800\n",
      "Epoch:  91 Train Loss: 0.01198398 Val Loss: 0.00764843\n",
      "Epoch:  92 Train Loss: 0.00868311 Val Loss: 0.00840386\n",
      "Epoch:  93 Train Loss: 0.00807287 Val Loss: 0.00940834\n",
      "Epoch:  94 Train Loss: 0.00865999 Val Loss: 0.00731429\n",
      "Epoch:  95 Train Loss: 0.00742291 Val Loss: 0.00721652\n",
      "Epoch:  96 Train Loss: 0.00685313 Val Loss: 0.00822277\n",
      "Epoch:  97 Train Loss: 0.00731195 Val Loss: 0.00706867\n",
      "Epoch:  98 Train Loss: 0.00890614 Val Loss: 0.00981353\n",
      "Epoch:  99 Train Loss: 0.00881317 Val Loss: 0.00758718\n",
      "Epoch: 100 Train Loss: 0.00988029 Val Loss: 0.01414752\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.18247177 Val Loss: 0.09788052\n",
      "Epoch:   2 Train Loss: 0.06888616 Val Loss: 0.04079739\n",
      "Epoch:   3 Train Loss: 0.05861595 Val Loss: 0.03076609\n",
      "Epoch:   4 Train Loss: 0.04349950 Val Loss: 0.03271932\n",
      "Epoch:   5 Train Loss: 0.04249085 Val Loss: 0.02416444\n",
      "Epoch:   6 Train Loss: 0.03574076 Val Loss: 0.02102824\n",
      "Epoch:   7 Train Loss: 0.02700636 Val Loss: 0.01868181\n",
      "Epoch:   8 Train Loss: 0.01976756 Val Loss: 0.01199656\n",
      "Epoch:   9 Train Loss: 0.01411601 Val Loss: 0.01402630\n",
      "Epoch:  10 Train Loss: 0.01438555 Val Loss: 0.00927950\n",
      "Epoch:  11 Train Loss: 0.01074890 Val Loss: 0.00984340\n",
      "Epoch:  12 Train Loss: 0.00865136 Val Loss: 0.00843237\n",
      "Epoch:  13 Train Loss: 0.01059742 Val Loss: 0.00959446\n",
      "Epoch:  14 Train Loss: 0.01041620 Val Loss: 0.00798811\n",
      "Epoch:  15 Train Loss: 0.00850338 Val Loss: 0.00868074\n",
      "Epoch:  16 Train Loss: 0.00860291 Val Loss: 0.00797512\n",
      "Epoch:  17 Train Loss: 0.00747455 Val Loss: 0.00774824\n",
      "Epoch:  18 Train Loss: 0.00855165 Val Loss: 0.00853127\n",
      "Epoch:  19 Train Loss: 0.00824355 Val Loss: 0.00776693\n",
      "Epoch:  20 Train Loss: 0.00856021 Val Loss: 0.00796436\n",
      "Epoch:  21 Train Loss: 0.00820666 Val Loss: 0.00764238\n",
      "Epoch:  22 Train Loss: 0.00901037 Val Loss: 0.00769127\n",
      "Epoch:  23 Train Loss: 0.00734542 Val Loss: 0.00769701\n",
      "Epoch:  24 Train Loss: 0.00720792 Val Loss: 0.00773564\n",
      "Epoch:  25 Train Loss: 0.00959209 Val Loss: 0.00800719\n",
      "Epoch:  26 Train Loss: 0.00836649 Val Loss: 0.00777801\n",
      "Epoch:  27 Train Loss: 0.00904161 Val Loss: 0.00868821\n",
      "Epoch:  28 Train Loss: 0.00825142 Val Loss: 0.00749422\n",
      "Epoch:  29 Train Loss: 0.00788420 Val Loss: 0.00747915\n",
      "Epoch:  30 Train Loss: 0.00912269 Val Loss: 0.00911808\n",
      "Epoch:  31 Train Loss: 0.00905368 Val Loss: 0.00759092\n",
      "Epoch:  32 Train Loss: 0.00761747 Val Loss: 0.00759460\n",
      "Epoch:  33 Train Loss: 0.00884436 Val Loss: 0.00770995\n",
      "Epoch:  34 Train Loss: 0.00836266 Val Loss: 0.00749787\n",
      "Epoch:  35 Train Loss: 0.00664069 Val Loss: 0.00863066\n",
      "Epoch:  36 Train Loss: 0.00968830 Val Loss: 0.00736457\n",
      "Epoch:  37 Train Loss: 0.00772906 Val Loss: 0.00767334\n",
      "Epoch:  38 Train Loss: 0.00737707 Val Loss: 0.00872571\n",
      "Epoch:  39 Train Loss: 0.00828013 Val Loss: 0.00737483\n",
      "Epoch:  40 Train Loss: 0.00790965 Val Loss: 0.00732383\n",
      "Epoch:  41 Train Loss: 0.00747472 Val Loss: 0.00725340\n",
      "Epoch:  42 Train Loss: 0.01005415 Val Loss: 0.00744876\n",
      "Epoch:  43 Train Loss: 0.00864505 Val Loss: 0.00754079\n",
      "Epoch:  44 Train Loss: 0.00966209 Val Loss: 0.00859411\n",
      "Epoch:  45 Train Loss: 0.00823332 Val Loss: 0.00770757\n",
      "Epoch:  46 Train Loss: 0.00798100 Val Loss: 0.00767288\n",
      "Epoch:  47 Train Loss: 0.00708343 Val Loss: 0.00705617\n",
      "Epoch:  48 Train Loss: 0.00728812 Val Loss: 0.00707794\n",
      "Epoch:  49 Train Loss: 0.00949854 Val Loss: 0.00714280\n",
      "Epoch:  50 Train Loss: 0.00769080 Val Loss: 0.00760018\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.15094026 Val Loss: 0.07437022\n",
      "Epoch:   2 Train Loss: 0.05838106 Val Loss: 0.04099738\n",
      "Epoch:   3 Train Loss: 0.05252045 Val Loss: 0.03070630\n",
      "Epoch:   4 Train Loss: 0.04457891 Val Loss: 0.03163474\n",
      "Epoch:   5 Train Loss: 0.03281760 Val Loss: 0.02008549\n",
      "Epoch:   6 Train Loss: 0.02702876 Val Loss: 0.01454125\n",
      "Epoch:   7 Train Loss: 0.01539995 Val Loss: 0.00875318\n",
      "Epoch:   8 Train Loss: 0.00996207 Val Loss: 0.01384190\n",
      "Epoch:   9 Train Loss: 0.00971646 Val Loss: 0.00816402\n",
      "Epoch:  10 Train Loss: 0.00801745 Val Loss: 0.00791854\n",
      "Epoch:  11 Train Loss: 0.00901337 Val Loss: 0.00876345\n",
      "Epoch:  12 Train Loss: 0.00872925 Val Loss: 0.00748217\n",
      "Epoch:  13 Train Loss: 0.00867685 Val Loss: 0.01042497\n",
      "Epoch:  14 Train Loss: 0.01025029 Val Loss: 0.00730982\n",
      "Epoch:  15 Train Loss: 0.00934949 Val Loss: 0.00735482\n",
      "Epoch:  16 Train Loss: 0.00787851 Val Loss: 0.00997490\n",
      "Epoch:  17 Train Loss: 0.00809233 Val Loss: 0.00731030\n",
      "Epoch:  18 Train Loss: 0.00882270 Val Loss: 0.01046028\n",
      "Epoch:  19 Train Loss: 0.00827267 Val Loss: 0.00747365\n",
      "Epoch:  20 Train Loss: 0.00940656 Val Loss: 0.00978843\n",
      "Epoch:  21 Train Loss: 0.00915423 Val Loss: 0.00739008\n",
      "Epoch:  22 Train Loss: 0.00795248 Val Loss: 0.00901407\n",
      "Epoch:  23 Train Loss: 0.00914768 Val Loss: 0.00767612\n",
      "Epoch:  24 Train Loss: 0.00738837 Val Loss: 0.00885729\n",
      "Epoch:  25 Train Loss: 0.00769981 Val Loss: 0.00733772\n",
      "Epoch:  26 Train Loss: 0.00861294 Val Loss: 0.00778474\n",
      "Epoch:  27 Train Loss: 0.00842241 Val Loss: 0.00700994\n",
      "Epoch:  28 Train Loss: 0.00789878 Val Loss: 0.00694009\n",
      "Epoch:  29 Train Loss: 0.00821115 Val Loss: 0.00711910\n",
      "Epoch:  30 Train Loss: 0.00733128 Val Loss: 0.00720521\n",
      "Epoch:  31 Train Loss: 0.00810716 Val Loss: 0.00744606\n",
      "Epoch:  32 Train Loss: 0.00752545 Val Loss: 0.00693790\n",
      "Epoch:  33 Train Loss: 0.00651143 Val Loss: 0.00706130\n",
      "Epoch:  34 Train Loss: 0.00855255 Val Loss: 0.00908476\n",
      "Epoch:  35 Train Loss: 0.00840741 Val Loss: 0.00739620\n",
      "Epoch:  36 Train Loss: 0.00802781 Val Loss: 0.00806745\n",
      "Epoch:  37 Train Loss: 0.00919248 Val Loss: 0.00693502\n",
      "Epoch:  38 Train Loss: 0.00816077 Val Loss: 0.00700231\n",
      "Epoch:  39 Train Loss: 0.00794727 Val Loss: 0.00699647\n",
      "Epoch:  40 Train Loss: 0.00795874 Val Loss: 0.00714891\n",
      "Epoch:  41 Train Loss: 0.00688547 Val Loss: 0.00685294\n",
      "Epoch:  42 Train Loss: 0.00726659 Val Loss: 0.00738516\n",
      "Epoch:  43 Train Loss: 0.00779340 Val Loss: 0.00771098\n",
      "Epoch:  44 Train Loss: 0.00863226 Val Loss: 0.01134588\n",
      "Epoch:  45 Train Loss: 0.00759134 Val Loss: 0.00859002\n",
      "Epoch:  46 Train Loss: 0.00867311 Val Loss: 0.00827756\n",
      "Epoch:  47 Train Loss: 0.00782795 Val Loss: 0.00715597\n",
      "Epoch:  48 Train Loss: 0.00794791 Val Loss: 0.00924634\n",
      "Epoch:  49 Train Loss: 0.00945691 Val Loss: 0.00777812\n",
      "Epoch:  50 Train Loss: 0.00892597 Val Loss: 0.00904629\n",
      "Epoch:  51 Train Loss: 0.00826514 Val Loss: 0.00673587\n",
      "Epoch:  52 Train Loss: 0.00705420 Val Loss: 0.00697653\n",
      "Epoch:  53 Train Loss: 0.00689305 Val Loss: 0.00679317\n",
      "Epoch:  54 Train Loss: 0.00831418 Val Loss: 0.00680448\n",
      "Epoch:  55 Train Loss: 0.00738272 Val Loss: 0.00699733\n",
      "Epoch:  56 Train Loss: 0.00736192 Val Loss: 0.00670193\n",
      "Epoch:  57 Train Loss: 0.00677829 Val Loss: 0.00668922\n",
      "Epoch:  58 Train Loss: 0.00759911 Val Loss: 0.00675247\n",
      "Epoch:  59 Train Loss: 0.00623611 Val Loss: 0.00671077\n",
      "Epoch:  60 Train Loss: 0.00648254 Val Loss: 0.00676075\n",
      "Epoch:  61 Train Loss: 0.00657267 Val Loss: 0.00712716\n",
      "Epoch:  62 Train Loss: 0.00714712 Val Loss: 0.00699714\n",
      "Epoch:  63 Train Loss: 0.00842184 Val Loss: 0.00710166\n",
      "Epoch:  64 Train Loss: 0.00727374 Val Loss: 0.00707801\n",
      "Epoch:  65 Train Loss: 0.00772570 Val Loss: 0.00675896\n",
      "Epoch:  66 Train Loss: 0.00722986 Val Loss: 0.00693350\n",
      "Epoch:  67 Train Loss: 0.00647305 Val Loss: 0.00678164\n",
      "Epoch:  68 Train Loss: 0.00714488 Val Loss: 0.00681479\n",
      "Epoch:  69 Train Loss: 0.00592424 Val Loss: 0.00682869\n",
      "Epoch:  70 Train Loss: 0.00848966 Val Loss: 0.00764253\n",
      "Epoch:  71 Train Loss: 0.00630055 Val Loss: 0.00661049\n",
      "Epoch:  72 Train Loss: 0.00647373 Val Loss: 0.00654453\n",
      "Epoch:  73 Train Loss: 0.00618611 Val Loss: 0.00669993\n",
      "Epoch:  74 Train Loss: 0.00673156 Val Loss: 0.00679951\n",
      "Epoch:  75 Train Loss: 0.00732290 Val Loss: 0.00759960\n",
      "Epoch:  76 Train Loss: 0.00726607 Val Loss: 0.00690200\n",
      "Epoch:  77 Train Loss: 0.00634197 Val Loss: 0.00726091\n",
      "Epoch:  78 Train Loss: 0.00643134 Val Loss: 0.00652477\n",
      "Epoch:  79 Train Loss: 0.00716502 Val Loss: 0.00697319\n",
      "Epoch:  80 Train Loss: 0.00678914 Val Loss: 0.00647265\n",
      "Epoch:  81 Train Loss: 0.00656280 Val Loss: 0.00661186\n",
      "Epoch:  82 Train Loss: 0.00785967 Val Loss: 0.00706776\n",
      "Epoch:  83 Train Loss: 0.00637697 Val Loss: 0.00673586\n",
      "Epoch:  84 Train Loss: 0.00804749 Val Loss: 0.00783124\n",
      "Epoch:  85 Train Loss: 0.00724923 Val Loss: 0.00659669\n",
      "Epoch:  86 Train Loss: 0.00732745 Val Loss: 0.00641687\n",
      "Epoch:  87 Train Loss: 0.00716003 Val Loss: 0.00688338\n",
      "Epoch:  88 Train Loss: 0.00650207 Val Loss: 0.00645348\n",
      "Epoch:  89 Train Loss: 0.00668616 Val Loss: 0.00649324\n",
      "Epoch:  90 Train Loss: 0.00731180 Val Loss: 0.00987549\n",
      "Epoch:  91 Train Loss: 0.00889755 Val Loss: 0.00762952\n",
      "Epoch:  92 Train Loss: 0.00759487 Val Loss: 0.00727623\n",
      "Epoch:  93 Train Loss: 0.00689494 Val Loss: 0.00633294\n",
      "Epoch:  94 Train Loss: 0.00749597 Val Loss: 0.00635092\n",
      "Epoch:  95 Train Loss: 0.00675362 Val Loss: 0.00660688\n",
      "Epoch:  96 Train Loss: 0.00652008 Val Loss: 0.00638199\n",
      "Epoch:  97 Train Loss: 0.00740356 Val Loss: 0.00648405\n",
      "Epoch:  98 Train Loss: 0.00672318 Val Loss: 0.00711063\n",
      "Epoch:  99 Train Loss: 0.00674027 Val Loss: 0.00633758\n",
      "Epoch: 100 Train Loss: 0.00584550 Val Loss: 0.00643425\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.06927795 Val Loss: 0.03062154\n",
      "Epoch:   2 Train Loss: 0.02500238 Val Loss: 0.02536784\n",
      "Epoch:   3 Train Loss: 0.01027250 Val Loss: 0.00808413\n",
      "Epoch:   4 Train Loss: 0.00922755 Val Loss: 0.00933772\n",
      "Epoch:   5 Train Loss: 0.01152908 Val Loss: 0.01187333\n",
      "Epoch:   6 Train Loss: 0.00968328 Val Loss: 0.00880581\n",
      "Epoch:   7 Train Loss: 0.00820003 Val Loss: 0.00890923\n",
      "Epoch:   8 Train Loss: 0.00783716 Val Loss: 0.00855164\n",
      "Epoch:   9 Train Loss: 0.00794426 Val Loss: 0.00854120\n",
      "Epoch:  10 Train Loss: 0.00968170 Val Loss: 0.00796682\n",
      "Epoch:  11 Train Loss: 0.00921491 Val Loss: 0.00768640\n",
      "Epoch:  12 Train Loss: 0.00825366 Val Loss: 0.00954833\n",
      "Epoch:  13 Train Loss: 0.01164918 Val Loss: 0.00739741\n",
      "Epoch:  14 Train Loss: 0.00796946 Val Loss: 0.01000504\n",
      "Epoch:  15 Train Loss: 0.00835165 Val Loss: 0.00800608\n",
      "Epoch:  16 Train Loss: 0.00862440 Val Loss: 0.00815733\n",
      "Epoch:  17 Train Loss: 0.00877401 Val Loss: 0.00770471\n",
      "Epoch:  18 Train Loss: 0.00772326 Val Loss: 0.00757486\n",
      "Epoch:  19 Train Loss: 0.00790853 Val Loss: 0.00848452\n",
      "Epoch:  20 Train Loss: 0.00810369 Val Loss: 0.00749877\n",
      "Epoch:  21 Train Loss: 0.00974721 Val Loss: 0.00793287\n",
      "Epoch:  22 Train Loss: 0.00819223 Val Loss: 0.00872967\n",
      "Epoch:  23 Train Loss: 0.00850806 Val Loss: 0.00794960\n",
      "Epoch:  24 Train Loss: 0.00955770 Val Loss: 0.00776138\n",
      "Epoch:  25 Train Loss: 0.00842666 Val Loss: 0.00807333\n",
      "Epoch:  26 Train Loss: 0.00766914 Val Loss: 0.01097453\n",
      "Epoch:  27 Train Loss: 0.00952585 Val Loss: 0.00958200\n",
      "Epoch:  28 Train Loss: 0.00975704 Val Loss: 0.00835744\n",
      "Epoch:  29 Train Loss: 0.01198945 Val Loss: 0.01481877\n",
      "Epoch:  30 Train Loss: 0.01038861 Val Loss: 0.01109443\n",
      "Epoch:  31 Train Loss: 0.00977070 Val Loss: 0.01148503\n",
      "Epoch:  32 Train Loss: 0.00929531 Val Loss: 0.01415490\n",
      "Epoch:  33 Train Loss: 0.01033454 Val Loss: 0.00977160\n",
      "Epoch:  34 Train Loss: 0.00974853 Val Loss: 0.00914311\n",
      "Epoch:  35 Train Loss: 0.00944840 Val Loss: 0.00997524\n",
      "Epoch:  36 Train Loss: 0.00946105 Val Loss: 0.01205365\n",
      "Epoch:  37 Train Loss: 0.00887010 Val Loss: 0.00841893\n",
      "Epoch:  38 Train Loss: 0.00790343 Val Loss: 0.00800760\n",
      "Epoch:  39 Train Loss: 0.00776768 Val Loss: 0.00880011\n",
      "Epoch:  40 Train Loss: 0.00801649 Val Loss: 0.01027361\n",
      "Epoch:  41 Train Loss: 0.00860069 Val Loss: 0.00747289\n",
      "Epoch:  42 Train Loss: 0.00763023 Val Loss: 0.01141782\n",
      "Epoch:  43 Train Loss: 0.00991179 Val Loss: 0.00965170\n",
      "Epoch:  44 Train Loss: 0.01096018 Val Loss: 0.01237994\n",
      "Epoch:  45 Train Loss: 0.00802222 Val Loss: 0.00762832\n",
      "Epoch:  46 Train Loss: 0.00795791 Val Loss: 0.00847191\n",
      "Epoch:  47 Train Loss: 0.00906796 Val Loss: 0.00810017\n",
      "Epoch:  48 Train Loss: 0.00763350 Val Loss: 0.00819176\n",
      "Epoch:  49 Train Loss: 0.00790776 Val Loss: 0.00807733\n",
      "Epoch:  50 Train Loss: 0.00759167 Val Loss: 0.01317933\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.13262720 Val Loss: 0.07415983\n",
      "Epoch:   2 Train Loss: 0.06175382 Val Loss: 0.02611297\n",
      "Epoch:   3 Train Loss: 0.03561317 Val Loss: 0.01749545\n",
      "Epoch:   4 Train Loss: 0.01638246 Val Loss: 0.01586404\n",
      "Epoch:   5 Train Loss: 0.01094789 Val Loss: 0.00942561\n",
      "Epoch:   6 Train Loss: 0.00934146 Val Loss: 0.00891418\n",
      "Epoch:   7 Train Loss: 0.00900684 Val Loss: 0.00854595\n",
      "Epoch:   8 Train Loss: 0.00760497 Val Loss: 0.01046468\n",
      "Epoch:   9 Train Loss: 0.01181408 Val Loss: 0.00855687\n",
      "Epoch:  10 Train Loss: 0.00974513 Val Loss: 0.00807411\n",
      "Epoch:  11 Train Loss: 0.00872162 Val Loss: 0.01127052\n",
      "Epoch:  12 Train Loss: 0.01129036 Val Loss: 0.00932858\n",
      "Epoch:  13 Train Loss: 0.00784404 Val Loss: 0.00802329\n",
      "Epoch:  14 Train Loss: 0.00714235 Val Loss: 0.00788604\n",
      "Epoch:  15 Train Loss: 0.01054979 Val Loss: 0.01264125\n",
      "Epoch:  16 Train Loss: 0.01223444 Val Loss: 0.00973132\n",
      "Epoch:  17 Train Loss: 0.01450570 Val Loss: 0.00764732\n",
      "Epoch:  18 Train Loss: 0.00913607 Val Loss: 0.01103197\n",
      "Epoch:  19 Train Loss: 0.00899459 Val Loss: 0.00779577\n",
      "Epoch:  20 Train Loss: 0.00655508 Val Loss: 0.00997228\n",
      "Epoch:  21 Train Loss: 0.00872294 Val Loss: 0.00767963\n",
      "Epoch:  22 Train Loss: 0.00696622 Val Loss: 0.00896192\n",
      "Epoch:  23 Train Loss: 0.00810627 Val Loss: 0.00827929\n",
      "Epoch:  24 Train Loss: 0.00719659 Val Loss: 0.00780123\n",
      "Epoch:  25 Train Loss: 0.00877974 Val Loss: 0.00795128\n",
      "Epoch:  26 Train Loss: 0.00938007 Val Loss: 0.00782025\n",
      "Epoch:  27 Train Loss: 0.00756321 Val Loss: 0.00739837\n",
      "Epoch:  28 Train Loss: 0.00941362 Val Loss: 0.00878288\n",
      "Epoch:  29 Train Loss: 0.01097862 Val Loss: 0.01161976\n",
      "Epoch:  30 Train Loss: 0.00911477 Val Loss: 0.01009617\n",
      "Epoch:  31 Train Loss: 0.00859729 Val Loss: 0.00851543\n",
      "Epoch:  32 Train Loss: 0.00922526 Val Loss: 0.00921633\n",
      "Epoch:  33 Train Loss: 0.00932415 Val Loss: 0.01024374\n",
      "Epoch:  34 Train Loss: 0.00903823 Val Loss: 0.00780498\n",
      "Epoch:  35 Train Loss: 0.00799097 Val Loss: 0.00780876\n",
      "Epoch:  36 Train Loss: 0.00849543 Val Loss: 0.00948871\n",
      "Epoch:  37 Train Loss: 0.00816307 Val Loss: 0.00750007\n",
      "Epoch:  38 Train Loss: 0.00727669 Val Loss: 0.00826613\n",
      "Epoch:  39 Train Loss: 0.00900497 Val Loss: 0.00761600\n",
      "Epoch:  40 Train Loss: 0.00888464 Val Loss: 0.00745934\n",
      "Epoch:  41 Train Loss: 0.00844418 Val Loss: 0.00869513\n",
      "Epoch:  42 Train Loss: 0.00777624 Val Loss: 0.00775911\n",
      "Epoch:  43 Train Loss: 0.00755757 Val Loss: 0.01525679\n",
      "Epoch:  44 Train Loss: 0.01145127 Val Loss: 0.00969363\n",
      "Epoch:  45 Train Loss: 0.00953041 Val Loss: 0.00872013\n",
      "Epoch:  46 Train Loss: 0.00809958 Val Loss: 0.00852465\n",
      "Epoch:  47 Train Loss: 0.00807695 Val Loss: 0.00814302\n",
      "Epoch:  48 Train Loss: 0.00779838 Val Loss: 0.00919502\n",
      "Epoch:  49 Train Loss: 0.01063081 Val Loss: 0.01069864\n",
      "Epoch:  50 Train Loss: 0.00981495 Val Loss: 0.00826073\n",
      "Epoch:  51 Train Loss: 0.00880133 Val Loss: 0.00800210\n",
      "Epoch:  52 Train Loss: 0.00799124 Val Loss: 0.00840162\n",
      "Epoch:  53 Train Loss: 0.00876224 Val Loss: 0.00771811\n",
      "Epoch:  54 Train Loss: 0.00671446 Val Loss: 0.00790108\n",
      "Epoch:  55 Train Loss: 0.00887460 Val Loss: 0.00934068\n",
      "Epoch:  56 Train Loss: 0.00865147 Val Loss: 0.00864443\n",
      "Epoch:  57 Train Loss: 0.00769123 Val Loss: 0.00732168\n",
      "Epoch:  58 Train Loss: 0.00670991 Val Loss: 0.00762898\n",
      "Epoch:  59 Train Loss: 0.00756282 Val Loss: 0.00735381\n",
      "Epoch:  60 Train Loss: 0.00746964 Val Loss: 0.00793384\n",
      "Epoch:  61 Train Loss: 0.00772791 Val Loss: 0.01124750\n",
      "Epoch:  62 Train Loss: 0.00874642 Val Loss: 0.00717156\n",
      "Epoch:  63 Train Loss: 0.00914990 Val Loss: 0.00865740\n",
      "Epoch:  64 Train Loss: 0.00797416 Val Loss: 0.00727681\n",
      "Epoch:  65 Train Loss: 0.00722672 Val Loss: 0.00880529\n",
      "Epoch:  66 Train Loss: 0.00797601 Val Loss: 0.00715662\n",
      "Epoch:  67 Train Loss: 0.00713395 Val Loss: 0.00755899\n",
      "Epoch:  68 Train Loss: 0.00754008 Val Loss: 0.00887079\n",
      "Epoch:  69 Train Loss: 0.00848507 Val Loss: 0.00717512\n",
      "Epoch:  70 Train Loss: 0.00807075 Val Loss: 0.00733865\n",
      "Epoch:  71 Train Loss: 0.00782634 Val Loss: 0.00873357\n",
      "Epoch:  72 Train Loss: 0.00943974 Val Loss: 0.00685767\n",
      "Epoch:  73 Train Loss: 0.00759949 Val Loss: 0.00692371\n",
      "Epoch:  74 Train Loss: 0.00763647 Val Loss: 0.00707395\n",
      "Epoch:  75 Train Loss: 0.00772544 Val Loss: 0.00803953\n",
      "Epoch:  76 Train Loss: 0.00862550 Val Loss: 0.00750104\n",
      "Epoch:  77 Train Loss: 0.00867051 Val Loss: 0.00919566\n",
      "Epoch:  78 Train Loss: 0.01020007 Val Loss: 0.01326772\n",
      "Epoch:  79 Train Loss: 0.01069185 Val Loss: 0.01038612\n",
      "Epoch:  80 Train Loss: 0.00969408 Val Loss: 0.00872277\n",
      "Epoch:  81 Train Loss: 0.00820549 Val Loss: 0.00938608\n",
      "Epoch:  82 Train Loss: 0.00706677 Val Loss: 0.00784268\n",
      "Epoch:  83 Train Loss: 0.00667401 Val Loss: 0.00797692\n",
      "Epoch:  84 Train Loss: 0.00859435 Val Loss: 0.00822791\n",
      "Epoch:  85 Train Loss: 0.00875990 Val Loss: 0.00743923\n",
      "Epoch:  86 Train Loss: 0.00832275 Val Loss: 0.00718112\n",
      "Epoch:  87 Train Loss: 0.00703104 Val Loss: 0.00810911\n",
      "Epoch:  88 Train Loss: 0.00683856 Val Loss: 0.00755266\n",
      "Epoch:  89 Train Loss: 0.00874896 Val Loss: 0.00764664\n",
      "Epoch:  90 Train Loss: 0.00753498 Val Loss: 0.00928050\n",
      "Epoch:  91 Train Loss: 0.00858482 Val Loss: 0.00937530\n",
      "Epoch:  92 Train Loss: 0.01121638 Val Loss: 0.01511260\n",
      "Epoch:  93 Train Loss: 0.01000185 Val Loss: 0.00776827\n",
      "Epoch:  94 Train Loss: 0.00763750 Val Loss: 0.01135567\n",
      "Epoch:  95 Train Loss: 0.00831770 Val Loss: 0.00742730\n",
      "Epoch:  96 Train Loss: 0.00793275 Val Loss: 0.00825375\n",
      "Epoch:  97 Train Loss: 0.00723965 Val Loss: 0.00743024\n",
      "Epoch:  98 Train Loss: 0.00713479 Val Loss: 0.00723501\n",
      "Epoch:  99 Train Loss: 0.00783574 Val Loss: 0.00746054\n",
      "Epoch: 100 Train Loss: 0.00738516 Val Loss: 0.00715572\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.15041465 Val Loss: 0.07859323\n",
      "Epoch:   2 Train Loss: 0.06318253 Val Loss: 0.04069743\n",
      "Epoch:   3 Train Loss: 0.05504505 Val Loss: 0.03055913\n",
      "Epoch:   4 Train Loss: 0.04634285 Val Loss: 0.03560296\n",
      "Epoch:   5 Train Loss: 0.04126346 Val Loss: 0.02344629\n",
      "Epoch:   6 Train Loss: 0.03317660 Val Loss: 0.01816205\n",
      "Epoch:   7 Train Loss: 0.02300965 Val Loss: 0.01351101\n",
      "Epoch:   8 Train Loss: 0.01647237 Val Loss: 0.01123178\n",
      "Epoch:   9 Train Loss: 0.01098353 Val Loss: 0.00772109\n",
      "Epoch:  10 Train Loss: 0.01225703 Val Loss: 0.01414273\n",
      "Epoch:  11 Train Loss: 0.00853681 Val Loss: 0.00848692\n",
      "Epoch:  12 Train Loss: 0.00920092 Val Loss: 0.01147222\n",
      "Epoch:  13 Train Loss: 0.01102706 Val Loss: 0.00752472\n",
      "Epoch:  14 Train Loss: 0.00921784 Val Loss: 0.00879478\n",
      "Epoch:  15 Train Loss: 0.01042929 Val Loss: 0.00809804\n",
      "Epoch:  16 Train Loss: 0.00756561 Val Loss: 0.00849609\n",
      "Epoch:  17 Train Loss: 0.01061502 Val Loss: 0.00734157\n",
      "Epoch:  18 Train Loss: 0.00990903 Val Loss: 0.00836945\n",
      "Epoch:  19 Train Loss: 0.00987140 Val Loss: 0.00732163\n",
      "Epoch:  20 Train Loss: 0.00872541 Val Loss: 0.00808039\n",
      "Epoch:  21 Train Loss: 0.00798296 Val Loss: 0.00712308\n",
      "Epoch:  22 Train Loss: 0.00902567 Val Loss: 0.00787603\n",
      "Epoch:  23 Train Loss: 0.00788547 Val Loss: 0.00710042\n",
      "Epoch:  24 Train Loss: 0.00757978 Val Loss: 0.00733279\n",
      "Epoch:  25 Train Loss: 0.00786818 Val Loss: 0.00754456\n",
      "Epoch:  26 Train Loss: 0.00705905 Val Loss: 0.00701330\n",
      "Epoch:  27 Train Loss: 0.01149152 Val Loss: 0.00734647\n",
      "Epoch:  28 Train Loss: 0.00731480 Val Loss: 0.00702265\n",
      "Epoch:  29 Train Loss: 0.00741363 Val Loss: 0.00704875\n",
      "Epoch:  30 Train Loss: 0.00655071 Val Loss: 0.00688804\n",
      "Epoch:  31 Train Loss: 0.00853410 Val Loss: 0.00759121\n",
      "Epoch:  32 Train Loss: 0.00830583 Val Loss: 0.00688202\n",
      "Epoch:  33 Train Loss: 0.00848733 Val Loss: 0.00683561\n",
      "Epoch:  34 Train Loss: 0.00864289 Val Loss: 0.00725682\n",
      "Epoch:  35 Train Loss: 0.00806158 Val Loss: 0.00712228\n",
      "Epoch:  36 Train Loss: 0.00674333 Val Loss: 0.00690668\n",
      "Epoch:  37 Train Loss: 0.00809872 Val Loss: 0.00688346\n",
      "Epoch:  38 Train Loss: 0.00785050 Val Loss: 0.00706755\n",
      "Epoch:  39 Train Loss: 0.00789403 Val Loss: 0.00678721\n",
      "Epoch:  40 Train Loss: 0.00835057 Val Loss: 0.00686367\n",
      "Epoch:  41 Train Loss: 0.00726389 Val Loss: 0.00690205\n",
      "Epoch:  42 Train Loss: 0.00993378 Val Loss: 0.00671404\n",
      "Epoch:  43 Train Loss: 0.00820709 Val Loss: 0.00750916\n",
      "Epoch:  44 Train Loss: 0.00797673 Val Loss: 0.00727714\n",
      "Epoch:  45 Train Loss: 0.00949844 Val Loss: 0.00776365\n",
      "Epoch:  46 Train Loss: 0.00875043 Val Loss: 0.00698451\n",
      "Epoch:  47 Train Loss: 0.00749571 Val Loss: 0.00743888\n",
      "Epoch:  48 Train Loss: 0.01026306 Val Loss: 0.00671448\n",
      "Epoch:  49 Train Loss: 0.00746600 Val Loss: 0.00684262\n",
      "Epoch:  50 Train Loss: 0.00921078 Val Loss: 0.00817700\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.17102279 Val Loss: 0.08147915\n",
      "Epoch:   2 Train Loss: 0.06803823 Val Loss: 0.03637028\n",
      "Epoch:   3 Train Loss: 0.04804290 Val Loss: 0.03038229\n",
      "Epoch:   4 Train Loss: 0.04941046 Val Loss: 0.03555300\n",
      "Epoch:   5 Train Loss: 0.03841375 Val Loss: 0.02426161\n",
      "Epoch:   6 Train Loss: 0.03801606 Val Loss: 0.02135390\n",
      "Epoch:   7 Train Loss: 0.03185964 Val Loss: 0.01860136\n",
      "Epoch:   8 Train Loss: 0.02039887 Val Loss: 0.01233689\n",
      "Epoch:   9 Train Loss: 0.01296968 Val Loss: 0.01358515\n",
      "Epoch:  10 Train Loss: 0.01280631 Val Loss: 0.01022646\n",
      "Epoch:  11 Train Loss: 0.01170367 Val Loss: 0.00802067\n",
      "Epoch:  12 Train Loss: 0.00876738 Val Loss: 0.01017380\n",
      "Epoch:  13 Train Loss: 0.00914566 Val Loss: 0.00797337\n",
      "Epoch:  14 Train Loss: 0.00903603 Val Loss: 0.00976785\n",
      "Epoch:  15 Train Loss: 0.00862926 Val Loss: 0.00772307\n",
      "Epoch:  16 Train Loss: 0.01024654 Val Loss: 0.00902384\n",
      "Epoch:  17 Train Loss: 0.00837986 Val Loss: 0.00888592\n",
      "Epoch:  18 Train Loss: 0.00995812 Val Loss: 0.00802692\n",
      "Epoch:  19 Train Loss: 0.00911779 Val Loss: 0.01102484\n",
      "Epoch:  20 Train Loss: 0.01227441 Val Loss: 0.00755634\n",
      "Epoch:  21 Train Loss: 0.00932316 Val Loss: 0.00876319\n",
      "Epoch:  22 Train Loss: 0.00756173 Val Loss: 0.00755307\n",
      "Epoch:  23 Train Loss: 0.00951770 Val Loss: 0.00759177\n",
      "Epoch:  24 Train Loss: 0.01161003 Val Loss: 0.00953666\n",
      "Epoch:  25 Train Loss: 0.00884089 Val Loss: 0.00753753\n",
      "Epoch:  26 Train Loss: 0.00836043 Val Loss: 0.00818520\n",
      "Epoch:  27 Train Loss: 0.00940222 Val Loss: 0.00746701\n",
      "Epoch:  28 Train Loss: 0.00907003 Val Loss: 0.00764963\n",
      "Epoch:  29 Train Loss: 0.00852628 Val Loss: 0.00735752\n",
      "Epoch:  30 Train Loss: 0.00772507 Val Loss: 0.00757665\n",
      "Epoch:  31 Train Loss: 0.00932720 Val Loss: 0.00757585\n",
      "Epoch:  32 Train Loss: 0.00902559 Val Loss: 0.00747068\n",
      "Epoch:  33 Train Loss: 0.00964074 Val Loss: 0.00761300\n",
      "Epoch:  34 Train Loss: 0.00776431 Val Loss: 0.00753757\n",
      "Epoch:  35 Train Loss: 0.01026556 Val Loss: 0.00737882\n",
      "Epoch:  36 Train Loss: 0.00856968 Val Loss: 0.00746146\n",
      "Epoch:  37 Train Loss: 0.01125000 Val Loss: 0.00816500\n",
      "Epoch:  38 Train Loss: 0.00939118 Val Loss: 0.00750001\n",
      "Epoch:  39 Train Loss: 0.00934594 Val Loss: 0.00742904\n",
      "Epoch:  40 Train Loss: 0.00761912 Val Loss: 0.00722007\n",
      "Epoch:  41 Train Loss: 0.00904429 Val Loss: 0.00723978\n",
      "Epoch:  42 Train Loss: 0.00781186 Val Loss: 0.00771089\n",
      "Epoch:  43 Train Loss: 0.00839881 Val Loss: 0.00736496\n",
      "Epoch:  44 Train Loss: 0.00744796 Val Loss: 0.00732676\n",
      "Epoch:  45 Train Loss: 0.00731209 Val Loss: 0.00750077\n",
      "Epoch:  46 Train Loss: 0.00740727 Val Loss: 0.00778882\n",
      "Epoch:  47 Train Loss: 0.00724613 Val Loss: 0.00715909\n",
      "Epoch:  48 Train Loss: 0.01006870 Val Loss: 0.00716098\n",
      "Epoch:  49 Train Loss: 0.00809384 Val Loss: 0.00706713\n",
      "Epoch:  50 Train Loss: 0.00912466 Val Loss: 0.00706168\n",
      "Epoch:  51 Train Loss: 0.00885232 Val Loss: 0.00800878\n",
      "Epoch:  52 Train Loss: 0.00907578 Val Loss: 0.00821790\n",
      "Epoch:  53 Train Loss: 0.00850236 Val Loss: 0.00739603\n",
      "Epoch:  54 Train Loss: 0.00674938 Val Loss: 0.00704024\n",
      "Epoch:  55 Train Loss: 0.00787887 Val Loss: 0.00778315\n",
      "Epoch:  56 Train Loss: 0.00737042 Val Loss: 0.00759177\n",
      "Epoch:  57 Train Loss: 0.01065452 Val Loss: 0.00846211\n",
      "Epoch:  58 Train Loss: 0.00858854 Val Loss: 0.00697284\n",
      "Epoch:  59 Train Loss: 0.00831986 Val Loss: 0.00735502\n",
      "Epoch:  60 Train Loss: 0.00926423 Val Loss: 0.00703720\n",
      "Epoch:  61 Train Loss: 0.00787781 Val Loss: 0.00745761\n",
      "Epoch:  62 Train Loss: 0.00926154 Val Loss: 0.01174022\n",
      "Epoch:  63 Train Loss: 0.00913918 Val Loss: 0.00841600\n",
      "Epoch:  64 Train Loss: 0.00941601 Val Loss: 0.00783767\n",
      "Epoch:  65 Train Loss: 0.00718329 Val Loss: 0.00704519\n",
      "Epoch:  66 Train Loss: 0.00823645 Val Loss: 0.00693318\n",
      "Epoch:  67 Train Loss: 0.00724778 Val Loss: 0.00692273\n",
      "Epoch:  68 Train Loss: 0.00795120 Val Loss: 0.00689501\n",
      "Epoch:  69 Train Loss: 0.00730031 Val Loss: 0.00683803\n",
      "Epoch:  70 Train Loss: 0.00754337 Val Loss: 0.00809274\n",
      "Epoch:  71 Train Loss: 0.00758118 Val Loss: 0.00686921\n",
      "Epoch:  72 Train Loss: 0.00906527 Val Loss: 0.00698211\n",
      "Epoch:  73 Train Loss: 0.00920947 Val Loss: 0.00873981\n",
      "Epoch:  74 Train Loss: 0.00779311 Val Loss: 0.00716252\n",
      "Epoch:  75 Train Loss: 0.00814113 Val Loss: 0.00759389\n",
      "Epoch:  76 Train Loss: 0.00773120 Val Loss: 0.00705647\n",
      "Epoch:  77 Train Loss: 0.00733126 Val Loss: 0.00725023\n",
      "Epoch:  78 Train Loss: 0.00854440 Val Loss: 0.00682785\n",
      "Epoch:  79 Train Loss: 0.00649385 Val Loss: 0.00706322\n",
      "Epoch:  80 Train Loss: 0.00698289 Val Loss: 0.00675271\n",
      "Epoch:  81 Train Loss: 0.00700448 Val Loss: 0.00682640\n",
      "Epoch:  82 Train Loss: 0.00785264 Val Loss: 0.00675130\n",
      "Epoch:  83 Train Loss: 0.00731063 Val Loss: 0.00676619\n",
      "Epoch:  84 Train Loss: 0.00854370 Val Loss: 0.00829704\n",
      "Epoch:  85 Train Loss: 0.00864824 Val Loss: 0.00698552\n",
      "Epoch:  86 Train Loss: 0.00746980 Val Loss: 0.00673287\n",
      "Epoch:  87 Train Loss: 0.00828479 Val Loss: 0.00680843\n",
      "Epoch:  88 Train Loss: 0.00974659 Val Loss: 0.00793434\n",
      "Epoch:  89 Train Loss: 0.00961190 Val Loss: 0.00714280\n",
      "Epoch:  90 Train Loss: 0.00771830 Val Loss: 0.00768383\n",
      "Epoch:  91 Train Loss: 0.00691194 Val Loss: 0.00682189\n",
      "Epoch:  92 Train Loss: 0.00604984 Val Loss: 0.00821069\n",
      "Epoch:  93 Train Loss: 0.00685126 Val Loss: 0.00679349\n",
      "Epoch:  94 Train Loss: 0.00839269 Val Loss: 0.00796073\n",
      "Epoch:  95 Train Loss: 0.00801333 Val Loss: 0.00678742\n",
      "Epoch:  96 Train Loss: 0.00733912 Val Loss: 0.00676834\n",
      "Epoch:  97 Train Loss: 0.00772943 Val Loss: 0.00685960\n",
      "Epoch:  98 Train Loss: 0.00662826 Val Loss: 0.00676599\n",
      "Epoch:  99 Train Loss: 0.00751678 Val Loss: 0.00661083\n",
      "Epoch: 100 Train Loss: 0.00792717 Val Loss: 0.00777347\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.09793223 Val Loss: 0.02900494\n",
      "Epoch:   2 Train Loss: 0.02939479 Val Loss: 0.02845053\n",
      "Epoch:   3 Train Loss: 0.01790084 Val Loss: 0.01395253\n",
      "Epoch:   4 Train Loss: 0.01522256 Val Loss: 0.01772891\n",
      "Epoch:   5 Train Loss: 0.01216475 Val Loss: 0.01272155\n",
      "Epoch:   6 Train Loss: 0.01028981 Val Loss: 0.00914208\n",
      "Epoch:   7 Train Loss: 0.00992846 Val Loss: 0.01198683\n",
      "Epoch:   8 Train Loss: 0.01078330 Val Loss: 0.01144934\n",
      "Epoch:   9 Train Loss: 0.01082311 Val Loss: 0.01426113\n",
      "Epoch:  10 Train Loss: 0.01200111 Val Loss: 0.01065411\n",
      "Epoch:  11 Train Loss: 0.01293763 Val Loss: 0.01313124\n",
      "Epoch:  12 Train Loss: 0.01244124 Val Loss: 0.00890650\n",
      "Epoch:  13 Train Loss: 0.00964989 Val Loss: 0.01361978\n",
      "Epoch:  14 Train Loss: 0.01157291 Val Loss: 0.00971290\n",
      "Epoch:  15 Train Loss: 0.01094957 Val Loss: 0.00965877\n",
      "Epoch:  16 Train Loss: 0.00961447 Val Loss: 0.00963665\n",
      "Epoch:  17 Train Loss: 0.00887273 Val Loss: 0.01153102\n",
      "Epoch:  18 Train Loss: 0.00902188 Val Loss: 0.00828579\n",
      "Epoch:  19 Train Loss: 0.00873952 Val Loss: 0.00816275\n",
      "Epoch:  20 Train Loss: 0.01015410 Val Loss: 0.01228713\n",
      "Epoch:  21 Train Loss: 0.01256216 Val Loss: 0.00872300\n",
      "Epoch:  22 Train Loss: 0.00946561 Val Loss: 0.01399489\n",
      "Epoch:  23 Train Loss: 0.01362012 Val Loss: 0.01087193\n",
      "Epoch:  24 Train Loss: 0.01009136 Val Loss: 0.01076821\n",
      "Epoch:  25 Train Loss: 0.01233000 Val Loss: 0.01045227\n",
      "Epoch:  26 Train Loss: 0.00954641 Val Loss: 0.00989167\n",
      "Epoch:  27 Train Loss: 0.00852377 Val Loss: 0.01033471\n",
      "Epoch:  28 Train Loss: 0.01062576 Val Loss: 0.00885529\n",
      "Epoch:  29 Train Loss: 0.00802318 Val Loss: 0.00863493\n",
      "Epoch:  30 Train Loss: 0.00877773 Val Loss: 0.00832984\n",
      "Epoch:  31 Train Loss: 0.01074913 Val Loss: 0.00835830\n",
      "Epoch:  32 Train Loss: 0.01085148 Val Loss: 0.00790675\n",
      "Epoch:  33 Train Loss: 0.00815142 Val Loss: 0.01133109\n",
      "Epoch:  34 Train Loss: 0.00986276 Val Loss: 0.00774440\n",
      "Epoch:  35 Train Loss: 0.01038961 Val Loss: 0.00787678\n",
      "Epoch:  36 Train Loss: 0.00792800 Val Loss: 0.01044739\n",
      "Epoch:  37 Train Loss: 0.00998854 Val Loss: 0.00986690\n",
      "Epoch:  38 Train Loss: 0.01112302 Val Loss: 0.00892892\n",
      "Epoch:  39 Train Loss: 0.00872112 Val Loss: 0.00797989\n",
      "Epoch:  40 Train Loss: 0.00933286 Val Loss: 0.00830870\n",
      "Epoch:  41 Train Loss: 0.00945088 Val Loss: 0.01259285\n",
      "Epoch:  42 Train Loss: 0.00922620 Val Loss: 0.00857064\n",
      "Epoch:  43 Train Loss: 0.00885883 Val Loss: 0.00942871\n",
      "Epoch:  44 Train Loss: 0.00992170 Val Loss: 0.00812330\n",
      "Epoch:  45 Train Loss: 0.01141464 Val Loss: 0.00835905\n",
      "Epoch:  46 Train Loss: 0.00988854 Val Loss: 0.00811675\n",
      "Epoch:  47 Train Loss: 0.00792146 Val Loss: 0.00768814\n",
      "Epoch:  48 Train Loss: 0.00777969 Val Loss: 0.00779478\n",
      "Epoch:  49 Train Loss: 0.00855117 Val Loss: 0.00790341\n",
      "Epoch:  50 Train Loss: 0.00909353 Val Loss: 0.00898308\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.16771483 Val Loss: 0.12350856\n",
      "Epoch:   2 Train Loss: 0.08432465 Val Loss: 0.04381013\n",
      "Epoch:   3 Train Loss: 0.05829674 Val Loss: 0.03817186\n",
      "Epoch:   4 Train Loss: 0.03911013 Val Loss: 0.01701121\n",
      "Epoch:   5 Train Loss: 0.01865835 Val Loss: 0.01592803\n",
      "Epoch:   6 Train Loss: 0.01219934 Val Loss: 0.01020766\n",
      "Epoch:   7 Train Loss: 0.01062388 Val Loss: 0.00877699\n",
      "Epoch:   8 Train Loss: 0.01042326 Val Loss: 0.00865128\n",
      "Epoch:   9 Train Loss: 0.01157046 Val Loss: 0.00888395\n",
      "Epoch:  10 Train Loss: 0.01009964 Val Loss: 0.00895860\n",
      "Epoch:  11 Train Loss: 0.01002100 Val Loss: 0.01200891\n",
      "Epoch:  12 Train Loss: 0.00984498 Val Loss: 0.01167709\n",
      "Epoch:  13 Train Loss: 0.00995415 Val Loss: 0.00849672\n",
      "Epoch:  14 Train Loss: 0.00984830 Val Loss: 0.00958906\n",
      "Epoch:  15 Train Loss: 0.01014284 Val Loss: 0.00808819\n",
      "Epoch:  16 Train Loss: 0.00927289 Val Loss: 0.00852247\n",
      "Epoch:  17 Train Loss: 0.00940807 Val Loss: 0.00824373\n",
      "Epoch:  18 Train Loss: 0.00805323 Val Loss: 0.01188326\n",
      "Epoch:  19 Train Loss: 0.01008893 Val Loss: 0.00987955\n",
      "Epoch:  20 Train Loss: 0.01012882 Val Loss: 0.01250318\n",
      "Epoch:  21 Train Loss: 0.00881260 Val Loss: 0.00875363\n",
      "Epoch:  22 Train Loss: 0.01247850 Val Loss: 0.01200976\n",
      "Epoch:  23 Train Loss: 0.01135526 Val Loss: 0.01158192\n",
      "Epoch:  24 Train Loss: 0.00910347 Val Loss: 0.00785736\n",
      "Epoch:  25 Train Loss: 0.00978709 Val Loss: 0.00909679\n",
      "Epoch:  26 Train Loss: 0.00820602 Val Loss: 0.00788870\n",
      "Epoch:  27 Train Loss: 0.00939136 Val Loss: 0.00782422\n",
      "Epoch:  28 Train Loss: 0.01049696 Val Loss: 0.00810214\n",
      "Epoch:  29 Train Loss: 0.01057923 Val Loss: 0.00958005\n",
      "Epoch:  30 Train Loss: 0.00937548 Val Loss: 0.00902016\n",
      "Epoch:  31 Train Loss: 0.00927544 Val Loss: 0.01035867\n",
      "Epoch:  32 Train Loss: 0.01010881 Val Loss: 0.00840967\n",
      "Epoch:  33 Train Loss: 0.00765718 Val Loss: 0.00766282\n",
      "Epoch:  34 Train Loss: 0.01130615 Val Loss: 0.00752078\n",
      "Epoch:  35 Train Loss: 0.00950729 Val Loss: 0.00832295\n",
      "Epoch:  36 Train Loss: 0.00930817 Val Loss: 0.00915223\n",
      "Epoch:  37 Train Loss: 0.00932636 Val Loss: 0.00765625\n",
      "Epoch:  38 Train Loss: 0.00694547 Val Loss: 0.00753286\n",
      "Epoch:  39 Train Loss: 0.00835923 Val Loss: 0.00761679\n",
      "Epoch:  40 Train Loss: 0.00852098 Val Loss: 0.00767773\n",
      "Epoch:  41 Train Loss: 0.00827466 Val Loss: 0.00795179\n",
      "Epoch:  42 Train Loss: 0.00691908 Val Loss: 0.00722543\n",
      "Epoch:  43 Train Loss: 0.00962764 Val Loss: 0.00792384\n",
      "Epoch:  44 Train Loss: 0.00910521 Val Loss: 0.00713788\n",
      "Epoch:  45 Train Loss: 0.00786553 Val Loss: 0.00830573\n",
      "Epoch:  46 Train Loss: 0.01099178 Val Loss: 0.00832925\n",
      "Epoch:  47 Train Loss: 0.01039174 Val Loss: 0.00855830\n",
      "Epoch:  48 Train Loss: 0.00987825 Val Loss: 0.01036537\n",
      "Epoch:  49 Train Loss: 0.00858393 Val Loss: 0.00945340\n",
      "Epoch:  50 Train Loss: 0.00790358 Val Loss: 0.00777721\n",
      "Epoch:  51 Train Loss: 0.00770223 Val Loss: 0.00781085\n",
      "Epoch:  52 Train Loss: 0.00874433 Val Loss: 0.01312533\n",
      "Epoch:  53 Train Loss: 0.00900245 Val Loss: 0.00850322\n",
      "Epoch:  54 Train Loss: 0.00709169 Val Loss: 0.00940771\n",
      "Epoch:  55 Train Loss: 0.01090443 Val Loss: 0.01007612\n",
      "Epoch:  56 Train Loss: 0.01131172 Val Loss: 0.01170427\n",
      "Epoch:  57 Train Loss: 0.01256322 Val Loss: 0.00832166\n",
      "Epoch:  58 Train Loss: 0.00790222 Val Loss: 0.00813796\n",
      "Epoch:  59 Train Loss: 0.00843404 Val Loss: 0.00776472\n",
      "Epoch:  60 Train Loss: 0.00742905 Val Loss: 0.00774573\n",
      "Epoch:  61 Train Loss: 0.00697653 Val Loss: 0.00916428\n",
      "Epoch:  62 Train Loss: 0.00732197 Val Loss: 0.00746876\n",
      "Epoch:  63 Train Loss: 0.00688243 Val Loss: 0.00744433\n",
      "Epoch:  64 Train Loss: 0.00673232 Val Loss: 0.00823330\n",
      "Epoch:  65 Train Loss: 0.00752062 Val Loss: 0.00765183\n",
      "Epoch:  66 Train Loss: 0.00905297 Val Loss: 0.00756188\n",
      "Epoch:  67 Train Loss: 0.00728606 Val Loss: 0.00775730\n",
      "Epoch:  68 Train Loss: 0.00985939 Val Loss: 0.00803802\n",
      "Epoch:  69 Train Loss: 0.00801619 Val Loss: 0.00780707\n",
      "Epoch:  70 Train Loss: 0.00982127 Val Loss: 0.01197642\n",
      "Epoch:  71 Train Loss: 0.00856136 Val Loss: 0.00775271\n",
      "Epoch:  72 Train Loss: 0.00861257 Val Loss: 0.00788767\n",
      "Epoch:  73 Train Loss: 0.00735415 Val Loss: 0.00790021\n",
      "Epoch:  74 Train Loss: 0.00756393 Val Loss: 0.00739073\n",
      "Epoch:  75 Train Loss: 0.00762553 Val Loss: 0.00750210\n",
      "Epoch:  76 Train Loss: 0.00738417 Val Loss: 0.00723196\n",
      "Epoch:  77 Train Loss: 0.00731259 Val Loss: 0.00793035\n",
      "Epoch:  78 Train Loss: 0.01035386 Val Loss: 0.01351713\n",
      "Epoch:  79 Train Loss: 0.01276618 Val Loss: 0.01041125\n",
      "Epoch:  80 Train Loss: 0.00965222 Val Loss: 0.01015687\n",
      "Epoch:  81 Train Loss: 0.00898470 Val Loss: 0.00781051\n",
      "Epoch:  82 Train Loss: 0.00956633 Val Loss: 0.00910034\n",
      "Epoch:  83 Train Loss: 0.00909538 Val Loss: 0.00815841\n",
      "Epoch:  84 Train Loss: 0.01017968 Val Loss: 0.00808479\n",
      "Epoch:  85 Train Loss: 0.00868156 Val Loss: 0.00908844\n",
      "Epoch:  86 Train Loss: 0.00942633 Val Loss: 0.00819214\n",
      "Epoch:  87 Train Loss: 0.00863820 Val Loss: 0.00867023\n",
      "Epoch:  88 Train Loss: 0.00758996 Val Loss: 0.00808412\n",
      "Epoch:  89 Train Loss: 0.00721321 Val Loss: 0.00719121\n",
      "Epoch:  90 Train Loss: 0.00703606 Val Loss: 0.00770366\n",
      "Epoch:  91 Train Loss: 0.00749972 Val Loss: 0.00988454\n",
      "Epoch:  92 Train Loss: 0.00934504 Val Loss: 0.00730759\n",
      "Epoch:  93 Train Loss: 0.00751166 Val Loss: 0.00791109\n",
      "Epoch:  94 Train Loss: 0.00802661 Val Loss: 0.00886165\n",
      "Epoch:  95 Train Loss: 0.00805225 Val Loss: 0.00764716\n",
      "Epoch:  96 Train Loss: 0.00763176 Val Loss: 0.00790926\n",
      "Epoch:  97 Train Loss: 0.00871963 Val Loss: 0.00753467\n",
      "Epoch:  98 Train Loss: 0.00796129 Val Loss: 0.00710747\n",
      "Epoch:  99 Train Loss: 0.00693572 Val Loss: 0.00853406\n",
      "Epoch: 100 Train Loss: 0.00698151 Val Loss: 0.00798942\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.12104826 Val Loss: 0.05659957\n",
      "Epoch:   2 Train Loss: 0.05923971 Val Loss: 0.03729489\n",
      "Epoch:   3 Train Loss: 0.05637485 Val Loss: 0.03114808\n",
      "Epoch:   4 Train Loss: 0.04638243 Val Loss: 0.03110025\n",
      "Epoch:   5 Train Loss: 0.04123583 Val Loss: 0.02262328\n",
      "Epoch:   6 Train Loss: 0.03009781 Val Loss: 0.01587777\n",
      "Epoch:   7 Train Loss: 0.01791355 Val Loss: 0.01170164\n",
      "Epoch:   8 Train Loss: 0.00991700 Val Loss: 0.00854061\n",
      "Epoch:   9 Train Loss: 0.01355197 Val Loss: 0.01341268\n",
      "Epoch:  10 Train Loss: 0.01019065 Val Loss: 0.00803140\n",
      "Epoch:  11 Train Loss: 0.00959898 Val Loss: 0.00921627\n",
      "Epoch:  12 Train Loss: 0.01078181 Val Loss: 0.00838684\n",
      "Epoch:  13 Train Loss: 0.01040420 Val Loss: 0.00763290\n",
      "Epoch:  14 Train Loss: 0.01055086 Val Loss: 0.00784578\n",
      "Epoch:  15 Train Loss: 0.00953310 Val Loss: 0.00730278\n",
      "Epoch:  16 Train Loss: 0.01066304 Val Loss: 0.01182886\n",
      "Epoch:  17 Train Loss: 0.01031451 Val Loss: 0.00763264\n",
      "Epoch:  18 Train Loss: 0.00932899 Val Loss: 0.00900673\n",
      "Epoch:  19 Train Loss: 0.01033435 Val Loss: 0.00715751\n",
      "Epoch:  20 Train Loss: 0.00789690 Val Loss: 0.00798863\n",
      "Epoch:  21 Train Loss: 0.00876849 Val Loss: 0.00751941\n",
      "Epoch:  22 Train Loss: 0.00834442 Val Loss: 0.00737256\n",
      "Epoch:  23 Train Loss: 0.00837858 Val Loss: 0.00778678\n",
      "Epoch:  24 Train Loss: 0.00996144 Val Loss: 0.00709920\n",
      "Epoch:  25 Train Loss: 0.01029877 Val Loss: 0.00860002\n",
      "Epoch:  26 Train Loss: 0.00978385 Val Loss: 0.00719105\n",
      "Epoch:  27 Train Loss: 0.01008621 Val Loss: 0.00840880\n",
      "Epoch:  28 Train Loss: 0.01072461 Val Loss: 0.00761880\n",
      "Epoch:  29 Train Loss: 0.00777896 Val Loss: 0.00707184\n",
      "Epoch:  30 Train Loss: 0.00986216 Val Loss: 0.00696275\n",
      "Epoch:  31 Train Loss: 0.01047745 Val Loss: 0.00710777\n",
      "Epoch:  32 Train Loss: 0.00950999 Val Loss: 0.00826551\n",
      "Epoch:  33 Train Loss: 0.00745914 Val Loss: 0.00723645\n",
      "Epoch:  34 Train Loss: 0.00921030 Val Loss: 0.00787802\n",
      "Epoch:  35 Train Loss: 0.01116086 Val Loss: 0.00706640\n",
      "Epoch:  36 Train Loss: 0.01047117 Val Loss: 0.00752296\n",
      "Epoch:  37 Train Loss: 0.01228276 Val Loss: 0.00687581\n",
      "Epoch:  38 Train Loss: 0.00930435 Val Loss: 0.00732770\n",
      "Epoch:  39 Train Loss: 0.01103290 Val Loss: 0.00711412\n",
      "Epoch:  40 Train Loss: 0.00941697 Val Loss: 0.00795365\n",
      "Epoch:  41 Train Loss: 0.00896110 Val Loss: 0.00686892\n",
      "Epoch:  42 Train Loss: 0.00970864 Val Loss: 0.00730665\n",
      "Epoch:  43 Train Loss: 0.00855009 Val Loss: 0.00682712\n",
      "Epoch:  44 Train Loss: 0.01041696 Val Loss: 0.00740388\n",
      "Epoch:  45 Train Loss: 0.00814471 Val Loss: 0.00695929\n",
      "Epoch:  46 Train Loss: 0.00833948 Val Loss: 0.00757012\n",
      "Epoch:  47 Train Loss: 0.00861986 Val Loss: 0.00680362\n",
      "Epoch:  48 Train Loss: 0.00947158 Val Loss: 0.00691056\n",
      "Epoch:  49 Train Loss: 0.00822086 Val Loss: 0.00678110\n",
      "Epoch:  50 Train Loss: 0.00855499 Val Loss: 0.00696791\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.09433526 Val Loss: 0.04117508\n",
      "Epoch:   2 Train Loss: 0.05441320 Val Loss: 0.03323673\n",
      "Epoch:   3 Train Loss: 0.05004079 Val Loss: 0.03054218\n",
      "Epoch:   4 Train Loss: 0.04353312 Val Loss: 0.03112668\n",
      "Epoch:   5 Train Loss: 0.04275423 Val Loss: 0.02289727\n",
      "Epoch:   6 Train Loss: 0.03219932 Val Loss: 0.01745717\n",
      "Epoch:   7 Train Loss: 0.02091588 Val Loss: 0.01107519\n",
      "Epoch:   8 Train Loss: 0.01300733 Val Loss: 0.01251200\n",
      "Epoch:   9 Train Loss: 0.01317375 Val Loss: 0.01071141\n",
      "Epoch:  10 Train Loss: 0.00996689 Val Loss: 0.00874373\n",
      "Epoch:  11 Train Loss: 0.00860505 Val Loss: 0.00847185\n",
      "Epoch:  12 Train Loss: 0.01094679 Val Loss: 0.00939990\n",
      "Epoch:  13 Train Loss: 0.01117871 Val Loss: 0.00873476\n",
      "Epoch:  14 Train Loss: 0.01018791 Val Loss: 0.00824452\n",
      "Epoch:  15 Train Loss: 0.00900213 Val Loss: 0.00850498\n",
      "Epoch:  16 Train Loss: 0.00849825 Val Loss: 0.00733869\n",
      "Epoch:  17 Train Loss: 0.00960451 Val Loss: 0.00761543\n",
      "Epoch:  18 Train Loss: 0.00810704 Val Loss: 0.00772500\n",
      "Epoch:  19 Train Loss: 0.00874682 Val Loss: 0.00729264\n",
      "Epoch:  20 Train Loss: 0.00918431 Val Loss: 0.00776384\n",
      "Epoch:  21 Train Loss: 0.00827287 Val Loss: 0.00757319\n",
      "Epoch:  22 Train Loss: 0.01115514 Val Loss: 0.00768574\n",
      "Epoch:  23 Train Loss: 0.00944929 Val Loss: 0.00773323\n",
      "Epoch:  24 Train Loss: 0.01140645 Val Loss: 0.00734630\n",
      "Epoch:  25 Train Loss: 0.01237087 Val Loss: 0.00747943\n",
      "Epoch:  26 Train Loss: 0.00755488 Val Loss: 0.00745649\n",
      "Epoch:  27 Train Loss: 0.00832870 Val Loss: 0.00751454\n",
      "Epoch:  28 Train Loss: 0.00825715 Val Loss: 0.00725275\n",
      "Epoch:  29 Train Loss: 0.00693077 Val Loss: 0.00723258\n",
      "Epoch:  30 Train Loss: 0.00827858 Val Loss: 0.00870228\n",
      "Epoch:  31 Train Loss: 0.00965345 Val Loss: 0.00725819\n",
      "Epoch:  32 Train Loss: 0.00851110 Val Loss: 0.00771090\n",
      "Epoch:  33 Train Loss: 0.00813155 Val Loss: 0.00712550\n",
      "Epoch:  34 Train Loss: 0.00786755 Val Loss: 0.00742322\n",
      "Epoch:  35 Train Loss: 0.00899037 Val Loss: 0.00726724\n",
      "Epoch:  36 Train Loss: 0.00818855 Val Loss: 0.00706469\n",
      "Epoch:  37 Train Loss: 0.00765951 Val Loss: 0.00700022\n",
      "Epoch:  38 Train Loss: 0.00879801 Val Loss: 0.00719079\n",
      "Epoch:  39 Train Loss: 0.00803887 Val Loss: 0.00726158\n",
      "Epoch:  40 Train Loss: 0.00822083 Val Loss: 0.00695910\n",
      "Epoch:  41 Train Loss: 0.00946474 Val Loss: 0.00966404\n",
      "Epoch:  42 Train Loss: 0.00834160 Val Loss: 0.00841647\n",
      "Epoch:  43 Train Loss: 0.01002540 Val Loss: 0.00895618\n",
      "Epoch:  44 Train Loss: 0.00895829 Val Loss: 0.00783316\n",
      "Epoch:  45 Train Loss: 0.00969467 Val Loss: 0.01009840\n",
      "Epoch:  46 Train Loss: 0.00934280 Val Loss: 0.00752972\n",
      "Epoch:  47 Train Loss: 0.00802435 Val Loss: 0.00827878\n",
      "Epoch:  48 Train Loss: 0.00777053 Val Loss: 0.00778738\n",
      "Epoch:  49 Train Loss: 0.00830005 Val Loss: 0.00687424\n",
      "Epoch:  50 Train Loss: 0.00672136 Val Loss: 0.00683812\n",
      "Epoch:  51 Train Loss: 0.00743236 Val Loss: 0.00675721\n",
      "Epoch:  52 Train Loss: 0.00866641 Val Loss: 0.00755373\n",
      "Epoch:  53 Train Loss: 0.01006233 Val Loss: 0.00713249\n",
      "Epoch:  54 Train Loss: 0.00918224 Val Loss: 0.00677551\n",
      "Epoch:  55 Train Loss: 0.00696820 Val Loss: 0.00678763\n",
      "Epoch:  56 Train Loss: 0.00763896 Val Loss: 0.00732776\n",
      "Epoch:  57 Train Loss: 0.00799859 Val Loss: 0.00686787\n",
      "Epoch:  58 Train Loss: 0.00813807 Val Loss: 0.00700268\n",
      "Epoch:  59 Train Loss: 0.00771531 Val Loss: 0.00722774\n",
      "Epoch:  60 Train Loss: 0.00939481 Val Loss: 0.00872603\n",
      "Epoch:  61 Train Loss: 0.00740290 Val Loss: 0.00707727\n",
      "Epoch:  62 Train Loss: 0.00748968 Val Loss: 0.00788829\n",
      "Epoch:  63 Train Loss: 0.00803159 Val Loss: 0.00687066\n",
      "Epoch:  64 Train Loss: 0.00851537 Val Loss: 0.00704063\n",
      "Epoch:  65 Train Loss: 0.00831736 Val Loss: 0.00691529\n",
      "Epoch:  66 Train Loss: 0.00751406 Val Loss: 0.00668033\n",
      "Epoch:  67 Train Loss: 0.00765883 Val Loss: 0.00670915\n",
      "Epoch:  68 Train Loss: 0.00841082 Val Loss: 0.00776138\n",
      "Epoch:  69 Train Loss: 0.00884265 Val Loss: 0.00661029\n",
      "Epoch:  70 Train Loss: 0.00931623 Val Loss: 0.00658046\n",
      "Epoch:  71 Train Loss: 0.00749438 Val Loss: 0.00697361\n",
      "Epoch:  72 Train Loss: 0.00683325 Val Loss: 0.00655840\n",
      "Epoch:  73 Train Loss: 0.00624430 Val Loss: 0.00659123\n",
      "Epoch:  74 Train Loss: 0.00664072 Val Loss: 0.00680364\n",
      "Epoch:  75 Train Loss: 0.00719496 Val Loss: 0.00662059\n",
      "Epoch:  76 Train Loss: 0.00788795 Val Loss: 0.00661404\n",
      "Epoch:  77 Train Loss: 0.00656248 Val Loss: 0.00652392\n",
      "Epoch:  78 Train Loss: 0.00734114 Val Loss: 0.00757683\n",
      "Epoch:  79 Train Loss: 0.00699614 Val Loss: 0.00648850\n",
      "Epoch:  80 Train Loss: 0.00694108 Val Loss: 0.00751332\n",
      "Epoch:  81 Train Loss: 0.00797065 Val Loss: 0.00687632\n",
      "Epoch:  82 Train Loss: 0.00705183 Val Loss: 0.00708289\n",
      "Epoch:  83 Train Loss: 0.00777480 Val Loss: 0.00729830\n",
      "Epoch:  84 Train Loss: 0.00713233 Val Loss: 0.00656341\n",
      "Epoch:  85 Train Loss: 0.00687649 Val Loss: 0.00745239\n",
      "Epoch:  86 Train Loss: 0.00643728 Val Loss: 0.00712066\n",
      "Epoch:  87 Train Loss: 0.00802933 Val Loss: 0.00671616\n",
      "Epoch:  88 Train Loss: 0.00810022 Val Loss: 0.00658646\n",
      "Epoch:  89 Train Loss: 0.00735857 Val Loss: 0.00684424\n",
      "Epoch:  90 Train Loss: 0.00697403 Val Loss: 0.00651368\n",
      "Epoch:  91 Train Loss: 0.00766754 Val Loss: 0.00662490\n",
      "Epoch:  92 Train Loss: 0.00753726 Val Loss: 0.00849045\n",
      "Epoch:  93 Train Loss: 0.00755209 Val Loss: 0.00754835\n",
      "Epoch:  94 Train Loss: 0.00823712 Val Loss: 0.00743830\n",
      "Epoch:  95 Train Loss: 0.00719529 Val Loss: 0.00646115\n",
      "Epoch:  96 Train Loss: 0.00652367 Val Loss: 0.00646789\n",
      "Epoch:  97 Train Loss: 0.00685786 Val Loss: 0.00724620\n",
      "Epoch:  98 Train Loss: 0.00670133 Val Loss: 0.00668936\n",
      "Epoch:  99 Train Loss: 0.00666033 Val Loss: 0.00683688\n",
      "Epoch: 100 Train Loss: 0.00693980 Val Loss: 0.00643232\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.05961082 Val Loss: 0.01536151\n",
      "Epoch:   2 Train Loss: 0.02549087 Val Loss: 0.01548484\n",
      "Epoch:   3 Train Loss: 0.01686530 Val Loss: 0.00928480\n",
      "Epoch:   4 Train Loss: 0.01148834 Val Loss: 0.01065968\n",
      "Epoch:   5 Train Loss: 0.01208962 Val Loss: 0.00979867\n",
      "Epoch:   6 Train Loss: 0.01216252 Val Loss: 0.01112837\n",
      "Epoch:   7 Train Loss: 0.01333574 Val Loss: 0.01190065\n",
      "Epoch:   8 Train Loss: 0.01099848 Val Loss: 0.00891250\n",
      "Epoch:   9 Train Loss: 0.01065849 Val Loss: 0.00975696\n",
      "Epoch:  10 Train Loss: 0.01002117 Val Loss: 0.00903275\n",
      "Epoch:  11 Train Loss: 0.00987641 Val Loss: 0.01062653\n",
      "Epoch:  12 Train Loss: 0.00968413 Val Loss: 0.01084275\n",
      "Epoch:  13 Train Loss: 0.00991084 Val Loss: 0.00939065\n",
      "Epoch:  14 Train Loss: 0.01185009 Val Loss: 0.00786326\n",
      "Epoch:  15 Train Loss: 0.00964496 Val Loss: 0.00848112\n",
      "Epoch:  16 Train Loss: 0.00999162 Val Loss: 0.00823000\n",
      "Epoch:  17 Train Loss: 0.00878019 Val Loss: 0.00942679\n",
      "Epoch:  18 Train Loss: 0.00941348 Val Loss: 0.00832692\n",
      "Epoch:  19 Train Loss: 0.01028908 Val Loss: 0.00773805\n",
      "Epoch:  20 Train Loss: 0.00770235 Val Loss: 0.00791926\n",
      "Epoch:  21 Train Loss: 0.00838856 Val Loss: 0.00770100\n",
      "Epoch:  22 Train Loss: 0.01015925 Val Loss: 0.00917020\n",
      "Epoch:  23 Train Loss: 0.00937094 Val Loss: 0.00796540\n",
      "Epoch:  24 Train Loss: 0.01116666 Val Loss: 0.00862563\n",
      "Epoch:  25 Train Loss: 0.00879507 Val Loss: 0.00865539\n",
      "Epoch:  26 Train Loss: 0.00922834 Val Loss: 0.00885197\n",
      "Epoch:  27 Train Loss: 0.00823493 Val Loss: 0.00869018\n",
      "Epoch:  28 Train Loss: 0.00880127 Val Loss: 0.00802111\n",
      "Epoch:  29 Train Loss: 0.00902615 Val Loss: 0.00789308\n",
      "Epoch:  30 Train Loss: 0.00852254 Val Loss: 0.00755156\n",
      "Epoch:  31 Train Loss: 0.00814945 Val Loss: 0.01243002\n",
      "Epoch:  32 Train Loss: 0.00969138 Val Loss: 0.00876820\n",
      "Epoch:  33 Train Loss: 0.00912478 Val Loss: 0.00824889\n",
      "Epoch:  34 Train Loss: 0.00947298 Val Loss: 0.00957002\n",
      "Epoch:  35 Train Loss: 0.01327144 Val Loss: 0.00824971\n",
      "Epoch:  36 Train Loss: 0.01785004 Val Loss: 0.01127853\n",
      "Epoch:  37 Train Loss: 0.01107152 Val Loss: 0.00950339\n",
      "Epoch:  38 Train Loss: 0.00928652 Val Loss: 0.00867665\n",
      "Epoch:  39 Train Loss: 0.01448167 Val Loss: 0.00901764\n",
      "Epoch:  40 Train Loss: 0.01034091 Val Loss: 0.00971178\n",
      "Epoch:  41 Train Loss: 0.00856392 Val Loss: 0.00889374\n",
      "Epoch:  42 Train Loss: 0.00994777 Val Loss: 0.00797537\n",
      "Epoch:  43 Train Loss: 0.00861106 Val Loss: 0.01066696\n",
      "Epoch:  44 Train Loss: 0.01044239 Val Loss: 0.00882542\n",
      "Epoch:  45 Train Loss: 0.00993367 Val Loss: 0.00781705\n",
      "Epoch:  46 Train Loss: 0.00893503 Val Loss: 0.00871925\n",
      "Epoch:  47 Train Loss: 0.00924127 Val Loss: 0.00770863\n",
      "Epoch:  48 Train Loss: 0.00897296 Val Loss: 0.00996405\n",
      "Epoch:  49 Train Loss: 0.00939973 Val Loss: 0.00792360\n",
      "Epoch:  50 Train Loss: 0.00847035 Val Loss: 0.00906587\n",
      "\n",
      "Training with parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.11279100 Val Loss: 0.05623770\n",
      "Epoch:   2 Train Loss: 0.04849149 Val Loss: 0.02316099\n",
      "Epoch:   3 Train Loss: 0.03315504 Val Loss: 0.04163764\n",
      "Epoch:   4 Train Loss: 0.02111985 Val Loss: 0.01018686\n",
      "Epoch:   5 Train Loss: 0.01187256 Val Loss: 0.01163822\n",
      "Epoch:   6 Train Loss: 0.01048380 Val Loss: 0.01053834\n",
      "Epoch:   7 Train Loss: 0.01057226 Val Loss: 0.00876780\n",
      "Epoch:   8 Train Loss: 0.00971680 Val Loss: 0.00854830\n",
      "Epoch:   9 Train Loss: 0.01026264 Val Loss: 0.01081355\n",
      "Epoch:  10 Train Loss: 0.01030245 Val Loss: 0.00865608\n",
      "Epoch:  11 Train Loss: 0.01152473 Val Loss: 0.00844359\n",
      "Epoch:  12 Train Loss: 0.00919116 Val Loss: 0.00862789\n",
      "Epoch:  13 Train Loss: 0.01000705 Val Loss: 0.00814401\n",
      "Epoch:  14 Train Loss: 0.01083966 Val Loss: 0.00802674\n",
      "Epoch:  15 Train Loss: 0.00962090 Val Loss: 0.00920095\n",
      "Epoch:  16 Train Loss: 0.01156411 Val Loss: 0.00833721\n",
      "Epoch:  17 Train Loss: 0.00855334 Val Loss: 0.00783336\n",
      "Epoch:  18 Train Loss: 0.01086573 Val Loss: 0.00850066\n",
      "Epoch:  19 Train Loss: 0.00964789 Val Loss: 0.00825969\n",
      "Epoch:  20 Train Loss: 0.01127823 Val Loss: 0.01213633\n",
      "Epoch:  21 Train Loss: 0.01112564 Val Loss: 0.00938030\n",
      "Epoch:  22 Train Loss: 0.00949117 Val Loss: 0.00812643\n",
      "Epoch:  23 Train Loss: 0.00908067 Val Loss: 0.00975603\n",
      "Epoch:  24 Train Loss: 0.00923868 Val Loss: 0.01290711\n",
      "Epoch:  25 Train Loss: 0.01314097 Val Loss: 0.00800158\n",
      "Epoch:  26 Train Loss: 0.00996654 Val Loss: 0.01010536\n",
      "Epoch:  27 Train Loss: 0.01133991 Val Loss: 0.00760785\n",
      "Epoch:  28 Train Loss: 0.00946513 Val Loss: 0.00981914\n",
      "Epoch:  29 Train Loss: 0.00909442 Val Loss: 0.00776219\n",
      "Epoch:  30 Train Loss: 0.00836706 Val Loss: 0.00838493\n",
      "Epoch:  31 Train Loss: 0.00801795 Val Loss: 0.01018800\n",
      "Epoch:  32 Train Loss: 0.00936199 Val Loss: 0.01123259\n",
      "Epoch:  33 Train Loss: 0.01119985 Val Loss: 0.00859860\n",
      "Epoch:  34 Train Loss: 0.00827425 Val Loss: 0.00767069\n",
      "Epoch:  35 Train Loss: 0.00853695 Val Loss: 0.00782401\n",
      "Epoch:  36 Train Loss: 0.00868240 Val Loss: 0.00810296\n",
      "Epoch:  37 Train Loss: 0.00857726 Val Loss: 0.00905546\n",
      "Epoch:  38 Train Loss: 0.00826148 Val Loss: 0.00846308\n",
      "Epoch:  39 Train Loss: 0.01008751 Val Loss: 0.01049310\n",
      "Epoch:  40 Train Loss: 0.00963435 Val Loss: 0.01128367\n",
      "Epoch:  41 Train Loss: 0.01067337 Val Loss: 0.00899146\n",
      "Epoch:  42 Train Loss: 0.00857289 Val Loss: 0.00871986\n",
      "Epoch:  43 Train Loss: 0.00866555 Val Loss: 0.00906610\n",
      "Epoch:  44 Train Loss: 0.00854832 Val Loss: 0.00733814\n",
      "Epoch:  45 Train Loss: 0.00837273 Val Loss: 0.00803690\n",
      "Epoch:  46 Train Loss: 0.00844414 Val Loss: 0.00738892\n",
      "Epoch:  47 Train Loss: 0.00971691 Val Loss: 0.01181588\n",
      "Epoch:  48 Train Loss: 0.01072881 Val Loss: 0.00800809\n",
      "Epoch:  49 Train Loss: 0.00836803 Val Loss: 0.00727224\n",
      "Epoch:  50 Train Loss: 0.00765153 Val Loss: 0.00936534\n",
      "Epoch:  51 Train Loss: 0.01284198 Val Loss: 0.00715911\n",
      "Epoch:  52 Train Loss: 0.00813337 Val Loss: 0.00776786\n",
      "Epoch:  53 Train Loss: 0.01018291 Val Loss: 0.00933521\n",
      "Epoch:  54 Train Loss: 0.00913706 Val Loss: 0.00746168\n",
      "Epoch:  55 Train Loss: 0.00819709 Val Loss: 0.00785851\n",
      "Epoch:  56 Train Loss: 0.00904871 Val Loss: 0.01017370\n",
      "Epoch:  57 Train Loss: 0.00682439 Val Loss: 0.00860813\n",
      "Epoch:  58 Train Loss: 0.01242543 Val Loss: 0.00808285\n",
      "Epoch:  59 Train Loss: 0.00957840 Val Loss: 0.00749817\n",
      "Epoch:  60 Train Loss: 0.00897572 Val Loss: 0.00697924\n",
      "Epoch:  61 Train Loss: 0.00672640 Val Loss: 0.00697668\n",
      "Epoch:  62 Train Loss: 0.00868149 Val Loss: 0.00728658\n",
      "Epoch:  63 Train Loss: 0.00809242 Val Loss: 0.00978292\n",
      "Epoch:  64 Train Loss: 0.00895265 Val Loss: 0.00867580\n",
      "Epoch:  65 Train Loss: 0.00854525 Val Loss: 0.00907478\n",
      "Epoch:  66 Train Loss: 0.00896352 Val Loss: 0.00957495\n",
      "Epoch:  67 Train Loss: 0.00981008 Val Loss: 0.00745335\n",
      "Epoch:  68 Train Loss: 0.00877622 Val Loss: 0.00997971\n",
      "Epoch:  69 Train Loss: 0.00958592 Val Loss: 0.00866988\n",
      "Epoch:  70 Train Loss: 0.00932256 Val Loss: 0.00991916\n",
      "Epoch:  71 Train Loss: 0.00987757 Val Loss: 0.00923857\n",
      "Epoch:  72 Train Loss: 0.01263547 Val Loss: 0.01150846\n",
      "Epoch:  73 Train Loss: 0.00787204 Val Loss: 0.00848208\n",
      "Epoch:  74 Train Loss: 0.00815247 Val Loss: 0.00863768\n",
      "Epoch:  75 Train Loss: 0.00852541 Val Loss: 0.00957296\n",
      "Epoch:  76 Train Loss: 0.00997479 Val Loss: 0.00850820\n",
      "Epoch:  77 Train Loss: 0.00927617 Val Loss: 0.00832441\n",
      "Epoch:  78 Train Loss: 0.00939245 Val Loss: 0.01195211\n",
      "Epoch:  79 Train Loss: 0.00794354 Val Loss: 0.00867139\n",
      "Epoch:  80 Train Loss: 0.01003958 Val Loss: 0.00770296\n",
      "Epoch:  81 Train Loss: 0.00839724 Val Loss: 0.00855243\n",
      "Epoch:  82 Train Loss: 0.00749945 Val Loss: 0.00740545\n",
      "Epoch:  83 Train Loss: 0.00868819 Val Loss: 0.00767292\n",
      "Epoch:  84 Train Loss: 0.00785651 Val Loss: 0.01307835\n",
      "Epoch:  85 Train Loss: 0.00893440 Val Loss: 0.00858374\n",
      "Epoch:  86 Train Loss: 0.00910616 Val Loss: 0.00917431\n",
      "Epoch:  87 Train Loss: 0.00900748 Val Loss: 0.00816820\n",
      "Epoch:  88 Train Loss: 0.00787035 Val Loss: 0.00809985\n",
      "Epoch:  89 Train Loss: 0.00762741 Val Loss: 0.00830153\n",
      "Epoch:  90 Train Loss: 0.00765499 Val Loss: 0.00752975\n",
      "Epoch:  91 Train Loss: 0.00821930 Val Loss: 0.01014105\n",
      "Epoch:  92 Train Loss: 0.00982378 Val Loss: 0.00788991\n",
      "Epoch:  93 Train Loss: 0.00745318 Val Loss: 0.00847333\n",
      "Epoch:  94 Train Loss: 0.00940761 Val Loss: 0.00739969\n",
      "Epoch:  95 Train Loss: 0.00727717 Val Loss: 0.00772591\n",
      "Epoch:  96 Train Loss: 0.00640527 Val Loss: 0.00748381\n",
      "Epoch:  97 Train Loss: 0.00853413 Val Loss: 0.00707105\n",
      "Epoch:  98 Train Loss: 0.00746070 Val Loss: 0.00820261\n",
      "Epoch:  99 Train Loss: 0.00926966 Val Loss: 0.00683835\n",
      "Epoch: 100 Train Loss: 0.00785605 Val Loss: 0.00799776\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.15596238 Val Loss: 0.09309996\n",
      "Epoch:   2 Train Loss: 0.06129683 Val Loss: 0.03586703\n",
      "Epoch:   3 Train Loss: 0.05248086 Val Loss: 0.02787515\n",
      "Epoch:   4 Train Loss: 0.03969540 Val Loss: 0.02731937\n",
      "Epoch:   5 Train Loss: 0.03523424 Val Loss: 0.02133928\n",
      "Epoch:   6 Train Loss: 0.02910709 Val Loss: 0.01792664\n",
      "Epoch:   7 Train Loss: 0.02185212 Val Loss: 0.01249250\n",
      "Epoch:   8 Train Loss: 0.01362250 Val Loss: 0.01504033\n",
      "Epoch:   9 Train Loss: 0.01113895 Val Loss: 0.01136481\n",
      "Epoch:  10 Train Loss: 0.00896859 Val Loss: 0.01279601\n",
      "Epoch:  11 Train Loss: 0.00966647 Val Loss: 0.00883270\n",
      "Epoch:  12 Train Loss: 0.00787839 Val Loss: 0.00749531\n",
      "Epoch:  13 Train Loss: 0.01093333 Val Loss: 0.01509397\n",
      "Epoch:  14 Train Loss: 0.01034202 Val Loss: 0.00797660\n",
      "Epoch:  15 Train Loss: 0.01125629 Val Loss: 0.00861652\n",
      "Epoch:  16 Train Loss: 0.00885356 Val Loss: 0.00805142\n",
      "Epoch:  17 Train Loss: 0.00833781 Val Loss: 0.00713882\n",
      "Epoch:  18 Train Loss: 0.00772200 Val Loss: 0.01029536\n",
      "Epoch:  19 Train Loss: 0.00755647 Val Loss: 0.00699771\n",
      "Epoch:  20 Train Loss: 0.00706414 Val Loss: 0.00698513\n",
      "Epoch:  21 Train Loss: 0.00818494 Val Loss: 0.00799166\n",
      "Epoch:  22 Train Loss: 0.00695615 Val Loss: 0.00684105\n",
      "Epoch:  23 Train Loss: 0.00644116 Val Loss: 0.00689050\n",
      "Epoch:  24 Train Loss: 0.00643578 Val Loss: 0.00743382\n",
      "Epoch:  25 Train Loss: 0.00671585 Val Loss: 0.00683377\n",
      "Epoch:  26 Train Loss: 0.00648253 Val Loss: 0.00678882\n",
      "Epoch:  27 Train Loss: 0.00615395 Val Loss: 0.00672646\n",
      "Epoch:  28 Train Loss: 0.00726065 Val Loss: 0.00756670\n",
      "Epoch:  29 Train Loss: 0.00778518 Val Loss: 0.00792504\n",
      "Epoch:  30 Train Loss: 0.00892930 Val Loss: 0.01026422\n",
      "Epoch:  31 Train Loss: 0.00853190 Val Loss: 0.00821734\n",
      "Epoch:  32 Train Loss: 0.00744057 Val Loss: 0.00778876\n",
      "Epoch:  33 Train Loss: 0.00724145 Val Loss: 0.00661763\n",
      "Epoch:  34 Train Loss: 0.00631329 Val Loss: 0.00666253\n",
      "Epoch:  35 Train Loss: 0.00763352 Val Loss: 0.00716603\n",
      "Epoch:  36 Train Loss: 0.00722723 Val Loss: 0.00751811\n",
      "Epoch:  37 Train Loss: 0.00702592 Val Loss: 0.00722991\n",
      "Epoch:  38 Train Loss: 0.00735496 Val Loss: 0.00704173\n",
      "Epoch:  39 Train Loss: 0.00939798 Val Loss: 0.00689590\n",
      "Epoch:  40 Train Loss: 0.00744079 Val Loss: 0.00700351\n",
      "Epoch:  41 Train Loss: 0.00638633 Val Loss: 0.00667184\n",
      "Epoch:  42 Train Loss: 0.00798864 Val Loss: 0.00811431\n",
      "Epoch:  43 Train Loss: 0.00739486 Val Loss: 0.00664582\n",
      "Epoch:  44 Train Loss: 0.00652363 Val Loss: 0.00681792\n",
      "Epoch:  45 Train Loss: 0.00757937 Val Loss: 0.00789829\n",
      "Epoch:  46 Train Loss: 0.00867849 Val Loss: 0.00681890\n",
      "Epoch:  47 Train Loss: 0.00658780 Val Loss: 0.00660232\n",
      "Epoch:  48 Train Loss: 0.00588084 Val Loss: 0.00662786\n",
      "Epoch:  49 Train Loss: 0.00679667 Val Loss: 0.00669708\n",
      "Epoch:  50 Train Loss: 0.00642162 Val Loss: 0.00650884\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.11570864 Val Loss: 0.06772475\n",
      "Epoch:   2 Train Loss: 0.05438086 Val Loss: 0.03739448\n",
      "Epoch:   3 Train Loss: 0.04663324 Val Loss: 0.02860344\n",
      "Epoch:   4 Train Loss: 0.04389748 Val Loss: 0.02749796\n",
      "Epoch:   5 Train Loss: 0.03504946 Val Loss: 0.02131881\n",
      "Epoch:   6 Train Loss: 0.02857577 Val Loss: 0.01733840\n",
      "Epoch:   7 Train Loss: 0.01861147 Val Loss: 0.01057877\n",
      "Epoch:   8 Train Loss: 0.01161738 Val Loss: 0.01307677\n",
      "Epoch:   9 Train Loss: 0.00996839 Val Loss: 0.01244312\n",
      "Epoch:  10 Train Loss: 0.01032197 Val Loss: 0.00953667\n",
      "Epoch:  11 Train Loss: 0.00841823 Val Loss: 0.01031640\n",
      "Epoch:  12 Train Loss: 0.00895601 Val Loss: 0.00755767\n",
      "Epoch:  13 Train Loss: 0.00995460 Val Loss: 0.01090381\n",
      "Epoch:  14 Train Loss: 0.00756666 Val Loss: 0.00721854\n",
      "Epoch:  15 Train Loss: 0.00846744 Val Loss: 0.00953472\n",
      "Epoch:  16 Train Loss: 0.00694529 Val Loss: 0.00774073\n",
      "Epoch:  17 Train Loss: 0.00799102 Val Loss: 0.00696586\n",
      "Epoch:  18 Train Loss: 0.00724071 Val Loss: 0.00859040\n",
      "Epoch:  19 Train Loss: 0.00739678 Val Loss: 0.00768731\n",
      "Epoch:  20 Train Loss: 0.00847462 Val Loss: 0.00997052\n",
      "Epoch:  21 Train Loss: 0.00743752 Val Loss: 0.00683286\n",
      "Epoch:  22 Train Loss: 0.00656227 Val Loss: 0.00711701\n",
      "Epoch:  23 Train Loss: 0.00624352 Val Loss: 0.00677983\n",
      "Epoch:  24 Train Loss: 0.00658281 Val Loss: 0.00676726\n",
      "Epoch:  25 Train Loss: 0.00776041 Val Loss: 0.00665961\n",
      "Epoch:  26 Train Loss: 0.00750067 Val Loss: 0.00720200\n",
      "Epoch:  27 Train Loss: 0.00658194 Val Loss: 0.00683169\n",
      "Epoch:  28 Train Loss: 0.00701691 Val Loss: 0.00672213\n",
      "Epoch:  29 Train Loss: 0.00776374 Val Loss: 0.00672107\n",
      "Epoch:  30 Train Loss: 0.00740172 Val Loss: 0.00662822\n",
      "Epoch:  31 Train Loss: 0.00670177 Val Loss: 0.00661296\n",
      "Epoch:  32 Train Loss: 0.00672153 Val Loss: 0.00656394\n",
      "Epoch:  33 Train Loss: 0.00688992 Val Loss: 0.00685933\n",
      "Epoch:  34 Train Loss: 0.00725233 Val Loss: 0.00797961\n",
      "Epoch:  35 Train Loss: 0.00824957 Val Loss: 0.00789346\n",
      "Epoch:  36 Train Loss: 0.00785872 Val Loss: 0.00790828\n",
      "Epoch:  37 Train Loss: 0.01034512 Val Loss: 0.00705872\n",
      "Epoch:  38 Train Loss: 0.00816259 Val Loss: 0.00696367\n",
      "Epoch:  39 Train Loss: 0.00816514 Val Loss: 0.00782771\n",
      "Epoch:  40 Train Loss: 0.00860623 Val Loss: 0.00668514\n",
      "Epoch:  41 Train Loss: 0.00742473 Val Loss: 0.00776589\n",
      "Epoch:  42 Train Loss: 0.00748953 Val Loss: 0.00656835\n",
      "Epoch:  43 Train Loss: 0.00777557 Val Loss: 0.00675454\n",
      "Epoch:  44 Train Loss: 0.00598930 Val Loss: 0.00665172\n",
      "Epoch:  45 Train Loss: 0.00687890 Val Loss: 0.00667400\n",
      "Epoch:  46 Train Loss: 0.00731921 Val Loss: 0.00748705\n",
      "Epoch:  47 Train Loss: 0.00849212 Val Loss: 0.00747586\n",
      "Epoch:  48 Train Loss: 0.00895106 Val Loss: 0.00786732\n",
      "Epoch:  49 Train Loss: 0.00736827 Val Loss: 0.00644456\n",
      "Epoch:  50 Train Loss: 0.00629805 Val Loss: 0.00655646\n",
      "Epoch:  51 Train Loss: 0.00587472 Val Loss: 0.00642015\n",
      "Epoch:  52 Train Loss: 0.00645851 Val Loss: 0.00661100\n",
      "Epoch:  53 Train Loss: 0.00906291 Val Loss: 0.00663255\n",
      "Epoch:  54 Train Loss: 0.00634536 Val Loss: 0.00645135\n",
      "Epoch:  55 Train Loss: 0.00623408 Val Loss: 0.00659385\n",
      "Epoch:  56 Train Loss: 0.00612949 Val Loss: 0.00642192\n",
      "Epoch:  57 Train Loss: 0.00750044 Val Loss: 0.00753878\n",
      "Epoch:  58 Train Loss: 0.00696677 Val Loss: 0.00638897\n",
      "Epoch:  59 Train Loss: 0.00566182 Val Loss: 0.00637402\n",
      "Epoch:  60 Train Loss: 0.00619321 Val Loss: 0.00645175\n",
      "Epoch:  61 Train Loss: 0.00648821 Val Loss: 0.00691543\n",
      "Epoch:  62 Train Loss: 0.00645344 Val Loss: 0.00675558\n",
      "Epoch:  63 Train Loss: 0.00723520 Val Loss: 0.00633490\n",
      "Epoch:  64 Train Loss: 0.00656308 Val Loss: 0.00655600\n",
      "Epoch:  65 Train Loss: 0.00775457 Val Loss: 0.00653016\n",
      "Epoch:  66 Train Loss: 0.00660266 Val Loss: 0.00673401\n",
      "Epoch:  67 Train Loss: 0.00652230 Val Loss: 0.00640845\n",
      "Epoch:  68 Train Loss: 0.00588299 Val Loss: 0.00710811\n",
      "Epoch:  69 Train Loss: 0.00666004 Val Loss: 0.00648530\n",
      "Epoch:  70 Train Loss: 0.00617714 Val Loss: 0.00841007\n",
      "Epoch:  71 Train Loss: 0.00655167 Val Loss: 0.00733565\n",
      "Epoch:  72 Train Loss: 0.00684913 Val Loss: 0.00837751\n",
      "Epoch:  73 Train Loss: 0.00753941 Val Loss: 0.00638502\n",
      "Epoch:  74 Train Loss: 0.00613037 Val Loss: 0.00624895\n",
      "Epoch:  75 Train Loss: 0.00615229 Val Loss: 0.00668581\n",
      "Epoch:  76 Train Loss: 0.00613409 Val Loss: 0.00730718\n",
      "Epoch:  77 Train Loss: 0.00685916 Val Loss: 0.00783704\n",
      "Epoch:  78 Train Loss: 0.00686955 Val Loss: 0.00630242\n",
      "Epoch:  79 Train Loss: 0.00728371 Val Loss: 0.00766692\n",
      "Epoch:  80 Train Loss: 0.00728935 Val Loss: 0.00995543\n",
      "Epoch:  81 Train Loss: 0.00905075 Val Loss: 0.00645049\n",
      "Epoch:  82 Train Loss: 0.00819730 Val Loss: 0.00620883\n",
      "Epoch:  83 Train Loss: 0.00687944 Val Loss: 0.00717365\n",
      "Epoch:  84 Train Loss: 0.00848416 Val Loss: 0.00654082\n",
      "Epoch:  85 Train Loss: 0.00754167 Val Loss: 0.00624743\n",
      "Epoch:  86 Train Loss: 0.00610326 Val Loss: 0.00704011\n",
      "Epoch:  87 Train Loss: 0.00590969 Val Loss: 0.00643264\n",
      "Epoch:  88 Train Loss: 0.00598641 Val Loss: 0.00624521\n",
      "Epoch:  89 Train Loss: 0.00640230 Val Loss: 0.00673834\n",
      "Epoch:  90 Train Loss: 0.00634207 Val Loss: 0.00647573\n",
      "Epoch:  91 Train Loss: 0.00628752 Val Loss: 0.00634281\n",
      "Epoch:  92 Train Loss: 0.00571591 Val Loss: 0.00697948\n",
      "Epoch:  93 Train Loss: 0.00728915 Val Loss: 0.00943621\n",
      "Epoch:  94 Train Loss: 0.00646781 Val Loss: 0.00732646\n",
      "Epoch:  95 Train Loss: 0.00767538 Val Loss: 0.00647454\n",
      "Epoch:  96 Train Loss: 0.00607482 Val Loss: 0.00623269\n",
      "Epoch:  97 Train Loss: 0.00574933 Val Loss: 0.00631098\n",
      "Epoch:  98 Train Loss: 0.00711477 Val Loss: 0.00823914\n",
      "Epoch:  99 Train Loss: 0.00663625 Val Loss: 0.00655636\n",
      "Epoch: 100 Train Loss: 0.00667668 Val Loss: 0.00637265\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.10768913 Val Loss: 0.06998714\n",
      "Epoch:   2 Train Loss: 0.05776407 Val Loss: 0.02773845\n",
      "Epoch:   3 Train Loss: 0.03016838 Val Loss: 0.02226158\n",
      "Epoch:   4 Train Loss: 0.01810792 Val Loss: 0.01032773\n",
      "Epoch:   5 Train Loss: 0.01365974 Val Loss: 0.01047606\n",
      "Epoch:   6 Train Loss: 0.01199896 Val Loss: 0.01201157\n",
      "Epoch:   7 Train Loss: 0.01222291 Val Loss: 0.01081493\n",
      "Epoch:   8 Train Loss: 0.01015812 Val Loss: 0.01081156\n",
      "Epoch:   9 Train Loss: 0.00961861 Val Loss: 0.01136377\n",
      "Epoch:  10 Train Loss: 0.01127882 Val Loss: 0.00972700\n",
      "Epoch:  11 Train Loss: 0.00930426 Val Loss: 0.00932663\n",
      "Epoch:  12 Train Loss: 0.00880080 Val Loss: 0.00906496\n",
      "Epoch:  13 Train Loss: 0.01028531 Val Loss: 0.00875177\n",
      "Epoch:  14 Train Loss: 0.01053503 Val Loss: 0.01005617\n",
      "Epoch:  15 Train Loss: 0.00970318 Val Loss: 0.01198584\n",
      "Epoch:  16 Train Loss: 0.01165702 Val Loss: 0.01381456\n",
      "Epoch:  17 Train Loss: 0.01161447 Val Loss: 0.00823581\n",
      "Epoch:  18 Train Loss: 0.00774933 Val Loss: 0.00870686\n",
      "Epoch:  19 Train Loss: 0.00933168 Val Loss: 0.00996399\n",
      "Epoch:  20 Train Loss: 0.00844299 Val Loss: 0.00872092\n",
      "Epoch:  21 Train Loss: 0.00887649 Val Loss: 0.00948782\n",
      "Epoch:  22 Train Loss: 0.00899629 Val Loss: 0.00869654\n",
      "Epoch:  23 Train Loss: 0.00756828 Val Loss: 0.00794036\n",
      "Epoch:  24 Train Loss: 0.00824800 Val Loss: 0.00790857\n",
      "Epoch:  25 Train Loss: 0.00863071 Val Loss: 0.00829505\n",
      "Epoch:  26 Train Loss: 0.00917364 Val Loss: 0.00872237\n",
      "Epoch:  27 Train Loss: 0.00927073 Val Loss: 0.00864250\n",
      "Epoch:  28 Train Loss: 0.00874651 Val Loss: 0.00772145\n",
      "Epoch:  29 Train Loss: 0.00914654 Val Loss: 0.00769787\n",
      "Epoch:  30 Train Loss: 0.00970434 Val Loss: 0.00915099\n",
      "Epoch:  31 Train Loss: 0.00882795 Val Loss: 0.01791622\n",
      "Epoch:  32 Train Loss: 0.01182149 Val Loss: 0.00787901\n",
      "Epoch:  33 Train Loss: 0.00978000 Val Loss: 0.01129965\n",
      "Epoch:  34 Train Loss: 0.00944196 Val Loss: 0.00957226\n",
      "Epoch:  35 Train Loss: 0.00878130 Val Loss: 0.01246120\n",
      "Epoch:  36 Train Loss: 0.00908742 Val Loss: 0.00868703\n",
      "Epoch:  37 Train Loss: 0.00958763 Val Loss: 0.01244386\n",
      "Epoch:  38 Train Loss: 0.00952337 Val Loss: 0.01738155\n",
      "Epoch:  39 Train Loss: 0.01214167 Val Loss: 0.00996598\n",
      "Epoch:  40 Train Loss: 0.01038954 Val Loss: 0.00987828\n",
      "Epoch:  41 Train Loss: 0.01014217 Val Loss: 0.01201462\n",
      "Epoch:  42 Train Loss: 0.01037025 Val Loss: 0.00840648\n",
      "Epoch:  43 Train Loss: 0.00797800 Val Loss: 0.00888754\n",
      "Epoch:  44 Train Loss: 0.00871481 Val Loss: 0.00851329\n",
      "Epoch:  45 Train Loss: 0.00853963 Val Loss: 0.00934321\n",
      "Epoch:  46 Train Loss: 0.00862085 Val Loss: 0.01020009\n",
      "Epoch:  47 Train Loss: 0.00845059 Val Loss: 0.00779543\n",
      "Epoch:  48 Train Loss: 0.00671762 Val Loss: 0.00804161\n",
      "Epoch:  49 Train Loss: 0.00709993 Val Loss: 0.00965107\n",
      "Epoch:  50 Train Loss: 0.00827278 Val Loss: 0.00758111\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.12878888 Val Loss: 0.09312749\n",
      "Epoch:   2 Train Loss: 0.06237006 Val Loss: 0.03972359\n",
      "Epoch:   3 Train Loss: 0.03959174 Val Loss: 0.02541128\n",
      "Epoch:   4 Train Loss: 0.02807410 Val Loss: 0.01629609\n",
      "Epoch:   5 Train Loss: 0.01649776 Val Loss: 0.01167155\n",
      "Epoch:   6 Train Loss: 0.01081440 Val Loss: 0.01122969\n",
      "Epoch:   7 Train Loss: 0.01303015 Val Loss: 0.01064276\n",
      "Epoch:   8 Train Loss: 0.01144390 Val Loss: 0.01074562\n",
      "Epoch:   9 Train Loss: 0.01168185 Val Loss: 0.01045725\n",
      "Epoch:  10 Train Loss: 0.00949001 Val Loss: 0.01045853\n",
      "Epoch:  11 Train Loss: 0.01261831 Val Loss: 0.00956619\n",
      "Epoch:  12 Train Loss: 0.01119239 Val Loss: 0.01084904\n",
      "Epoch:  13 Train Loss: 0.01008798 Val Loss: 0.01648616\n",
      "Epoch:  14 Train Loss: 0.01135484 Val Loss: 0.00921528\n",
      "Epoch:  15 Train Loss: 0.00941329 Val Loss: 0.00918669\n",
      "Epoch:  16 Train Loss: 0.00995733 Val Loss: 0.01123341\n",
      "Epoch:  17 Train Loss: 0.01032456 Val Loss: 0.01041230\n",
      "Epoch:  18 Train Loss: 0.00954871 Val Loss: 0.00922506\n",
      "Epoch:  19 Train Loss: 0.00951644 Val Loss: 0.01074379\n",
      "Epoch:  20 Train Loss: 0.01107563 Val Loss: 0.01149078\n",
      "Epoch:  21 Train Loss: 0.01329326 Val Loss: 0.01614027\n",
      "Epoch:  22 Train Loss: 0.01286231 Val Loss: 0.00825397\n",
      "Epoch:  23 Train Loss: 0.00794918 Val Loss: 0.00987937\n",
      "Epoch:  24 Train Loss: 0.00846381 Val Loss: 0.01166365\n",
      "Epoch:  25 Train Loss: 0.01055488 Val Loss: 0.01274073\n",
      "Epoch:  26 Train Loss: 0.01206647 Val Loss: 0.00987616\n",
      "Epoch:  27 Train Loss: 0.01054255 Val Loss: 0.00831479\n",
      "Epoch:  28 Train Loss: 0.00883853 Val Loss: 0.00837804\n",
      "Epoch:  29 Train Loss: 0.00795625 Val Loss: 0.00881389\n",
      "Epoch:  30 Train Loss: 0.00770396 Val Loss: 0.00994219\n",
      "Epoch:  31 Train Loss: 0.00913346 Val Loss: 0.00803792\n",
      "Epoch:  32 Train Loss: 0.00726316 Val Loss: 0.00885961\n",
      "Epoch:  33 Train Loss: 0.00827236 Val Loss: 0.01458623\n",
      "Epoch:  34 Train Loss: 0.01168788 Val Loss: 0.00827160\n",
      "Epoch:  35 Train Loss: 0.00756272 Val Loss: 0.00979407\n",
      "Epoch:  36 Train Loss: 0.01023586 Val Loss: 0.00858269\n",
      "Epoch:  37 Train Loss: 0.00907741 Val Loss: 0.01709710\n",
      "Epoch:  38 Train Loss: 0.01155117 Val Loss: 0.01073161\n",
      "Epoch:  39 Train Loss: 0.00900186 Val Loss: 0.00818259\n",
      "Epoch:  40 Train Loss: 0.00847649 Val Loss: 0.00970560\n",
      "Epoch:  41 Train Loss: 0.00849899 Val Loss: 0.00883149\n",
      "Epoch:  42 Train Loss: 0.00794863 Val Loss: 0.00884417\n",
      "Epoch:  43 Train Loss: 0.00737936 Val Loss: 0.00860448\n",
      "Epoch:  44 Train Loss: 0.01056849 Val Loss: 0.00976902\n",
      "Epoch:  45 Train Loss: 0.00742034 Val Loss: 0.00783862\n",
      "Epoch:  46 Train Loss: 0.00931057 Val Loss: 0.00770854\n",
      "Epoch:  47 Train Loss: 0.00718535 Val Loss: 0.00838916\n",
      "Epoch:  48 Train Loss: 0.00789939 Val Loss: 0.00803597\n",
      "Epoch:  49 Train Loss: 0.00765746 Val Loss: 0.00755424\n",
      "Epoch:  50 Train Loss: 0.00737213 Val Loss: 0.00754814\n",
      "Epoch:  51 Train Loss: 0.00984803 Val Loss: 0.00747154\n",
      "Epoch:  52 Train Loss: 0.00855327 Val Loss: 0.00800662\n",
      "Epoch:  53 Train Loss: 0.00942857 Val Loss: 0.00827556\n",
      "Epoch:  54 Train Loss: 0.01273350 Val Loss: 0.00862254\n",
      "Epoch:  55 Train Loss: 0.00785763 Val Loss: 0.00947572\n",
      "Epoch:  56 Train Loss: 0.00711414 Val Loss: 0.00793832\n",
      "Epoch:  57 Train Loss: 0.00708533 Val Loss: 0.00713865\n",
      "Epoch:  58 Train Loss: 0.00765438 Val Loss: 0.00762517\n",
      "Epoch:  59 Train Loss: 0.00905386 Val Loss: 0.01028751\n",
      "Epoch:  60 Train Loss: 0.00869319 Val Loss: 0.00788054\n",
      "Epoch:  61 Train Loss: 0.00910615 Val Loss: 0.01287812\n",
      "Epoch:  62 Train Loss: 0.01090109 Val Loss: 0.01145826\n",
      "Epoch:  63 Train Loss: 0.00926076 Val Loss: 0.00803385\n",
      "Epoch:  64 Train Loss: 0.00818945 Val Loss: 0.00727779\n",
      "Epoch:  65 Train Loss: 0.00844917 Val Loss: 0.00860234\n",
      "Epoch:  66 Train Loss: 0.00679604 Val Loss: 0.00846420\n",
      "Epoch:  67 Train Loss: 0.00863911 Val Loss: 0.00833521\n",
      "Epoch:  68 Train Loss: 0.00754565 Val Loss: 0.00791207\n",
      "Epoch:  69 Train Loss: 0.00734186 Val Loss: 0.00728368\n",
      "Epoch:  70 Train Loss: 0.00943810 Val Loss: 0.00797165\n",
      "Epoch:  71 Train Loss: 0.00864044 Val Loss: 0.01068378\n",
      "Epoch:  72 Train Loss: 0.00905335 Val Loss: 0.00858184\n",
      "Epoch:  73 Train Loss: 0.00748067 Val Loss: 0.00842190\n",
      "Epoch:  74 Train Loss: 0.00898981 Val Loss: 0.00836547\n",
      "Epoch:  75 Train Loss: 0.00686905 Val Loss: 0.00737022\n",
      "Epoch:  76 Train Loss: 0.00814021 Val Loss: 0.00933787\n",
      "Epoch:  77 Train Loss: 0.01007891 Val Loss: 0.00793261\n",
      "Epoch:  78 Train Loss: 0.00793919 Val Loss: 0.00885128\n",
      "Epoch:  79 Train Loss: 0.00851549 Val Loss: 0.00893696\n",
      "Epoch:  80 Train Loss: 0.00725633 Val Loss: 0.00749657\n",
      "Epoch:  81 Train Loss: 0.00742361 Val Loss: 0.01090198\n",
      "Epoch:  82 Train Loss: 0.00946659 Val Loss: 0.00806819\n",
      "Epoch:  83 Train Loss: 0.00820973 Val Loss: 0.00737816\n",
      "Epoch:  84 Train Loss: 0.00700005 Val Loss: 0.00694401\n",
      "Epoch:  85 Train Loss: 0.00683075 Val Loss: 0.00737209\n",
      "Epoch:  86 Train Loss: 0.00673881 Val Loss: 0.00673266\n",
      "Epoch:  87 Train Loss: 0.00590755 Val Loss: 0.00722484\n",
      "Epoch:  88 Train Loss: 0.00856424 Val Loss: 0.00680429\n",
      "Epoch:  89 Train Loss: 0.00587393 Val Loss: 0.00667411\n",
      "Epoch:  90 Train Loss: 0.00646631 Val Loss: 0.00654721\n",
      "Epoch:  91 Train Loss: 0.00715873 Val Loss: 0.00675390\n",
      "Epoch:  92 Train Loss: 0.00689724 Val Loss: 0.01021573\n",
      "Epoch:  93 Train Loss: 0.00898846 Val Loss: 0.01224033\n",
      "Epoch:  94 Train Loss: 0.01042161 Val Loss: 0.01311680\n",
      "Epoch:  95 Train Loss: 0.01058327 Val Loss: 0.00736299\n",
      "Epoch:  96 Train Loss: 0.00923085 Val Loss: 0.00724758\n",
      "Epoch:  97 Train Loss: 0.00857581 Val Loss: 0.00991979\n",
      "Epoch:  98 Train Loss: 0.00963116 Val Loss: 0.01186258\n",
      "Epoch:  99 Train Loss: 0.00990260 Val Loss: 0.00963347\n",
      "Epoch: 100 Train Loss: 0.00814034 Val Loss: 0.01071857\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.15313336 Val Loss: 0.08664005\n",
      "Epoch:   2 Train Loss: 0.06167815 Val Loss: 0.03246379\n",
      "Epoch:   3 Train Loss: 0.04525721 Val Loss: 0.02792070\n",
      "Epoch:   4 Train Loss: 0.04068939 Val Loss: 0.02993407\n",
      "Epoch:   5 Train Loss: 0.03323637 Val Loss: 0.01974433\n",
      "Epoch:   6 Train Loss: 0.02475017 Val Loss: 0.01500990\n",
      "Epoch:   7 Train Loss: 0.01678788 Val Loss: 0.00912842\n",
      "Epoch:   8 Train Loss: 0.00915920 Val Loss: 0.01097102\n",
      "Epoch:   9 Train Loss: 0.01014398 Val Loss: 0.01232190\n",
      "Epoch:  10 Train Loss: 0.01061665 Val Loss: 0.01236895\n",
      "Epoch:  11 Train Loss: 0.00892430 Val Loss: 0.00798626\n",
      "Epoch:  12 Train Loss: 0.00838265 Val Loss: 0.00924141\n",
      "Epoch:  13 Train Loss: 0.00826593 Val Loss: 0.01013251\n",
      "Epoch:  14 Train Loss: 0.00838762 Val Loss: 0.00728950\n",
      "Epoch:  15 Train Loss: 0.00960035 Val Loss: 0.00911358\n",
      "Epoch:  16 Train Loss: 0.00988287 Val Loss: 0.00965916\n",
      "Epoch:  17 Train Loss: 0.00864611 Val Loss: 0.00703820\n",
      "Epoch:  18 Train Loss: 0.00729266 Val Loss: 0.00813863\n",
      "Epoch:  19 Train Loss: 0.00903525 Val Loss: 0.00707337\n",
      "Epoch:  20 Train Loss: 0.00775265 Val Loss: 0.00777932\n",
      "Epoch:  21 Train Loss: 0.00742855 Val Loss: 0.00677464\n",
      "Epoch:  22 Train Loss: 0.00719032 Val Loss: 0.00683018\n",
      "Epoch:  23 Train Loss: 0.00731978 Val Loss: 0.00690656\n",
      "Epoch:  24 Train Loss: 0.00656617 Val Loss: 0.00683825\n",
      "Epoch:  25 Train Loss: 0.00715223 Val Loss: 0.00673793\n",
      "Epoch:  26 Train Loss: 0.00702188 Val Loss: 0.00667295\n",
      "Epoch:  27 Train Loss: 0.00833617 Val Loss: 0.00672300\n",
      "Epoch:  28 Train Loss: 0.00627383 Val Loss: 0.00658274\n",
      "Epoch:  29 Train Loss: 0.00819805 Val Loss: 0.00741359\n",
      "Epoch:  30 Train Loss: 0.00690730 Val Loss: 0.00684559\n",
      "Epoch:  31 Train Loss: 0.00683444 Val Loss: 0.00660762\n",
      "Epoch:  32 Train Loss: 0.00746351 Val Loss: 0.00727947\n",
      "Epoch:  33 Train Loss: 0.00639396 Val Loss: 0.00655370\n",
      "Epoch:  34 Train Loss: 0.00844553 Val Loss: 0.00932844\n",
      "Epoch:  35 Train Loss: 0.00785646 Val Loss: 0.00776689\n",
      "Epoch:  36 Train Loss: 0.00907732 Val Loss: 0.00767184\n",
      "Epoch:  37 Train Loss: 0.00782830 Val Loss: 0.00656595\n",
      "Epoch:  38 Train Loss: 0.00765407 Val Loss: 0.00681867\n",
      "Epoch:  39 Train Loss: 0.00815713 Val Loss: 0.01081151\n",
      "Epoch:  40 Train Loss: 0.00889256 Val Loss: 0.00716319\n",
      "Epoch:  41 Train Loss: 0.00780327 Val Loss: 0.00741742\n",
      "Epoch:  42 Train Loss: 0.00705778 Val Loss: 0.00728640\n",
      "Epoch:  43 Train Loss: 0.00881668 Val Loss: 0.00684910\n",
      "Epoch:  44 Train Loss: 0.00623133 Val Loss: 0.00694905\n",
      "Epoch:  45 Train Loss: 0.00821155 Val Loss: 0.00676639\n",
      "Epoch:  46 Train Loss: 0.00773930 Val Loss: 0.00646151\n",
      "Epoch:  47 Train Loss: 0.00710747 Val Loss: 0.00655360\n",
      "Epoch:  48 Train Loss: 0.00695364 Val Loss: 0.00691028\n",
      "Epoch:  49 Train Loss: 0.00665991 Val Loss: 0.00643155\n",
      "Epoch:  50 Train Loss: 0.00757817 Val Loss: 0.00643900\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.22810120 Val Loss: 0.14508494\n",
      "Epoch:   2 Train Loss: 0.08339816 Val Loss: 0.02405095\n",
      "Epoch:   3 Train Loss: 0.04324816 Val Loss: 0.02315264\n",
      "Epoch:   4 Train Loss: 0.03251963 Val Loss: 0.03281432\n",
      "Epoch:   5 Train Loss: 0.03537870 Val Loss: 0.02213559\n",
      "Epoch:   6 Train Loss: 0.02848995 Val Loss: 0.01859804\n",
      "Epoch:   7 Train Loss: 0.02337056 Val Loss: 0.01671139\n",
      "Epoch:   8 Train Loss: 0.01698996 Val Loss: 0.01105730\n",
      "Epoch:   9 Train Loss: 0.01385232 Val Loss: 0.01041338\n",
      "Epoch:  10 Train Loss: 0.00791110 Val Loss: 0.00747666\n",
      "Epoch:  11 Train Loss: 0.00994642 Val Loss: 0.00756848\n",
      "Epoch:  12 Train Loss: 0.01113440 Val Loss: 0.01534482\n",
      "Epoch:  13 Train Loss: 0.01126797 Val Loss: 0.00751938\n",
      "Epoch:  14 Train Loss: 0.01194785 Val Loss: 0.01081730\n",
      "Epoch:  15 Train Loss: 0.00884443 Val Loss: 0.00777537\n",
      "Epoch:  16 Train Loss: 0.00951399 Val Loss: 0.00843118\n",
      "Epoch:  17 Train Loss: 0.00743963 Val Loss: 0.00719510\n",
      "Epoch:  18 Train Loss: 0.00863560 Val Loss: 0.00779323\n",
      "Epoch:  19 Train Loss: 0.00900786 Val Loss: 0.01074192\n",
      "Epoch:  20 Train Loss: 0.01001286 Val Loss: 0.00798533\n",
      "Epoch:  21 Train Loss: 0.00764485 Val Loss: 0.00782358\n",
      "Epoch:  22 Train Loss: 0.00995582 Val Loss: 0.00777682\n",
      "Epoch:  23 Train Loss: 0.00789198 Val Loss: 0.00699082\n",
      "Epoch:  24 Train Loss: 0.00929532 Val Loss: 0.00784223\n",
      "Epoch:  25 Train Loss: 0.00832583 Val Loss: 0.00852512\n",
      "Epoch:  26 Train Loss: 0.00794643 Val Loss: 0.00705556\n",
      "Epoch:  27 Train Loss: 0.00703303 Val Loss: 0.00827882\n",
      "Epoch:  28 Train Loss: 0.00709970 Val Loss: 0.00711234\n",
      "Epoch:  29 Train Loss: 0.00976330 Val Loss: 0.00689924\n",
      "Epoch:  30 Train Loss: 0.00783498 Val Loss: 0.00742810\n",
      "Epoch:  31 Train Loss: 0.00672835 Val Loss: 0.00684897\n",
      "Epoch:  32 Train Loss: 0.01020879 Val Loss: 0.00748377\n",
      "Epoch:  33 Train Loss: 0.00812697 Val Loss: 0.00795575\n",
      "Epoch:  34 Train Loss: 0.00874730 Val Loss: 0.00715306\n",
      "Epoch:  35 Train Loss: 0.00779972 Val Loss: 0.00854878\n",
      "Epoch:  36 Train Loss: 0.00749814 Val Loss: 0.00701698\n",
      "Epoch:  37 Train Loss: 0.00880126 Val Loss: 0.00700728\n",
      "Epoch:  38 Train Loss: 0.00765846 Val Loss: 0.00671504\n",
      "Epoch:  39 Train Loss: 0.00789102 Val Loss: 0.00673725\n",
      "Epoch:  40 Train Loss: 0.00744767 Val Loss: 0.00687770\n",
      "Epoch:  41 Train Loss: 0.00638199 Val Loss: 0.00680157\n",
      "Epoch:  42 Train Loss: 0.00760385 Val Loss: 0.00668715\n",
      "Epoch:  43 Train Loss: 0.00751839 Val Loss: 0.00668142\n",
      "Epoch:  44 Train Loss: 0.00804711 Val Loss: 0.00691255\n",
      "Epoch:  45 Train Loss: 0.00739889 Val Loss: 0.00667651\n",
      "Epoch:  46 Train Loss: 0.00705384 Val Loss: 0.00711458\n",
      "Epoch:  47 Train Loss: 0.00720973 Val Loss: 0.00671345\n",
      "Epoch:  48 Train Loss: 0.00808808 Val Loss: 0.00842898\n",
      "Epoch:  49 Train Loss: 0.00918961 Val Loss: 0.00896720\n",
      "Epoch:  50 Train Loss: 0.00990687 Val Loss: 0.00837429\n",
      "Epoch:  51 Train Loss: 0.00770552 Val Loss: 0.00680248\n",
      "Epoch:  52 Train Loss: 0.00693802 Val Loss: 0.00872774\n",
      "Epoch:  53 Train Loss: 0.00887229 Val Loss: 0.00666395\n",
      "Epoch:  54 Train Loss: 0.00657027 Val Loss: 0.00670229\n",
      "Epoch:  55 Train Loss: 0.00818662 Val Loss: 0.00706073\n",
      "Epoch:  56 Train Loss: 0.00808357 Val Loss: 0.00659524\n",
      "Epoch:  57 Train Loss: 0.00712237 Val Loss: 0.00696770\n",
      "Epoch:  58 Train Loss: 0.00723850 Val Loss: 0.00665073\n",
      "Epoch:  59 Train Loss: 0.00760326 Val Loss: 0.00775831\n",
      "Epoch:  60 Train Loss: 0.00693579 Val Loss: 0.00659868\n",
      "Epoch:  61 Train Loss: 0.00614109 Val Loss: 0.00686370\n",
      "Epoch:  62 Train Loss: 0.00800858 Val Loss: 0.00655504\n",
      "Epoch:  63 Train Loss: 0.00727053 Val Loss: 0.00658537\n",
      "Epoch:  64 Train Loss: 0.00809835 Val Loss: 0.00709500\n",
      "Epoch:  65 Train Loss: 0.00689780 Val Loss: 0.00701019\n",
      "Epoch:  66 Train Loss: 0.00936435 Val Loss: 0.00683069\n",
      "Epoch:  67 Train Loss: 0.00660907 Val Loss: 0.00704598\n",
      "Epoch:  68 Train Loss: 0.00658499 Val Loss: 0.00682756\n",
      "Epoch:  69 Train Loss: 0.00881289 Val Loss: 0.00696918\n",
      "Epoch:  70 Train Loss: 0.00810305 Val Loss: 0.00679229\n",
      "Epoch:  71 Train Loss: 0.00760985 Val Loss: 0.00658761\n",
      "Epoch:  72 Train Loss: 0.00750866 Val Loss: 0.00731273\n",
      "Epoch:  73 Train Loss: 0.00736255 Val Loss: 0.00649755\n",
      "Epoch:  74 Train Loss: 0.00607270 Val Loss: 0.00684420\n",
      "Epoch:  75 Train Loss: 0.00676729 Val Loss: 0.00679444\n",
      "Epoch:  76 Train Loss: 0.00746250 Val Loss: 0.00717075\n",
      "Epoch:  77 Train Loss: 0.00726834 Val Loss: 0.00661970\n",
      "Epoch:  78 Train Loss: 0.00693513 Val Loss: 0.00658049\n",
      "Epoch:  79 Train Loss: 0.00709115 Val Loss: 0.00683917\n",
      "Epoch:  80 Train Loss: 0.00666569 Val Loss: 0.00659662\n",
      "Epoch:  81 Train Loss: 0.00813545 Val Loss: 0.00645856\n",
      "Epoch:  82 Train Loss: 0.00805645 Val Loss: 0.00863016\n",
      "Epoch:  83 Train Loss: 0.00756057 Val Loss: 0.00708877\n",
      "Epoch:  84 Train Loss: 0.00723549 Val Loss: 0.00723003\n",
      "Epoch:  85 Train Loss: 0.00632616 Val Loss: 0.00656961\n",
      "Epoch:  86 Train Loss: 0.00629568 Val Loss: 0.00695672\n",
      "Epoch:  87 Train Loss: 0.00652342 Val Loss: 0.00668534\n",
      "Epoch:  88 Train Loss: 0.00807174 Val Loss: 0.00640215\n",
      "Epoch:  89 Train Loss: 0.00757756 Val Loss: 0.00665835\n",
      "Epoch:  90 Train Loss: 0.00714028 Val Loss: 0.00759414\n",
      "Epoch:  91 Train Loss: 0.00737232 Val Loss: 0.00663105\n",
      "Epoch:  92 Train Loss: 0.00773003 Val Loss: 0.00631246\n",
      "Epoch:  93 Train Loss: 0.00733901 Val Loss: 0.00734944\n",
      "Epoch:  94 Train Loss: 0.00798309 Val Loss: 0.00644724\n",
      "Epoch:  95 Train Loss: 0.00619056 Val Loss: 0.00676989\n",
      "Epoch:  96 Train Loss: 0.00833522 Val Loss: 0.00636219\n",
      "Epoch:  97 Train Loss: 0.00638618 Val Loss: 0.00680007\n",
      "Epoch:  98 Train Loss: 0.00584048 Val Loss: 0.00635775\n",
      "Epoch:  99 Train Loss: 0.00674747 Val Loss: 0.00632030\n",
      "Epoch: 100 Train Loss: 0.00715556 Val Loss: 0.00788301\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.08129105 Val Loss: 0.03145727\n",
      "Epoch:   2 Train Loss: 0.03611580 Val Loss: 0.02104026\n",
      "Epoch:   3 Train Loss: 0.02620604 Val Loss: 0.01346132\n",
      "Epoch:   4 Train Loss: 0.01273887 Val Loss: 0.02075080\n",
      "Epoch:   5 Train Loss: 0.01660484 Val Loss: 0.01749902\n",
      "Epoch:   6 Train Loss: 0.01101367 Val Loss: 0.00999456\n",
      "Epoch:   7 Train Loss: 0.00920474 Val Loss: 0.00998610\n",
      "Epoch:   8 Train Loss: 0.00961668 Val Loss: 0.00975762\n",
      "Epoch:   9 Train Loss: 0.01000579 Val Loss: 0.00990290\n",
      "Epoch:  10 Train Loss: 0.01120558 Val Loss: 0.01175215\n",
      "Epoch:  11 Train Loss: 0.01005915 Val Loss: 0.00931893\n",
      "Epoch:  12 Train Loss: 0.00915360 Val Loss: 0.00890217\n",
      "Epoch:  13 Train Loss: 0.00981612 Val Loss: 0.01089839\n",
      "Epoch:  14 Train Loss: 0.01129820 Val Loss: 0.00905253\n",
      "Epoch:  15 Train Loss: 0.00961750 Val Loss: 0.01102015\n",
      "Epoch:  16 Train Loss: 0.01004716 Val Loss: 0.01023591\n",
      "Epoch:  17 Train Loss: 0.00986274 Val Loss: 0.00970012\n",
      "Epoch:  18 Train Loss: 0.00940357 Val Loss: 0.00880442\n",
      "Epoch:  19 Train Loss: 0.00906133 Val Loss: 0.00969958\n",
      "Epoch:  20 Train Loss: 0.01004765 Val Loss: 0.00905852\n",
      "Epoch:  21 Train Loss: 0.01016105 Val Loss: 0.01030628\n",
      "Epoch:  22 Train Loss: 0.01067218 Val Loss: 0.01054605\n",
      "Epoch:  23 Train Loss: 0.00983954 Val Loss: 0.00835370\n",
      "Epoch:  24 Train Loss: 0.00797752 Val Loss: 0.00827120\n",
      "Epoch:  25 Train Loss: 0.00860519 Val Loss: 0.01210426\n",
      "Epoch:  26 Train Loss: 0.01086326 Val Loss: 0.01317068\n",
      "Epoch:  27 Train Loss: 0.01457721 Val Loss: 0.01339949\n",
      "Epoch:  28 Train Loss: 0.01094356 Val Loss: 0.01046596\n",
      "Epoch:  29 Train Loss: 0.01077026 Val Loss: 0.01006508\n",
      "Epoch:  30 Train Loss: 0.01066760 Val Loss: 0.01030470\n",
      "Epoch:  31 Train Loss: 0.00981039 Val Loss: 0.01063635\n",
      "Epoch:  32 Train Loss: 0.01083597 Val Loss: 0.00978643\n",
      "Epoch:  33 Train Loss: 0.00893172 Val Loss: 0.00956187\n",
      "Epoch:  34 Train Loss: 0.01126688 Val Loss: 0.01101833\n",
      "Epoch:  35 Train Loss: 0.00902029 Val Loss: 0.00844143\n",
      "Epoch:  36 Train Loss: 0.00799700 Val Loss: 0.00932886\n",
      "Epoch:  37 Train Loss: 0.00922022 Val Loss: 0.00818351\n",
      "Epoch:  38 Train Loss: 0.00842566 Val Loss: 0.01146574\n",
      "Epoch:  39 Train Loss: 0.01147907 Val Loss: 0.00812716\n",
      "Epoch:  40 Train Loss: 0.00934590 Val Loss: 0.00828034\n",
      "Epoch:  41 Train Loss: 0.00830734 Val Loss: 0.00833418\n",
      "Epoch:  42 Train Loss: 0.00743065 Val Loss: 0.00992238\n",
      "Epoch:  43 Train Loss: 0.00812477 Val Loss: 0.01160041\n",
      "Epoch:  44 Train Loss: 0.00820353 Val Loss: 0.00868217\n",
      "Epoch:  45 Train Loss: 0.01210604 Val Loss: 0.01186888\n",
      "Epoch:  46 Train Loss: 0.01155505 Val Loss: 0.01265342\n",
      "Epoch:  47 Train Loss: 0.01290528 Val Loss: 0.01053975\n",
      "Epoch:  48 Train Loss: 0.00964020 Val Loss: 0.00796285\n",
      "Epoch:  49 Train Loss: 0.00803373 Val Loss: 0.00816565\n",
      "Epoch:  50 Train Loss: 0.00796356 Val Loss: 0.00826687\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.28756420 Val Loss: 0.15469509\n",
      "Epoch:   2 Train Loss: 0.10235190 Val Loss: 0.02602949\n",
      "Epoch:   3 Train Loss: 0.05165254 Val Loss: 0.02240910\n",
      "Epoch:   4 Train Loss: 0.03728360 Val Loss: 0.02400899\n",
      "Epoch:   5 Train Loss: 0.02430047 Val Loss: 0.01787081\n",
      "Epoch:   6 Train Loss: 0.02067073 Val Loss: 0.01426175\n",
      "Epoch:   7 Train Loss: 0.01476105 Val Loss: 0.01152610\n",
      "Epoch:   8 Train Loss: 0.01143078 Val Loss: 0.01144558\n",
      "Epoch:   9 Train Loss: 0.01572850 Val Loss: 0.01183331\n",
      "Epoch:  10 Train Loss: 0.01342562 Val Loss: 0.01113237\n",
      "Epoch:  11 Train Loss: 0.01213545 Val Loss: 0.01123558\n",
      "Epoch:  12 Train Loss: 0.01550488 Val Loss: 0.01047321\n",
      "Epoch:  13 Train Loss: 0.00992442 Val Loss: 0.00955844\n",
      "Epoch:  14 Train Loss: 0.00930059 Val Loss: 0.00943997\n",
      "Epoch:  15 Train Loss: 0.01042763 Val Loss: 0.00896756\n",
      "Epoch:  16 Train Loss: 0.00942705 Val Loss: 0.00928543\n",
      "Epoch:  17 Train Loss: 0.00961869 Val Loss: 0.01049284\n",
      "Epoch:  18 Train Loss: 0.01169466 Val Loss: 0.00965793\n",
      "Epoch:  19 Train Loss: 0.00805522 Val Loss: 0.00811219\n",
      "Epoch:  20 Train Loss: 0.00821321 Val Loss: 0.00844944\n",
      "Epoch:  21 Train Loss: 0.00880548 Val Loss: 0.00784570\n",
      "Epoch:  22 Train Loss: 0.00820403 Val Loss: 0.00884170\n",
      "Epoch:  23 Train Loss: 0.00968554 Val Loss: 0.00781655\n",
      "Epoch:  24 Train Loss: 0.00843635 Val Loss: 0.01374388\n",
      "Epoch:  25 Train Loss: 0.01294948 Val Loss: 0.01428432\n",
      "Epoch:  26 Train Loss: 0.01930600 Val Loss: 0.01627829\n",
      "Epoch:  27 Train Loss: 0.01403505 Val Loss: 0.01554605\n",
      "Epoch:  28 Train Loss: 0.01258806 Val Loss: 0.00912637\n",
      "Epoch:  29 Train Loss: 0.01093159 Val Loss: 0.00912843\n",
      "Epoch:  30 Train Loss: 0.00995014 Val Loss: 0.01173954\n",
      "Epoch:  31 Train Loss: 0.01268039 Val Loss: 0.00999391\n",
      "Epoch:  32 Train Loss: 0.00939522 Val Loss: 0.00856485\n",
      "Epoch:  33 Train Loss: 0.00926109 Val Loss: 0.00891011\n",
      "Epoch:  34 Train Loss: 0.00876350 Val Loss: 0.00816881\n",
      "Epoch:  35 Train Loss: 0.00975309 Val Loss: 0.01120764\n",
      "Epoch:  36 Train Loss: 0.01008224 Val Loss: 0.01398747\n",
      "Epoch:  37 Train Loss: 0.01063196 Val Loss: 0.00958658\n",
      "Epoch:  38 Train Loss: 0.00971234 Val Loss: 0.00791472\n",
      "Epoch:  39 Train Loss: 0.00906234 Val Loss: 0.00787785\n",
      "Epoch:  40 Train Loss: 0.00824323 Val Loss: 0.00787839\n",
      "Epoch:  41 Train Loss: 0.00736001 Val Loss: 0.00832214\n",
      "Epoch:  42 Train Loss: 0.00825617 Val Loss: 0.01001449\n",
      "Epoch:  43 Train Loss: 0.00952902 Val Loss: 0.00775627\n",
      "Epoch:  44 Train Loss: 0.00957679 Val Loss: 0.00834408\n",
      "Epoch:  45 Train Loss: 0.00970773 Val Loss: 0.00934999\n",
      "Epoch:  46 Train Loss: 0.01075547 Val Loss: 0.00945892\n",
      "Epoch:  47 Train Loss: 0.00927740 Val Loss: 0.00790644\n",
      "Epoch:  48 Train Loss: 0.00916303 Val Loss: 0.00805406\n",
      "Epoch:  49 Train Loss: 0.00798069 Val Loss: 0.00819196\n",
      "Epoch:  50 Train Loss: 0.00895643 Val Loss: 0.00800511\n",
      "Epoch:  51 Train Loss: 0.00773138 Val Loss: 0.00899394\n",
      "Epoch:  52 Train Loss: 0.00694018 Val Loss: 0.00853705\n",
      "Epoch:  53 Train Loss: 0.01097165 Val Loss: 0.00747773\n",
      "Epoch:  54 Train Loss: 0.00731257 Val Loss: 0.00842445\n",
      "Epoch:  55 Train Loss: 0.00844663 Val Loss: 0.00904403\n",
      "Epoch:  56 Train Loss: 0.00839241 Val Loss: 0.01035461\n",
      "Epoch:  57 Train Loss: 0.01287587 Val Loss: 0.01319202\n",
      "Epoch:  58 Train Loss: 0.00969597 Val Loss: 0.00966015\n",
      "Epoch:  59 Train Loss: 0.00867598 Val Loss: 0.00914847\n",
      "Epoch:  60 Train Loss: 0.00848621 Val Loss: 0.00828658\n",
      "Epoch:  61 Train Loss: 0.00700216 Val Loss: 0.00941931\n",
      "Epoch:  62 Train Loss: 0.00752817 Val Loss: 0.00773488\n",
      "Epoch:  63 Train Loss: 0.00745037 Val Loss: 0.00782533\n",
      "Epoch:  64 Train Loss: 0.00787400 Val Loss: 0.00905109\n",
      "Epoch:  65 Train Loss: 0.01040889 Val Loss: 0.00736299\n",
      "Epoch:  66 Train Loss: 0.01011284 Val Loss: 0.00720871\n",
      "Epoch:  67 Train Loss: 0.00815632 Val Loss: 0.00754528\n",
      "Epoch:  68 Train Loss: 0.00855327 Val Loss: 0.00924033\n",
      "Epoch:  69 Train Loss: 0.00947403 Val Loss: 0.00756245\n",
      "Epoch:  70 Train Loss: 0.00869631 Val Loss: 0.00940856\n",
      "Epoch:  71 Train Loss: 0.00867136 Val Loss: 0.00758128\n",
      "Epoch:  72 Train Loss: 0.00694054 Val Loss: 0.00682915\n",
      "Epoch:  73 Train Loss: 0.00681249 Val Loss: 0.00670930\n",
      "Epoch:  74 Train Loss: 0.00695144 Val Loss: 0.00687420\n",
      "Epoch:  75 Train Loss: 0.00822817 Val Loss: 0.00916368\n",
      "Epoch:  76 Train Loss: 0.00979125 Val Loss: 0.00688416\n",
      "Epoch:  77 Train Loss: 0.01166069 Val Loss: 0.00848261\n",
      "Epoch:  78 Train Loss: 0.00931306 Val Loss: 0.00734813\n",
      "Epoch:  79 Train Loss: 0.00743190 Val Loss: 0.00680724\n",
      "Epoch:  80 Train Loss: 0.00625756 Val Loss: 0.00772680\n",
      "Epoch:  81 Train Loss: 0.00738873 Val Loss: 0.01108672\n",
      "Epoch:  82 Train Loss: 0.00877647 Val Loss: 0.00659789\n",
      "Epoch:  83 Train Loss: 0.00598469 Val Loss: 0.00679745\n",
      "Epoch:  84 Train Loss: 0.00712595 Val Loss: 0.00716148\n",
      "Epoch:  85 Train Loss: 0.00804487 Val Loss: 0.00871198\n",
      "Epoch:  86 Train Loss: 0.00971638 Val Loss: 0.00878224\n",
      "Epoch:  87 Train Loss: 0.00742007 Val Loss: 0.00968355\n",
      "Epoch:  88 Train Loss: 0.00849700 Val Loss: 0.01124860\n",
      "Epoch:  89 Train Loss: 0.01082425 Val Loss: 0.00639310\n",
      "Epoch:  90 Train Loss: 0.00844169 Val Loss: 0.01078842\n",
      "Epoch:  91 Train Loss: 0.00697191 Val Loss: 0.00915002\n",
      "Epoch:  92 Train Loss: 0.00683577 Val Loss: 0.00648799\n",
      "Epoch:  93 Train Loss: 0.00780984 Val Loss: 0.00686736\n",
      "Epoch:  94 Train Loss: 0.00780177 Val Loss: 0.00674718\n",
      "Epoch:  95 Train Loss: 0.00618346 Val Loss: 0.00565086\n",
      "Epoch:  96 Train Loss: 0.00540231 Val Loss: 0.00622978\n",
      "Epoch:  97 Train Loss: 0.00487863 Val Loss: 0.00605615\n",
      "Epoch:  98 Train Loss: 0.00505250 Val Loss: 0.00568232\n",
      "Epoch:  99 Train Loss: 0.00519075 Val Loss: 0.00640549\n",
      "Epoch: 100 Train Loss: 0.00562735 Val Loss: 0.00777481\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.15834415 Val Loss: 0.09505250\n",
      "Epoch:   2 Train Loss: 0.05855116 Val Loss: 0.04095823\n",
      "Epoch:   3 Train Loss: 0.05730660 Val Loss: 0.02783918\n",
      "Epoch:   4 Train Loss: 0.04525918 Val Loss: 0.03453339\n",
      "Epoch:   5 Train Loss: 0.04006049 Val Loss: 0.02456019\n",
      "Epoch:   6 Train Loss: 0.03288992 Val Loss: 0.02013655\n",
      "Epoch:   7 Train Loss: 0.02639386 Val Loss: 0.01581805\n",
      "Epoch:   8 Train Loss: 0.01682168 Val Loss: 0.01072938\n",
      "Epoch:   9 Train Loss: 0.01045322 Val Loss: 0.01054714\n",
      "Epoch:  10 Train Loss: 0.00979226 Val Loss: 0.01108212\n",
      "Epoch:  11 Train Loss: 0.01672764 Val Loss: 0.01719197\n",
      "Epoch:  12 Train Loss: 0.01012658 Val Loss: 0.01106584\n",
      "Epoch:  13 Train Loss: 0.01174838 Val Loss: 0.01306092\n",
      "Epoch:  14 Train Loss: 0.00951588 Val Loss: 0.00764131\n",
      "Epoch:  15 Train Loss: 0.01321623 Val Loss: 0.00904354\n",
      "Epoch:  16 Train Loss: 0.01046640 Val Loss: 0.00762989\n",
      "Epoch:  17 Train Loss: 0.01099189 Val Loss: 0.00878819\n",
      "Epoch:  18 Train Loss: 0.00911734 Val Loss: 0.00757804\n",
      "Epoch:  19 Train Loss: 0.00737066 Val Loss: 0.00770030\n",
      "Epoch:  20 Train Loss: 0.00824499 Val Loss: 0.00724722\n",
      "Epoch:  21 Train Loss: 0.00971903 Val Loss: 0.00777893\n",
      "Epoch:  22 Train Loss: 0.00795821 Val Loss: 0.00713957\n",
      "Epoch:  23 Train Loss: 0.00872868 Val Loss: 0.00800259\n",
      "Epoch:  24 Train Loss: 0.00807710 Val Loss: 0.00696425\n",
      "Epoch:  25 Train Loss: 0.00850514 Val Loss: 0.00711841\n",
      "Epoch:  26 Train Loss: 0.00799436 Val Loss: 0.00681823\n",
      "Epoch:  27 Train Loss: 0.00967968 Val Loss: 0.00757393\n",
      "Epoch:  28 Train Loss: 0.00847392 Val Loss: 0.00681578\n",
      "Epoch:  29 Train Loss: 0.00967603 Val Loss: 0.00811337\n",
      "Epoch:  30 Train Loss: 0.00979032 Val Loss: 0.00705427\n",
      "Epoch:  31 Train Loss: 0.00869827 Val Loss: 0.00753370\n",
      "Epoch:  32 Train Loss: 0.00882544 Val Loss: 0.00675014\n",
      "Epoch:  33 Train Loss: 0.00764344 Val Loss: 0.00725313\n",
      "Epoch:  34 Train Loss: 0.00818748 Val Loss: 0.00695708\n",
      "Epoch:  35 Train Loss: 0.00698710 Val Loss: 0.00679644\n",
      "Epoch:  36 Train Loss: 0.00810507 Val Loss: 0.00713489\n",
      "Epoch:  37 Train Loss: 0.00830922 Val Loss: 0.00669585\n",
      "Epoch:  38 Train Loss: 0.00699230 Val Loss: 0.00681062\n",
      "Epoch:  39 Train Loss: 0.00801982 Val Loss: 0.00696042\n",
      "Epoch:  40 Train Loss: 0.00738137 Val Loss: 0.00674805\n",
      "Epoch:  41 Train Loss: 0.00736442 Val Loss: 0.00682337\n",
      "Epoch:  42 Train Loss: 0.00773991 Val Loss: 0.00669977\n",
      "Epoch:  43 Train Loss: 0.00661582 Val Loss: 0.00684757\n",
      "Epoch:  44 Train Loss: 0.00732535 Val Loss: 0.00670284\n",
      "Epoch:  45 Train Loss: 0.00799336 Val Loss: 0.00692399\n",
      "Epoch:  46 Train Loss: 0.00752489 Val Loss: 0.00728284\n",
      "Epoch:  47 Train Loss: 0.00818533 Val Loss: 0.00680744\n",
      "Epoch:  48 Train Loss: 0.00711687 Val Loss: 0.00685894\n",
      "Epoch:  49 Train Loss: 0.00748225 Val Loss: 0.00683302\n",
      "Epoch:  50 Train Loss: 0.00743914 Val Loss: 0.00671033\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.19516930 Val Loss: 0.12068092\n",
      "Epoch:   2 Train Loss: 0.07705958 Val Loss: 0.02883227\n",
      "Epoch:   3 Train Loss: 0.05487394 Val Loss: 0.02708161\n",
      "Epoch:   4 Train Loss: 0.04154156 Val Loss: 0.03482842\n",
      "Epoch:   5 Train Loss: 0.04000910 Val Loss: 0.02661514\n",
      "Epoch:   6 Train Loss: 0.03610486 Val Loss: 0.02098054\n",
      "Epoch:   7 Train Loss: 0.03092536 Val Loss: 0.01820804\n",
      "Epoch:   8 Train Loss: 0.02253684 Val Loss: 0.01420585\n",
      "Epoch:   9 Train Loss: 0.01827197 Val Loss: 0.01026470\n",
      "Epoch:  10 Train Loss: 0.01056976 Val Loss: 0.01009495\n",
      "Epoch:  11 Train Loss: 0.00979986 Val Loss: 0.00872558\n",
      "Epoch:  12 Train Loss: 0.01203443 Val Loss: 0.01124739\n",
      "Epoch:  13 Train Loss: 0.00937993 Val Loss: 0.00950793\n",
      "Epoch:  14 Train Loss: 0.00902115 Val Loss: 0.00982751\n",
      "Epoch:  15 Train Loss: 0.00823822 Val Loss: 0.00825922\n",
      "Epoch:  16 Train Loss: 0.01327650 Val Loss: 0.01663403\n",
      "Epoch:  17 Train Loss: 0.01188704 Val Loss: 0.00790606\n",
      "Epoch:  18 Train Loss: 0.01038552 Val Loss: 0.00926902\n",
      "Epoch:  19 Train Loss: 0.00891607 Val Loss: 0.00744082\n",
      "Epoch:  20 Train Loss: 0.00947710 Val Loss: 0.00741719\n",
      "Epoch:  21 Train Loss: 0.00977570 Val Loss: 0.01081189\n",
      "Epoch:  22 Train Loss: 0.01038763 Val Loss: 0.00750846\n",
      "Epoch:  23 Train Loss: 0.00941721 Val Loss: 0.01205904\n",
      "Epoch:  24 Train Loss: 0.01080253 Val Loss: 0.00735593\n",
      "Epoch:  25 Train Loss: 0.00953299 Val Loss: 0.00916310\n",
      "Epoch:  26 Train Loss: 0.00941384 Val Loss: 0.00716489\n",
      "Epoch:  27 Train Loss: 0.00794912 Val Loss: 0.00789947\n",
      "Epoch:  28 Train Loss: 0.00809556 Val Loss: 0.00690808\n",
      "Epoch:  29 Train Loss: 0.00749368 Val Loss: 0.00781499\n",
      "Epoch:  30 Train Loss: 0.00732263 Val Loss: 0.00695003\n",
      "Epoch:  31 Train Loss: 0.00898224 Val Loss: 0.00907889\n",
      "Epoch:  32 Train Loss: 0.00964635 Val Loss: 0.00701846\n",
      "Epoch:  33 Train Loss: 0.00814119 Val Loss: 0.00693833\n",
      "Epoch:  34 Train Loss: 0.00773420 Val Loss: 0.00762064\n",
      "Epoch:  35 Train Loss: 0.00771962 Val Loss: 0.00677328\n",
      "Epoch:  36 Train Loss: 0.00708566 Val Loss: 0.00679423\n",
      "Epoch:  37 Train Loss: 0.00948962 Val Loss: 0.00677567\n",
      "Epoch:  38 Train Loss: 0.00870495 Val Loss: 0.00695583\n",
      "Epoch:  39 Train Loss: 0.00753463 Val Loss: 0.00671369\n",
      "Epoch:  40 Train Loss: 0.00728062 Val Loss: 0.00690555\n",
      "Epoch:  41 Train Loss: 0.00739729 Val Loss: 0.00741986\n",
      "Epoch:  42 Train Loss: 0.00630690 Val Loss: 0.00691134\n",
      "Epoch:  43 Train Loss: 0.01066152 Val Loss: 0.00744768\n",
      "Epoch:  44 Train Loss: 0.00946008 Val Loss: 0.00714724\n",
      "Epoch:  45 Train Loss: 0.00970507 Val Loss: 0.00843416\n",
      "Epoch:  46 Train Loss: 0.00831059 Val Loss: 0.00691063\n",
      "Epoch:  47 Train Loss: 0.00835773 Val Loss: 0.00700429\n",
      "Epoch:  48 Train Loss: 0.00724717 Val Loss: 0.00700515\n",
      "Epoch:  49 Train Loss: 0.00686646 Val Loss: 0.00678617\n",
      "Epoch:  50 Train Loss: 0.00798789 Val Loss: 0.00689351\n",
      "Epoch:  51 Train Loss: 0.00802821 Val Loss: 0.00668770\n",
      "Epoch:  52 Train Loss: 0.00906739 Val Loss: 0.00664086\n",
      "Epoch:  53 Train Loss: 0.00669725 Val Loss: 0.00676080\n",
      "Epoch:  54 Train Loss: 0.00801755 Val Loss: 0.00705250\n",
      "Epoch:  55 Train Loss: 0.00713828 Val Loss: 0.00667353\n",
      "Epoch:  56 Train Loss: 0.00733144 Val Loss: 0.00686122\n",
      "Epoch:  57 Train Loss: 0.00748958 Val Loss: 0.00672817\n",
      "Epoch:  58 Train Loss: 0.00704822 Val Loss: 0.00829001\n",
      "Epoch:  59 Train Loss: 0.00760840 Val Loss: 0.00685113\n",
      "Epoch:  60 Train Loss: 0.00954377 Val Loss: 0.00677629\n",
      "Epoch:  61 Train Loss: 0.00804844 Val Loss: 0.01073375\n",
      "Epoch:  62 Train Loss: 0.01050274 Val Loss: 0.00788694\n",
      "Epoch:  63 Train Loss: 0.00906516 Val Loss: 0.00788509\n",
      "Epoch:  64 Train Loss: 0.00816256 Val Loss: 0.00661757\n",
      "Epoch:  65 Train Loss: 0.00746209 Val Loss: 0.00663746\n",
      "Epoch:  66 Train Loss: 0.00665029 Val Loss: 0.00753236\n",
      "Epoch:  67 Train Loss: 0.00663337 Val Loss: 0.00659429\n",
      "Epoch:  68 Train Loss: 0.00743234 Val Loss: 0.00689465\n",
      "Epoch:  69 Train Loss: 0.00685179 Val Loss: 0.00673155\n",
      "Epoch:  70 Train Loss: 0.00748211 Val Loss: 0.00670769\n",
      "Epoch:  71 Train Loss: 0.00673863 Val Loss: 0.00652098\n",
      "Epoch:  72 Train Loss: 0.00679574 Val Loss: 0.00661094\n",
      "Epoch:  73 Train Loss: 0.00757505 Val Loss: 0.00772043\n",
      "Epoch:  74 Train Loss: 0.00850667 Val Loss: 0.00669000\n",
      "Epoch:  75 Train Loss: 0.00823001 Val Loss: 0.00654494\n",
      "Epoch:  76 Train Loss: 0.00691107 Val Loss: 0.00670222\n",
      "Epoch:  77 Train Loss: 0.00722417 Val Loss: 0.00653557\n",
      "Epoch:  78 Train Loss: 0.00716663 Val Loss: 0.00790754\n",
      "Epoch:  79 Train Loss: 0.00791177 Val Loss: 0.00670615\n",
      "Epoch:  80 Train Loss: 0.00698310 Val Loss: 0.00675184\n",
      "Epoch:  81 Train Loss: 0.00723326 Val Loss: 0.00646998\n",
      "Epoch:  82 Train Loss: 0.00674909 Val Loss: 0.00686762\n",
      "Epoch:  83 Train Loss: 0.00638833 Val Loss: 0.00654813\n",
      "Epoch:  84 Train Loss: 0.00812285 Val Loss: 0.00661010\n",
      "Epoch:  85 Train Loss: 0.00763075 Val Loss: 0.00647944\n",
      "Epoch:  86 Train Loss: 0.00616887 Val Loss: 0.00642161\n",
      "Epoch:  87 Train Loss: 0.00898616 Val Loss: 0.00650030\n",
      "Epoch:  88 Train Loss: 0.00671695 Val Loss: 0.00652086\n",
      "Epoch:  89 Train Loss: 0.00697730 Val Loss: 0.00641641\n",
      "Epoch:  90 Train Loss: 0.00721059 Val Loss: 0.00650687\n",
      "Epoch:  91 Train Loss: 0.00835745 Val Loss: 0.00727159\n",
      "Epoch:  92 Train Loss: 0.00779716 Val Loss: 0.00644609\n",
      "Epoch:  93 Train Loss: 0.00746528 Val Loss: 0.00645681\n",
      "Epoch:  94 Train Loss: 0.00840198 Val Loss: 0.00720885\n",
      "Epoch:  95 Train Loss: 0.00616177 Val Loss: 0.00654795\n",
      "Epoch:  96 Train Loss: 0.00752799 Val Loss: 0.00852522\n",
      "Epoch:  97 Train Loss: 0.00941803 Val Loss: 0.00684499\n",
      "Epoch:  98 Train Loss: 0.00629693 Val Loss: 0.00669764\n",
      "Epoch:  99 Train Loss: 0.00678986 Val Loss: 0.00634321\n",
      "Epoch: 100 Train Loss: 0.00760359 Val Loss: 0.00668637\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.14024340 Val Loss: 0.08845161\n",
      "Epoch:   2 Train Loss: 0.05624562 Val Loss: 0.04363570\n",
      "Epoch:   3 Train Loss: 0.04269821 Val Loss: 0.02624182\n",
      "Epoch:   4 Train Loss: 0.03008193 Val Loss: 0.01723301\n",
      "Epoch:   5 Train Loss: 0.02115736 Val Loss: 0.01248143\n",
      "Epoch:   6 Train Loss: 0.01216646 Val Loss: 0.01054748\n",
      "Epoch:   7 Train Loss: 0.01304010 Val Loss: 0.01028841\n",
      "Epoch:   8 Train Loss: 0.01212153 Val Loss: 0.01122379\n",
      "Epoch:   9 Train Loss: 0.01315198 Val Loss: 0.01041740\n",
      "Epoch:  10 Train Loss: 0.01250041 Val Loss: 0.01084084\n",
      "Epoch:  11 Train Loss: 0.01153337 Val Loss: 0.01506173\n",
      "Epoch:  12 Train Loss: 0.01466992 Val Loss: 0.01151843\n",
      "Epoch:  13 Train Loss: 0.01201294 Val Loss: 0.01003272\n",
      "Epoch:  14 Train Loss: 0.01214434 Val Loss: 0.00934699\n",
      "Epoch:  15 Train Loss: 0.00993960 Val Loss: 0.01312144\n",
      "Epoch:  16 Train Loss: 0.01077514 Val Loss: 0.00975917\n",
      "Epoch:  17 Train Loss: 0.00998241 Val Loss: 0.00877425\n",
      "Epoch:  18 Train Loss: 0.00931131 Val Loss: 0.00956562\n",
      "Epoch:  19 Train Loss: 0.00987113 Val Loss: 0.00957672\n",
      "Epoch:  20 Train Loss: 0.00936510 Val Loss: 0.01332699\n",
      "Epoch:  21 Train Loss: 0.01415517 Val Loss: 0.00890141\n",
      "Epoch:  22 Train Loss: 0.01070272 Val Loss: 0.00915673\n",
      "Epoch:  23 Train Loss: 0.01260877 Val Loss: 0.00904415\n",
      "Epoch:  24 Train Loss: 0.00922454 Val Loss: 0.00907978\n",
      "Epoch:  25 Train Loss: 0.01020913 Val Loss: 0.00844710\n",
      "Epoch:  26 Train Loss: 0.00922310 Val Loss: 0.00968224\n",
      "Epoch:  27 Train Loss: 0.01361406 Val Loss: 0.01297655\n",
      "Epoch:  28 Train Loss: 0.01297081 Val Loss: 0.01109595\n",
      "Epoch:  29 Train Loss: 0.01289990 Val Loss: 0.01262733\n",
      "Epoch:  30 Train Loss: 0.01327890 Val Loss: 0.00937628\n",
      "Epoch:  31 Train Loss: 0.00991317 Val Loss: 0.01001461\n",
      "Epoch:  32 Train Loss: 0.00919292 Val Loss: 0.00851316\n",
      "Epoch:  33 Train Loss: 0.00832514 Val Loss: 0.00828198\n",
      "Epoch:  34 Train Loss: 0.00892070 Val Loss: 0.01033583\n",
      "Epoch:  35 Train Loss: 0.01100333 Val Loss: 0.00960438\n",
      "Epoch:  36 Train Loss: 0.00993865 Val Loss: 0.00810956\n",
      "Epoch:  37 Train Loss: 0.00876961 Val Loss: 0.01245621\n",
      "Epoch:  38 Train Loss: 0.01170775 Val Loss: 0.00919475\n",
      "Epoch:  39 Train Loss: 0.01001674 Val Loss: 0.00865285\n",
      "Epoch:  40 Train Loss: 0.00976481 Val Loss: 0.01026031\n",
      "Epoch:  41 Train Loss: 0.00891005 Val Loss: 0.01059665\n",
      "Epoch:  42 Train Loss: 0.01125157 Val Loss: 0.00812307\n",
      "Epoch:  43 Train Loss: 0.00754764 Val Loss: 0.00820875\n",
      "Epoch:  44 Train Loss: 0.00786689 Val Loss: 0.00801222\n",
      "Epoch:  45 Train Loss: 0.00941078 Val Loss: 0.00833394\n",
      "Epoch:  46 Train Loss: 0.00818035 Val Loss: 0.00783245\n",
      "Epoch:  47 Train Loss: 0.00921118 Val Loss: 0.00986489\n",
      "Epoch:  48 Train Loss: 0.00842342 Val Loss: 0.00882785\n",
      "Epoch:  49 Train Loss: 0.00870516 Val Loss: 0.00765314\n",
      "Epoch:  50 Train Loss: 0.00958587 Val Loss: 0.01046188\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.11205151 Val Loss: 0.06946871\n",
      "Epoch:   2 Train Loss: 0.05873166 Val Loss: 0.02755593\n",
      "Epoch:   3 Train Loss: 0.03089024 Val Loss: 0.02528075\n",
      "Epoch:   4 Train Loss: 0.02471658 Val Loss: 0.01477539\n",
      "Epoch:   5 Train Loss: 0.01487646 Val Loss: 0.01126202\n",
      "Epoch:   6 Train Loss: 0.01713361 Val Loss: 0.01108735\n",
      "Epoch:   7 Train Loss: 0.01222012 Val Loss: 0.01203629\n",
      "Epoch:   8 Train Loss: 0.01229623 Val Loss: 0.01134895\n",
      "Epoch:   9 Train Loss: 0.01101644 Val Loss: 0.01080658\n",
      "Epoch:  10 Train Loss: 0.01053735 Val Loss: 0.01159354\n",
      "Epoch:  11 Train Loss: 0.01369503 Val Loss: 0.01096610\n",
      "Epoch:  12 Train Loss: 0.01283468 Val Loss: 0.01057682\n",
      "Epoch:  13 Train Loss: 0.01035939 Val Loss: 0.00923262\n",
      "Epoch:  14 Train Loss: 0.01073635 Val Loss: 0.00919091\n",
      "Epoch:  15 Train Loss: 0.00978976 Val Loss: 0.00961572\n",
      "Epoch:  16 Train Loss: 0.01016639 Val Loss: 0.00911505\n",
      "Epoch:  17 Train Loss: 0.00924077 Val Loss: 0.00865893\n",
      "Epoch:  18 Train Loss: 0.01182609 Val Loss: 0.00843802\n",
      "Epoch:  19 Train Loss: 0.01059111 Val Loss: 0.01001522\n",
      "Epoch:  20 Train Loss: 0.01083408 Val Loss: 0.00829523\n",
      "Epoch:  21 Train Loss: 0.01032089 Val Loss: 0.00824820\n",
      "Epoch:  22 Train Loss: 0.00888034 Val Loss: 0.00807861\n",
      "Epoch:  23 Train Loss: 0.00865737 Val Loss: 0.01058057\n",
      "Epoch:  24 Train Loss: 0.00781309 Val Loss: 0.00774775\n",
      "Epoch:  25 Train Loss: 0.00841445 Val Loss: 0.00810680\n",
      "Epoch:  26 Train Loss: 0.00988813 Val Loss: 0.01006964\n",
      "Epoch:  27 Train Loss: 0.01045072 Val Loss: 0.01130007\n",
      "Epoch:  28 Train Loss: 0.00980497 Val Loss: 0.01433995\n",
      "Epoch:  29 Train Loss: 0.01274647 Val Loss: 0.01468252\n",
      "Epoch:  30 Train Loss: 0.01279415 Val Loss: 0.00992840\n",
      "Epoch:  31 Train Loss: 0.01095110 Val Loss: 0.00908782\n",
      "Epoch:  32 Train Loss: 0.01173750 Val Loss: 0.01240135\n",
      "Epoch:  33 Train Loss: 0.01443667 Val Loss: 0.02192500\n",
      "Epoch:  34 Train Loss: 0.01339014 Val Loss: 0.00896611\n",
      "Epoch:  35 Train Loss: 0.00894504 Val Loss: 0.00826448\n",
      "Epoch:  36 Train Loss: 0.00851663 Val Loss: 0.00937902\n",
      "Epoch:  37 Train Loss: 0.00881226 Val Loss: 0.00819066\n",
      "Epoch:  38 Train Loss: 0.00757598 Val Loss: 0.00841796\n",
      "Epoch:  39 Train Loss: 0.00898122 Val Loss: 0.00807117\n",
      "Epoch:  40 Train Loss: 0.00941001 Val Loss: 0.00808561\n",
      "Epoch:  41 Train Loss: 0.00842515 Val Loss: 0.00950016\n",
      "Epoch:  42 Train Loss: 0.00850203 Val Loss: 0.00867698\n",
      "Epoch:  43 Train Loss: 0.00998299 Val Loss: 0.00833910\n",
      "Epoch:  44 Train Loss: 0.01255781 Val Loss: 0.00850975\n",
      "Epoch:  45 Train Loss: 0.01026856 Val Loss: 0.01081006\n",
      "Epoch:  46 Train Loss: 0.01109648 Val Loss: 0.00964170\n",
      "Epoch:  47 Train Loss: 0.00998277 Val Loss: 0.00862580\n",
      "Epoch:  48 Train Loss: 0.00934236 Val Loss: 0.01415294\n",
      "Epoch:  49 Train Loss: 0.01034658 Val Loss: 0.00802596\n",
      "Epoch:  50 Train Loss: 0.00856681 Val Loss: 0.01019745\n",
      "Epoch:  51 Train Loss: 0.01010518 Val Loss: 0.00817900\n",
      "Epoch:  52 Train Loss: 0.00867136 Val Loss: 0.01229286\n",
      "Epoch:  53 Train Loss: 0.00938529 Val Loss: 0.00919195\n",
      "Epoch:  54 Train Loss: 0.01096932 Val Loss: 0.00795376\n",
      "Epoch:  55 Train Loss: 0.00779479 Val Loss: 0.00819423\n",
      "Epoch:  56 Train Loss: 0.01014250 Val Loss: 0.00969900\n",
      "Epoch:  57 Train Loss: 0.00959745 Val Loss: 0.00933203\n",
      "Epoch:  58 Train Loss: 0.00790950 Val Loss: 0.00983479\n",
      "Epoch:  59 Train Loss: 0.00823238 Val Loss: 0.00901894\n",
      "Epoch:  60 Train Loss: 0.00950610 Val Loss: 0.00757189\n",
      "Epoch:  61 Train Loss: 0.00770485 Val Loss: 0.00754595\n",
      "Epoch:  62 Train Loss: 0.00834033 Val Loss: 0.00876685\n",
      "Epoch:  63 Train Loss: 0.00765448 Val Loss: 0.00746712\n",
      "Epoch:  64 Train Loss: 0.00855671 Val Loss: 0.00749715\n",
      "Epoch:  65 Train Loss: 0.00736455 Val Loss: 0.00755659\n",
      "Epoch:  66 Train Loss: 0.00728778 Val Loss: 0.00831482\n",
      "Epoch:  67 Train Loss: 0.00880902 Val Loss: 0.00851947\n",
      "Epoch:  68 Train Loss: 0.00944847 Val Loss: 0.00773231\n",
      "Epoch:  69 Train Loss: 0.00812307 Val Loss: 0.00801552\n",
      "Epoch:  70 Train Loss: 0.00913306 Val Loss: 0.01504506\n",
      "Epoch:  71 Train Loss: 0.00964767 Val Loss: 0.00772051\n",
      "Epoch:  72 Train Loss: 0.00813304 Val Loss: 0.00817090\n",
      "Epoch:  73 Train Loss: 0.00811009 Val Loss: 0.00856288\n",
      "Epoch:  74 Train Loss: 0.00892503 Val Loss: 0.00794005\n",
      "Epoch:  75 Train Loss: 0.00820163 Val Loss: 0.00818389\n",
      "Epoch:  76 Train Loss: 0.00973187 Val Loss: 0.00876947\n",
      "Epoch:  77 Train Loss: 0.00974540 Val Loss: 0.00715012\n",
      "Epoch:  78 Train Loss: 0.00855975 Val Loss: 0.00701094\n",
      "Epoch:  79 Train Loss: 0.00840957 Val Loss: 0.00779145\n",
      "Epoch:  80 Train Loss: 0.00683051 Val Loss: 0.00751560\n",
      "Epoch:  81 Train Loss: 0.00678473 Val Loss: 0.00687590\n",
      "Epoch:  82 Train Loss: 0.00885525 Val Loss: 0.00665066\n",
      "Epoch:  83 Train Loss: 0.00816039 Val Loss: 0.01092035\n",
      "Epoch:  84 Train Loss: 0.00909681 Val Loss: 0.00982229\n",
      "Epoch:  85 Train Loss: 0.01002432 Val Loss: 0.00710275\n",
      "Epoch:  86 Train Loss: 0.00772297 Val Loss: 0.00827785\n",
      "Epoch:  87 Train Loss: 0.00752591 Val Loss: 0.00706652\n",
      "Epoch:  88 Train Loss: 0.00860168 Val Loss: 0.00970727\n",
      "Epoch:  89 Train Loss: 0.00956791 Val Loss: 0.00710592\n",
      "Epoch:  90 Train Loss: 0.01070820 Val Loss: 0.00964569\n",
      "Epoch:  91 Train Loss: 0.00892862 Val Loss: 0.00940921\n",
      "Epoch:  92 Train Loss: 0.01200999 Val Loss: 0.01386155\n",
      "Epoch:  93 Train Loss: 0.01006369 Val Loss: 0.00772651\n",
      "Epoch:  94 Train Loss: 0.00801624 Val Loss: 0.00791830\n",
      "Epoch:  95 Train Loss: 0.00709857 Val Loss: 0.00719169\n",
      "Epoch:  96 Train Loss: 0.00722448 Val Loss: 0.00691668\n",
      "Epoch:  97 Train Loss: 0.00700945 Val Loss: 0.01033364\n",
      "Epoch:  98 Train Loss: 0.01039906 Val Loss: 0.00692933\n",
      "Epoch:  99 Train Loss: 0.00763343 Val Loss: 0.00754265\n",
      "Epoch: 100 Train Loss: 0.00685439 Val Loss: 0.00754387\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.13188944 Val Loss: 0.03351927\n",
      "Epoch:   2 Train Loss: 0.06084682 Val Loss: 0.03452180\n",
      "Epoch:   3 Train Loss: 0.05063601 Val Loss: 0.03505087\n",
      "Epoch:   4 Train Loss: 0.03655146 Val Loss: 0.02227918\n",
      "Epoch:   5 Train Loss: 0.02858402 Val Loss: 0.01579587\n",
      "Epoch:   6 Train Loss: 0.01590664 Val Loss: 0.00841584\n",
      "Epoch:   7 Train Loss: 0.00834015 Val Loss: 0.01319624\n",
      "Epoch:   8 Train Loss: 0.00954753 Val Loss: 0.00823949\n",
      "Epoch:   9 Train Loss: 0.00865524 Val Loss: 0.00825504\n",
      "Epoch:  10 Train Loss: 0.00913175 Val Loss: 0.00839462\n",
      "Epoch:  11 Train Loss: 0.00881890 Val Loss: 0.00754214\n",
      "Epoch:  12 Train Loss: 0.00996259 Val Loss: 0.01117320\n",
      "Epoch:  13 Train Loss: 0.00873657 Val Loss: 0.00931543\n",
      "Epoch:  14 Train Loss: 0.01095586 Val Loss: 0.01143207\n",
      "Epoch:  15 Train Loss: 0.00797618 Val Loss: 0.00791957\n",
      "Epoch:  16 Train Loss: 0.00780422 Val Loss: 0.00833843\n",
      "Epoch:  17 Train Loss: 0.00805534 Val Loss: 0.00750628\n",
      "Epoch:  18 Train Loss: 0.00805639 Val Loss: 0.00725922\n",
      "Epoch:  19 Train Loss: 0.00716645 Val Loss: 0.01068617\n",
      "Epoch:  20 Train Loss: 0.00855422 Val Loss: 0.00716812\n",
      "Epoch:  21 Train Loss: 0.00749437 Val Loss: 0.00711504\n",
      "Epoch:  22 Train Loss: 0.00791794 Val Loss: 0.00796120\n",
      "Epoch:  23 Train Loss: 0.00864857 Val Loss: 0.01030798\n",
      "Epoch:  24 Train Loss: 0.00941797 Val Loss: 0.00957276\n",
      "Epoch:  25 Train Loss: 0.00970443 Val Loss: 0.01206048\n",
      "Epoch:  26 Train Loss: 0.00813708 Val Loss: 0.00986065\n",
      "Epoch:  27 Train Loss: 0.00680760 Val Loss: 0.00999367\n",
      "Epoch:  28 Train Loss: 0.01117296 Val Loss: 0.00736164\n",
      "Epoch:  29 Train Loss: 0.00765585 Val Loss: 0.00831454\n",
      "Epoch:  30 Train Loss: 0.00961798 Val Loss: 0.00689607\n",
      "Epoch:  31 Train Loss: 0.00797782 Val Loss: 0.00694934\n",
      "Epoch:  32 Train Loss: 0.00804361 Val Loss: 0.00750646\n",
      "Epoch:  33 Train Loss: 0.00828382 Val Loss: 0.00751912\n",
      "Epoch:  34 Train Loss: 0.00735878 Val Loss: 0.00749844\n",
      "Epoch:  35 Train Loss: 0.00736839 Val Loss: 0.00693884\n",
      "Epoch:  36 Train Loss: 0.00732339 Val Loss: 0.00675843\n",
      "Epoch:  37 Train Loss: 0.00741440 Val Loss: 0.00807826\n",
      "Epoch:  38 Train Loss: 0.00805980 Val Loss: 0.00675945\n",
      "Epoch:  39 Train Loss: 0.00775870 Val Loss: 0.00681236\n",
      "Epoch:  40 Train Loss: 0.00827723 Val Loss: 0.01017750\n",
      "Epoch:  41 Train Loss: 0.00939365 Val Loss: 0.00839749\n",
      "Epoch:  42 Train Loss: 0.00859220 Val Loss: 0.00810300\n",
      "Epoch:  43 Train Loss: 0.00635183 Val Loss: 0.00669623\n",
      "Epoch:  44 Train Loss: 0.00646106 Val Loss: 0.00698807\n",
      "Epoch:  45 Train Loss: 0.00693333 Val Loss: 0.00688596\n",
      "Epoch:  46 Train Loss: 0.00702966 Val Loss: 0.00660852\n",
      "Epoch:  47 Train Loss: 0.00693816 Val Loss: 0.00666789\n",
      "Epoch:  48 Train Loss: 0.00771222 Val Loss: 0.00778254\n",
      "Epoch:  49 Train Loss: 0.00840249 Val Loss: 0.00682545\n",
      "Epoch:  50 Train Loss: 0.00721738 Val Loss: 0.00660490\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.09676139 Val Loss: 0.03901744\n",
      "Epoch:   2 Train Loss: 0.05255999 Val Loss: 0.03503380\n",
      "Epoch:   3 Train Loss: 0.04793247 Val Loss: 0.02816891\n",
      "Epoch:   4 Train Loss: 0.03438442 Val Loss: 0.01842934\n",
      "Epoch:   5 Train Loss: 0.01959193 Val Loss: 0.01327808\n",
      "Epoch:   6 Train Loss: 0.01083282 Val Loss: 0.01864787\n",
      "Epoch:   7 Train Loss: 0.01083717 Val Loss: 0.00858579\n",
      "Epoch:   8 Train Loss: 0.01130909 Val Loss: 0.01267431\n",
      "Epoch:   9 Train Loss: 0.01034354 Val Loss: 0.00783868\n",
      "Epoch:  10 Train Loss: 0.01036059 Val Loss: 0.00931205\n",
      "Epoch:  11 Train Loss: 0.00945600 Val Loss: 0.00762745\n",
      "Epoch:  12 Train Loss: 0.00803588 Val Loss: 0.00877239\n",
      "Epoch:  13 Train Loss: 0.00739641 Val Loss: 0.00725732\n",
      "Epoch:  14 Train Loss: 0.00691061 Val Loss: 0.00755672\n",
      "Epoch:  15 Train Loss: 0.00728979 Val Loss: 0.00728023\n",
      "Epoch:  16 Train Loss: 0.00772411 Val Loss: 0.00891294\n",
      "Epoch:  17 Train Loss: 0.00771499 Val Loss: 0.00717190\n",
      "Epoch:  18 Train Loss: 0.00716256 Val Loss: 0.00721294\n",
      "Epoch:  19 Train Loss: 0.00727294 Val Loss: 0.00742404\n",
      "Epoch:  20 Train Loss: 0.00710127 Val Loss: 0.00724550\n",
      "Epoch:  21 Train Loss: 0.00807461 Val Loss: 0.00917776\n",
      "Epoch:  22 Train Loss: 0.01008216 Val Loss: 0.00812447\n",
      "Epoch:  23 Train Loss: 0.00800778 Val Loss: 0.00822099\n",
      "Epoch:  24 Train Loss: 0.01073000 Val Loss: 0.00792468\n",
      "Epoch:  25 Train Loss: 0.00860835 Val Loss: 0.00754871\n",
      "Epoch:  26 Train Loss: 0.00813971 Val Loss: 0.00949638\n",
      "Epoch:  27 Train Loss: 0.00847960 Val Loss: 0.00838877\n",
      "Epoch:  28 Train Loss: 0.00974354 Val Loss: 0.00936464\n",
      "Epoch:  29 Train Loss: 0.00767558 Val Loss: 0.00856186\n",
      "Epoch:  30 Train Loss: 0.00974836 Val Loss: 0.00807221\n",
      "Epoch:  31 Train Loss: 0.00633862 Val Loss: 0.00716657\n",
      "Epoch:  32 Train Loss: 0.00760937 Val Loss: 0.00750803\n",
      "Epoch:  33 Train Loss: 0.00774901 Val Loss: 0.00679586\n",
      "Epoch:  34 Train Loss: 0.00812293 Val Loss: 0.00681230\n",
      "Epoch:  35 Train Loss: 0.00655592 Val Loss: 0.00676826\n",
      "Epoch:  36 Train Loss: 0.00703327 Val Loss: 0.00819468\n",
      "Epoch:  37 Train Loss: 0.00767506 Val Loss: 0.00710903\n",
      "Epoch:  38 Train Loss: 0.00722512 Val Loss: 0.00758218\n",
      "Epoch:  39 Train Loss: 0.00652775 Val Loss: 0.00677428\n",
      "Epoch:  40 Train Loss: 0.00631077 Val Loss: 0.00673381\n",
      "Epoch:  41 Train Loss: 0.00642171 Val Loss: 0.00698236\n",
      "Epoch:  42 Train Loss: 0.00907079 Val Loss: 0.00718948\n",
      "Epoch:  43 Train Loss: 0.00842207 Val Loss: 0.00671126\n",
      "Epoch:  44 Train Loss: 0.00621967 Val Loss: 0.00668559\n",
      "Epoch:  45 Train Loss: 0.00793937 Val Loss: 0.00688277\n",
      "Epoch:  46 Train Loss: 0.00668155 Val Loss: 0.00645820\n",
      "Epoch:  47 Train Loss: 0.00652117 Val Loss: 0.00673008\n",
      "Epoch:  48 Train Loss: 0.00665066 Val Loss: 0.00641446\n",
      "Epoch:  49 Train Loss: 0.00765274 Val Loss: 0.00645935\n",
      "Epoch:  50 Train Loss: 0.00676013 Val Loss: 0.00695630\n",
      "Epoch:  51 Train Loss: 0.00718564 Val Loss: 0.00975004\n",
      "Epoch:  52 Train Loss: 0.00842988 Val Loss: 0.00643186\n",
      "Epoch:  53 Train Loss: 0.00623288 Val Loss: 0.00638906\n",
      "Epoch:  54 Train Loss: 0.00799909 Val Loss: 0.00862960\n",
      "Epoch:  55 Train Loss: 0.00736241 Val Loss: 0.00731071\n",
      "Epoch:  56 Train Loss: 0.00613610 Val Loss: 0.00835531\n",
      "Epoch:  57 Train Loss: 0.00677777 Val Loss: 0.00693273\n",
      "Epoch:  58 Train Loss: 0.00658225 Val Loss: 0.00661933\n",
      "Epoch:  59 Train Loss: 0.00612777 Val Loss: 0.00632447\n",
      "Epoch:  60 Train Loss: 0.00719452 Val Loss: 0.00635425\n",
      "Epoch:  61 Train Loss: 0.00628085 Val Loss: 0.00769919\n",
      "Epoch:  62 Train Loss: 0.00600023 Val Loss: 0.00697709\n",
      "Epoch:  63 Train Loss: 0.00635282 Val Loss: 0.00684654\n",
      "Epoch:  64 Train Loss: 0.00617525 Val Loss: 0.00626958\n",
      "Epoch:  65 Train Loss: 0.00767390 Val Loss: 0.00627593\n",
      "Epoch:  66 Train Loss: 0.00651393 Val Loss: 0.00892474\n",
      "Epoch:  67 Train Loss: 0.00798879 Val Loss: 0.00754390\n",
      "Epoch:  68 Train Loss: 0.01013887 Val Loss: 0.00623812\n",
      "Epoch:  69 Train Loss: 0.00767950 Val Loss: 0.00981982\n",
      "Epoch:  70 Train Loss: 0.00780221 Val Loss: 0.00758426\n",
      "Epoch:  71 Train Loss: 0.00687118 Val Loss: 0.00708755\n",
      "Epoch:  72 Train Loss: 0.00616124 Val Loss: 0.00639255\n",
      "Epoch:  73 Train Loss: 0.00712464 Val Loss: 0.00657256\n",
      "Epoch:  74 Train Loss: 0.00670536 Val Loss: 0.00646922\n",
      "Epoch:  75 Train Loss: 0.00645435 Val Loss: 0.00773797\n",
      "Epoch:  76 Train Loss: 0.00641005 Val Loss: 0.00676200\n",
      "Epoch:  77 Train Loss: 0.00635942 Val Loss: 0.00869486\n",
      "Epoch:  78 Train Loss: 0.00749132 Val Loss: 0.00761449\n",
      "Epoch:  79 Train Loss: 0.00804671 Val Loss: 0.00686709\n",
      "Epoch:  80 Train Loss: 0.00709406 Val Loss: 0.00769469\n",
      "Epoch:  81 Train Loss: 0.00743887 Val Loss: 0.00792228\n",
      "Epoch:  82 Train Loss: 0.00834217 Val Loss: 0.00681106\n",
      "Epoch:  83 Train Loss: 0.00654506 Val Loss: 0.00741698\n",
      "Epoch:  84 Train Loss: 0.00556515 Val Loss: 0.00636964\n",
      "Epoch:  85 Train Loss: 0.00641527 Val Loss: 0.00653959\n",
      "Epoch:  86 Train Loss: 0.00574912 Val Loss: 0.00608926\n",
      "Epoch:  87 Train Loss: 0.00641124 Val Loss: 0.00639878\n",
      "Epoch:  88 Train Loss: 0.00613951 Val Loss: 0.00610689\n",
      "Epoch:  89 Train Loss: 0.00688595 Val Loss: 0.00797098\n",
      "Epoch:  90 Train Loss: 0.00652286 Val Loss: 0.00659271\n",
      "Epoch:  91 Train Loss: 0.00632289 Val Loss: 0.00640708\n",
      "Epoch:  92 Train Loss: 0.00707799 Val Loss: 0.00593387\n",
      "Epoch:  93 Train Loss: 0.00765301 Val Loss: 0.00611902\n",
      "Epoch:  94 Train Loss: 0.00581086 Val Loss: 0.00594504\n",
      "Epoch:  95 Train Loss: 0.00694905 Val Loss: 0.00606810\n",
      "Epoch:  96 Train Loss: 0.00710294 Val Loss: 0.00673525\n",
      "Epoch:  97 Train Loss: 0.00592844 Val Loss: 0.00679915\n",
      "Epoch:  98 Train Loss: 0.00757389 Val Loss: 0.00800275\n",
      "Epoch:  99 Train Loss: 0.00735283 Val Loss: 0.00771050\n",
      "Epoch: 100 Train Loss: 0.00829903 Val Loss: 0.00752461\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.11751534 Val Loss: 0.08634943\n",
      "Epoch:   2 Train Loss: 0.05181235 Val Loss: 0.03070643\n",
      "Epoch:   3 Train Loss: 0.02526799 Val Loss: 0.01545179\n",
      "Epoch:   4 Train Loss: 0.01190792 Val Loss: 0.01163802\n",
      "Epoch:   5 Train Loss: 0.01104337 Val Loss: 0.00980287\n",
      "Epoch:   6 Train Loss: 0.01006974 Val Loss: 0.01120710\n",
      "Epoch:   7 Train Loss: 0.01168796 Val Loss: 0.01230463\n",
      "Epoch:   8 Train Loss: 0.01401161 Val Loss: 0.01035445\n",
      "Epoch:   9 Train Loss: 0.01099907 Val Loss: 0.00995104\n",
      "Epoch:  10 Train Loss: 0.01144945 Val Loss: 0.00944190\n",
      "Epoch:  11 Train Loss: 0.00945271 Val Loss: 0.00939815\n",
      "Epoch:  12 Train Loss: 0.00927171 Val Loss: 0.00927802\n",
      "Epoch:  13 Train Loss: 0.01016826 Val Loss: 0.00855355\n",
      "Epoch:  14 Train Loss: 0.00891846 Val Loss: 0.01053768\n",
      "Epoch:  15 Train Loss: 0.01041982 Val Loss: 0.00927332\n",
      "Epoch:  16 Train Loss: 0.00812189 Val Loss: 0.00884663\n",
      "Epoch:  17 Train Loss: 0.00819257 Val Loss: 0.01580072\n",
      "Epoch:  18 Train Loss: 0.01080064 Val Loss: 0.01380888\n",
      "Epoch:  19 Train Loss: 0.01077521 Val Loss: 0.00872328\n",
      "Epoch:  20 Train Loss: 0.01273328 Val Loss: 0.00856306\n",
      "Epoch:  21 Train Loss: 0.01365061 Val Loss: 0.01143807\n",
      "Epoch:  22 Train Loss: 0.01042095 Val Loss: 0.00973827\n",
      "Epoch:  23 Train Loss: 0.00890180 Val Loss: 0.00864004\n",
      "Epoch:  24 Train Loss: 0.00929906 Val Loss: 0.00829769\n",
      "Epoch:  25 Train Loss: 0.00737906 Val Loss: 0.00833737\n",
      "Epoch:  26 Train Loss: 0.00778067 Val Loss: 0.00965541\n",
      "Epoch:  27 Train Loss: 0.00894017 Val Loss: 0.00820995\n",
      "Epoch:  28 Train Loss: 0.00754807 Val Loss: 0.00770396\n",
      "Epoch:  29 Train Loss: 0.00815676 Val Loss: 0.00806013\n",
      "Epoch:  30 Train Loss: 0.01099979 Val Loss: 0.00893237\n",
      "Epoch:  31 Train Loss: 0.00861735 Val Loss: 0.00780262\n",
      "Epoch:  32 Train Loss: 0.00865361 Val Loss: 0.00851895\n",
      "Epoch:  33 Train Loss: 0.00929342 Val Loss: 0.00998992\n",
      "Epoch:  34 Train Loss: 0.00836879 Val Loss: 0.00878494\n",
      "Epoch:  35 Train Loss: 0.00944052 Val Loss: 0.00966612\n",
      "Epoch:  36 Train Loss: 0.00818059 Val Loss: 0.00852133\n",
      "Epoch:  37 Train Loss: 0.00846357 Val Loss: 0.00770983\n",
      "Epoch:  38 Train Loss: 0.00839476 Val Loss: 0.00894264\n",
      "Epoch:  39 Train Loss: 0.00858793 Val Loss: 0.00760052\n",
      "Epoch:  40 Train Loss: 0.00930439 Val Loss: 0.00813582\n",
      "Epoch:  41 Train Loss: 0.00712767 Val Loss: 0.00766459\n",
      "Epoch:  42 Train Loss: 0.00687060 Val Loss: 0.00754318\n",
      "Epoch:  43 Train Loss: 0.00835537 Val Loss: 0.01128947\n",
      "Epoch:  44 Train Loss: 0.01067801 Val Loss: 0.00888319\n",
      "Epoch:  45 Train Loss: 0.01266748 Val Loss: 0.01597367\n",
      "Epoch:  46 Train Loss: 0.01038564 Val Loss: 0.01215769\n",
      "Epoch:  47 Train Loss: 0.01130279 Val Loss: 0.00867941\n",
      "Epoch:  48 Train Loss: 0.00852903 Val Loss: 0.00692074\n",
      "Epoch:  49 Train Loss: 0.00860194 Val Loss: 0.00937808\n",
      "Epoch:  50 Train Loss: 0.01007705 Val Loss: 0.01239906\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.12150801 Val Loss: 0.03485716\n",
      "Epoch:   2 Train Loss: 0.03571289 Val Loss: 0.02272755\n",
      "Epoch:   3 Train Loss: 0.01347375 Val Loss: 0.01965620\n",
      "Epoch:   4 Train Loss: 0.01556809 Val Loss: 0.01167602\n",
      "Epoch:   5 Train Loss: 0.01103871 Val Loss: 0.01198575\n",
      "Epoch:   6 Train Loss: 0.01151900 Val Loss: 0.00992839\n",
      "Epoch:   7 Train Loss: 0.01185798 Val Loss: 0.01161717\n",
      "Epoch:   8 Train Loss: 0.01368876 Val Loss: 0.01027801\n",
      "Epoch:   9 Train Loss: 0.00927942 Val Loss: 0.01140279\n",
      "Epoch:  10 Train Loss: 0.01010885 Val Loss: 0.00969485\n",
      "Epoch:  11 Train Loss: 0.01034525 Val Loss: 0.00876921\n",
      "Epoch:  12 Train Loss: 0.01058272 Val Loss: 0.00889614\n",
      "Epoch:  13 Train Loss: 0.00950648 Val Loss: 0.00898714\n",
      "Epoch:  14 Train Loss: 0.00980897 Val Loss: 0.01061727\n",
      "Epoch:  15 Train Loss: 0.00841601 Val Loss: 0.00840483\n",
      "Epoch:  16 Train Loss: 0.01107483 Val Loss: 0.01547488\n",
      "Epoch:  17 Train Loss: 0.01276796 Val Loss: 0.00924687\n",
      "Epoch:  18 Train Loss: 0.00934509 Val Loss: 0.00887516\n",
      "Epoch:  19 Train Loss: 0.00751145 Val Loss: 0.00889087\n",
      "Epoch:  20 Train Loss: 0.00921911 Val Loss: 0.00853210\n",
      "Epoch:  21 Train Loss: 0.00877182 Val Loss: 0.01052818\n",
      "Epoch:  22 Train Loss: 0.00940340 Val Loss: 0.00809660\n",
      "Epoch:  23 Train Loss: 0.00998124 Val Loss: 0.01126431\n",
      "Epoch:  24 Train Loss: 0.00963953 Val Loss: 0.00820759\n",
      "Epoch:  25 Train Loss: 0.00914216 Val Loss: 0.00839183\n",
      "Epoch:  26 Train Loss: 0.00916459 Val Loss: 0.00805958\n",
      "Epoch:  27 Train Loss: 0.00710504 Val Loss: 0.00772989\n",
      "Epoch:  28 Train Loss: 0.01104241 Val Loss: 0.00804177\n",
      "Epoch:  29 Train Loss: 0.00811834 Val Loss: 0.00728793\n",
      "Epoch:  30 Train Loss: 0.00774706 Val Loss: 0.00679794\n",
      "Epoch:  31 Train Loss: 0.00943846 Val Loss: 0.00736255\n",
      "Epoch:  32 Train Loss: 0.00785897 Val Loss: 0.00988943\n",
      "Epoch:  33 Train Loss: 0.00797737 Val Loss: 0.00441339\n",
      "Epoch:  34 Train Loss: 0.00544248 Val Loss: 0.00373025\n",
      "Epoch:  35 Train Loss: 0.00364359 Val Loss: 0.00460133\n",
      "Epoch:  36 Train Loss: 0.00689446 Val Loss: 0.01040777\n",
      "Epoch:  37 Train Loss: 0.00803411 Val Loss: 0.00661809\n",
      "Epoch:  38 Train Loss: 0.00683236 Val Loss: 0.00704225\n",
      "Epoch:  39 Train Loss: 0.00626451 Val Loss: 0.00556352\n",
      "Epoch:  40 Train Loss: 0.00659593 Val Loss: 0.00842992\n",
      "Epoch:  41 Train Loss: 0.00735414 Val Loss: 0.01088427\n",
      "Epoch:  42 Train Loss: 0.00812854 Val Loss: 0.00641822\n",
      "Epoch:  43 Train Loss: 0.00696385 Val Loss: 0.00541025\n",
      "Epoch:  44 Train Loss: 0.00763960 Val Loss: 0.00637468\n",
      "Epoch:  45 Train Loss: 0.00560833 Val Loss: 0.00554951\n",
      "Epoch:  46 Train Loss: 0.00533376 Val Loss: 0.00495089\n",
      "Epoch:  47 Train Loss: 0.00507844 Val Loss: 0.00613732\n",
      "Epoch:  48 Train Loss: 0.00584165 Val Loss: 0.00424815\n",
      "Epoch:  49 Train Loss: 0.00688918 Val Loss: 0.00414890\n",
      "Epoch:  50 Train Loss: 0.00497530 Val Loss: 0.00780752\n",
      "Epoch:  51 Train Loss: 0.00714301 Val Loss: 0.00548995\n",
      "Epoch:  52 Train Loss: 0.00638485 Val Loss: 0.00411919\n",
      "Epoch:  53 Train Loss: 0.00624873 Val Loss: 0.00594192\n",
      "Epoch:  54 Train Loss: 0.00639342 Val Loss: 0.00559234\n",
      "Epoch:  55 Train Loss: 0.00475281 Val Loss: 0.00521351\n",
      "Epoch:  56 Train Loss: 0.00416298 Val Loss: 0.00414498\n",
      "Epoch:  57 Train Loss: 0.00511210 Val Loss: 0.00786385\n",
      "Epoch:  58 Train Loss: 0.00758877 Val Loss: 0.00628102\n",
      "Epoch:  59 Train Loss: 0.00598117 Val Loss: 0.00491331\n",
      "Epoch:  60 Train Loss: 0.00615931 Val Loss: 0.00421105\n",
      "Epoch:  61 Train Loss: 0.00560285 Val Loss: 0.00547566\n",
      "Epoch:  62 Train Loss: 0.00481416 Val Loss: 0.00812086\n",
      "Epoch:  63 Train Loss: 0.00766946 Val Loss: 0.00514488\n",
      "Epoch:  64 Train Loss: 0.00467047 Val Loss: 0.00516883\n",
      "Epoch:  65 Train Loss: 0.00549922 Val Loss: 0.00384144\n",
      "Epoch:  66 Train Loss: 0.00409100 Val Loss: 0.00418931\n",
      "Epoch:  67 Train Loss: 0.00352692 Val Loss: 0.00406441\n",
      "Epoch:  68 Train Loss: 0.00361283 Val Loss: 0.00503783\n",
      "Epoch:  69 Train Loss: 0.00394277 Val Loss: 0.00335546\n",
      "Epoch:  70 Train Loss: 0.00327066 Val Loss: 0.00509846\n",
      "Epoch:  71 Train Loss: 0.00405955 Val Loss: 0.00483014\n",
      "Epoch:  72 Train Loss: 0.00471840 Val Loss: 0.00560418\n",
      "Epoch:  73 Train Loss: 0.00459703 Val Loss: 0.00362715\n",
      "Epoch:  74 Train Loss: 0.00354465 Val Loss: 0.00440205\n",
      "Epoch:  75 Train Loss: 0.00384153 Val Loss: 0.00605039\n",
      "Epoch:  76 Train Loss: 0.00446100 Val Loss: 0.00665759\n",
      "Epoch:  77 Train Loss: 0.00570778 Val Loss: 0.00312043\n",
      "Epoch:  78 Train Loss: 0.00316267 Val Loss: 0.00282998\n",
      "Epoch:  79 Train Loss: 0.00249493 Val Loss: 0.00441449\n",
      "Epoch:  80 Train Loss: 0.00328681 Val Loss: 0.00557782\n",
      "Epoch:  81 Train Loss: 0.00388377 Val Loss: 0.00342945\n",
      "Epoch:  82 Train Loss: 0.00385501 Val Loss: 0.00434115\n",
      "Epoch:  83 Train Loss: 0.00342306 Val Loss: 0.00387370\n",
      "Epoch:  84 Train Loss: 0.00385014 Val Loss: 0.00336871\n",
      "Epoch:  85 Train Loss: 0.00376009 Val Loss: 0.00263237\n",
      "Epoch:  86 Train Loss: 0.00351630 Val Loss: 0.00410735\n",
      "Epoch:  87 Train Loss: 0.00323460 Val Loss: 0.00238515\n",
      "Epoch:  88 Train Loss: 0.00229461 Val Loss: 0.00295314\n",
      "Epoch:  89 Train Loss: 0.00240991 Val Loss: 0.00314860\n",
      "Epoch:  90 Train Loss: 0.00296627 Val Loss: 0.00321099\n",
      "Epoch:  91 Train Loss: 0.00280158 Val Loss: 0.00254339\n",
      "Epoch:  92 Train Loss: 0.00254736 Val Loss: 0.00229602\n",
      "Epoch:  93 Train Loss: 0.00244029 Val Loss: 0.00179295\n",
      "Epoch:  94 Train Loss: 0.00177974 Val Loss: 0.00240265\n",
      "Epoch:  95 Train Loss: 0.00233517 Val Loss: 0.00191860\n",
      "Epoch:  96 Train Loss: 0.00207118 Val Loss: 0.00369274\n",
      "Epoch:  97 Train Loss: 0.00306703 Val Loss: 0.00279870\n",
      "Epoch:  98 Train Loss: 0.00220543 Val Loss: 0.00273411\n",
      "Epoch:  99 Train Loss: 0.00236587 Val Loss: 0.00294822\n",
      "Epoch: 100 Train Loss: 0.00235411 Val Loss: 0.00199431\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.12634980 Val Loss: 0.04045632\n",
      "Epoch:   2 Train Loss: 0.06143503 Val Loss: 0.03466457\n",
      "Epoch:   3 Train Loss: 0.04393099 Val Loss: 0.02507225\n",
      "Epoch:   4 Train Loss: 0.03272679 Val Loss: 0.01785048\n",
      "Epoch:   5 Train Loss: 0.01972458 Val Loss: 0.01050283\n",
      "Epoch:   6 Train Loss: 0.00935005 Val Loss: 0.01488603\n",
      "Epoch:   7 Train Loss: 0.01094784 Val Loss: 0.00752369\n",
      "Epoch:   8 Train Loss: 0.01035391 Val Loss: 0.00790623\n",
      "Epoch:   9 Train Loss: 0.00874308 Val Loss: 0.00802496\n",
      "Epoch:  10 Train Loss: 0.00804262 Val Loss: 0.00738988\n",
      "Epoch:  11 Train Loss: 0.00897390 Val Loss: 0.00747994\n",
      "Epoch:  12 Train Loss: 0.00898092 Val Loss: 0.01322864\n",
      "Epoch:  13 Train Loss: 0.01176273 Val Loss: 0.00894389\n",
      "Epoch:  14 Train Loss: 0.00951232 Val Loss: 0.01399311\n",
      "Epoch:  15 Train Loss: 0.01228378 Val Loss: 0.00727000\n",
      "Epoch:  16 Train Loss: 0.00821182 Val Loss: 0.00809599\n",
      "Epoch:  17 Train Loss: 0.00806787 Val Loss: 0.00727245\n",
      "Epoch:  18 Train Loss: 0.00856200 Val Loss: 0.00897891\n",
      "Epoch:  19 Train Loss: 0.00788761 Val Loss: 0.00754046\n",
      "Epoch:  20 Train Loss: 0.00751094 Val Loss: 0.00762115\n",
      "Epoch:  21 Train Loss: 0.00888480 Val Loss: 0.00754211\n",
      "Epoch:  22 Train Loss: 0.00911582 Val Loss: 0.00878166\n",
      "Epoch:  23 Train Loss: 0.00981197 Val Loss: 0.01165497\n",
      "Epoch:  24 Train Loss: 0.00941274 Val Loss: 0.00784475\n",
      "Epoch:  25 Train Loss: 0.00838137 Val Loss: 0.00744588\n",
      "Epoch:  26 Train Loss: 0.00973262 Val Loss: 0.00879466\n",
      "Epoch:  27 Train Loss: 0.01014344 Val Loss: 0.00783632\n",
      "Epoch:  28 Train Loss: 0.00979278 Val Loss: 0.00933695\n",
      "Epoch:  29 Train Loss: 0.00793484 Val Loss: 0.00778143\n",
      "Epoch:  30 Train Loss: 0.00842343 Val Loss: 0.00931399\n",
      "Epoch:  31 Train Loss: 0.00861769 Val Loss: 0.00708306\n",
      "Epoch:  32 Train Loss: 0.00785920 Val Loss: 0.00717059\n",
      "Epoch:  33 Train Loss: 0.00724871 Val Loss: 0.00685910\n",
      "Epoch:  34 Train Loss: 0.00800485 Val Loss: 0.00795981\n",
      "Epoch:  35 Train Loss: 0.00851957 Val Loss: 0.00685450\n",
      "Epoch:  36 Train Loss: 0.00651299 Val Loss: 0.00684363\n",
      "Epoch:  37 Train Loss: 0.00811274 Val Loss: 0.00792274\n",
      "Epoch:  38 Train Loss: 0.00840161 Val Loss: 0.00709623\n",
      "Epoch:  39 Train Loss: 0.00837285 Val Loss: 0.00866657\n",
      "Epoch:  40 Train Loss: 0.00755999 Val Loss: 0.00677608\n",
      "Epoch:  41 Train Loss: 0.00696772 Val Loss: 0.00691905\n",
      "Epoch:  42 Train Loss: 0.00680746 Val Loss: 0.00677327\n",
      "Epoch:  43 Train Loss: 0.00957722 Val Loss: 0.00666622\n",
      "Epoch:  44 Train Loss: 0.00647710 Val Loss: 0.00739266\n",
      "Epoch:  45 Train Loss: 0.00699525 Val Loss: 0.00671055\n",
      "Epoch:  46 Train Loss: 0.00714030 Val Loss: 0.00662873\n",
      "Epoch:  47 Train Loss: 0.00643408 Val Loss: 0.00683031\n",
      "Epoch:  48 Train Loss: 0.00676524 Val Loss: 0.00662877\n",
      "Epoch:  49 Train Loss: 0.00633497 Val Loss: 0.00664016\n",
      "Epoch:  50 Train Loss: 0.00909787 Val Loss: 0.00652905\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.13040972 Val Loss: 0.04115822\n",
      "Epoch:   2 Train Loss: 0.07358968 Val Loss: 0.03614056\n",
      "Epoch:   3 Train Loss: 0.06544589 Val Loss: 0.05331378\n",
      "Epoch:   4 Train Loss: 0.05061673 Val Loss: 0.02590874\n",
      "Epoch:   5 Train Loss: 0.03921975 Val Loss: 0.02383680\n",
      "Epoch:   6 Train Loss: 0.03302743 Val Loss: 0.01751859\n",
      "Epoch:   7 Train Loss: 0.02096030 Val Loss: 0.01115529\n",
      "Epoch:   8 Train Loss: 0.00909206 Val Loss: 0.00848572\n",
      "Epoch:   9 Train Loss: 0.00965346 Val Loss: 0.01171837\n",
      "Epoch:  10 Train Loss: 0.01396657 Val Loss: 0.01482811\n",
      "Epoch:  11 Train Loss: 0.00834770 Val Loss: 0.00991994\n",
      "Epoch:  12 Train Loss: 0.01088453 Val Loss: 0.01084009\n",
      "Epoch:  13 Train Loss: 0.00880775 Val Loss: 0.00774909\n",
      "Epoch:  14 Train Loss: 0.00949982 Val Loss: 0.01219094\n",
      "Epoch:  15 Train Loss: 0.00923247 Val Loss: 0.00775845\n",
      "Epoch:  16 Train Loss: 0.00857674 Val Loss: 0.00834394\n",
      "Epoch:  17 Train Loss: 0.00778649 Val Loss: 0.00753935\n",
      "Epoch:  18 Train Loss: 0.00941247 Val Loss: 0.00767787\n",
      "Epoch:  19 Train Loss: 0.00928913 Val Loss: 0.00764914\n",
      "Epoch:  20 Train Loss: 0.00919939 Val Loss: 0.00858992\n",
      "Epoch:  21 Train Loss: 0.01145030 Val Loss: 0.00779573\n",
      "Epoch:  22 Train Loss: 0.00801719 Val Loss: 0.00737868\n",
      "Epoch:  23 Train Loss: 0.00807221 Val Loss: 0.00754709\n",
      "Epoch:  24 Train Loss: 0.00705943 Val Loss: 0.00720022\n",
      "Epoch:  25 Train Loss: 0.00786122 Val Loss: 0.00758728\n",
      "Epoch:  26 Train Loss: 0.00762487 Val Loss: 0.00718278\n",
      "Epoch:  27 Train Loss: 0.00812457 Val Loss: 0.00844605\n",
      "Epoch:  28 Train Loss: 0.00853977 Val Loss: 0.00715781\n",
      "Epoch:  29 Train Loss: 0.00903717 Val Loss: 0.00725698\n",
      "Epoch:  30 Train Loss: 0.00705312 Val Loss: 0.00747467\n",
      "Epoch:  31 Train Loss: 0.00843839 Val Loss: 0.00732515\n",
      "Epoch:  32 Train Loss: 0.00802997 Val Loss: 0.00696779\n",
      "Epoch:  33 Train Loss: 0.00808160 Val Loss: 0.00759703\n",
      "Epoch:  34 Train Loss: 0.00743747 Val Loss: 0.00725334\n",
      "Epoch:  35 Train Loss: 0.00813002 Val Loss: 0.00689134\n",
      "Epoch:  36 Train Loss: 0.00804758 Val Loss: 0.00715596\n",
      "Epoch:  37 Train Loss: 0.00850956 Val Loss: 0.00732652\n",
      "Epoch:  38 Train Loss: 0.00916027 Val Loss: 0.00711182\n",
      "Epoch:  39 Train Loss: 0.00820596 Val Loss: 0.00730056\n",
      "Epoch:  40 Train Loss: 0.00922946 Val Loss: 0.00932421\n",
      "Epoch:  41 Train Loss: 0.00850452 Val Loss: 0.00717550\n",
      "Epoch:  42 Train Loss: 0.00935477 Val Loss: 0.00853926\n",
      "Epoch:  43 Train Loss: 0.00875605 Val Loss: 0.00718458\n",
      "Epoch:  44 Train Loss: 0.00756157 Val Loss: 0.00681464\n",
      "Epoch:  45 Train Loss: 0.00912781 Val Loss: 0.00804674\n",
      "Epoch:  46 Train Loss: 0.00903612 Val Loss: 0.00786814\n",
      "Epoch:  47 Train Loss: 0.00938802 Val Loss: 0.00874666\n",
      "Epoch:  48 Train Loss: 0.00658470 Val Loss: 0.00709420\n",
      "Epoch:  49 Train Loss: 0.00862164 Val Loss: 0.00725522\n",
      "Epoch:  50 Train Loss: 0.00699709 Val Loss: 0.00746012\n",
      "Epoch:  51 Train Loss: 0.00971727 Val Loss: 0.00806565\n",
      "Epoch:  52 Train Loss: 0.00820302 Val Loss: 0.00661968\n",
      "Epoch:  53 Train Loss: 0.00703762 Val Loss: 0.00677024\n",
      "Epoch:  54 Train Loss: 0.00896724 Val Loss: 0.00682146\n",
      "Epoch:  55 Train Loss: 0.00670799 Val Loss: 0.00662833\n",
      "Epoch:  56 Train Loss: 0.00724666 Val Loss: 0.00659859\n",
      "Epoch:  57 Train Loss: 0.00742136 Val Loss: 0.00894370\n",
      "Epoch:  58 Train Loss: 0.00728555 Val Loss: 0.00847218\n",
      "Epoch:  59 Train Loss: 0.00945098 Val Loss: 0.01004634\n",
      "Epoch:  60 Train Loss: 0.00763451 Val Loss: 0.00701181\n",
      "Epoch:  61 Train Loss: 0.00770942 Val Loss: 0.00696464\n",
      "Epoch:  62 Train Loss: 0.00594482 Val Loss: 0.00652200\n",
      "Epoch:  63 Train Loss: 0.00651262 Val Loss: 0.00717274\n",
      "Epoch:  64 Train Loss: 0.00790489 Val Loss: 0.00651702\n",
      "Epoch:  65 Train Loss: 0.00762631 Val Loss: 0.00647063\n",
      "Epoch:  66 Train Loss: 0.00717458 Val Loss: 0.00689165\n",
      "Epoch:  67 Train Loss: 0.00705950 Val Loss: 0.00645484\n",
      "Epoch:  68 Train Loss: 0.00747052 Val Loss: 0.00653482\n",
      "Epoch:  69 Train Loss: 0.00769154 Val Loss: 0.00753440\n",
      "Epoch:  70 Train Loss: 0.00619056 Val Loss: 0.00684366\n",
      "Epoch:  71 Train Loss: 0.00615380 Val Loss: 0.00671565\n",
      "Epoch:  72 Train Loss: 0.00619916 Val Loss: 0.00653266\n",
      "Epoch:  73 Train Loss: 0.00670768 Val Loss: 0.00679202\n",
      "Epoch:  74 Train Loss: 0.00862203 Val Loss: 0.00690629\n",
      "Epoch:  75 Train Loss: 0.00900944 Val Loss: 0.00809539\n",
      "Epoch:  76 Train Loss: 0.00784634 Val Loss: 0.00866355\n",
      "Epoch:  77 Train Loss: 0.00796846 Val Loss: 0.00650302\n",
      "Epoch:  78 Train Loss: 0.00712083 Val Loss: 0.00634709\n",
      "Epoch:  79 Train Loss: 0.00655158 Val Loss: 0.00668853\n",
      "Epoch:  80 Train Loss: 0.00616863 Val Loss: 0.00669865\n",
      "Epoch:  81 Train Loss: 0.00680560 Val Loss: 0.00653843\n",
      "Epoch:  82 Train Loss: 0.00703586 Val Loss: 0.00859859\n",
      "Epoch:  83 Train Loss: 0.00816467 Val Loss: 0.00638700\n",
      "Epoch:  84 Train Loss: 0.00636823 Val Loss: 0.00634295\n",
      "Epoch:  85 Train Loss: 0.00744038 Val Loss: 0.00628516\n",
      "Epoch:  86 Train Loss: 0.00641410 Val Loss: 0.00674658\n",
      "Epoch:  87 Train Loss: 0.00663454 Val Loss: 0.00628398\n",
      "Epoch:  88 Train Loss: 0.00627542 Val Loss: 0.00635324\n",
      "Epoch:  89 Train Loss: 0.00657733 Val Loss: 0.00684817\n",
      "Epoch:  90 Train Loss: 0.00727966 Val Loss: 0.00629764\n",
      "Epoch:  91 Train Loss: 0.00653497 Val Loss: 0.00642093\n",
      "Epoch:  92 Train Loss: 0.00701814 Val Loss: 0.00665442\n",
      "Epoch:  93 Train Loss: 0.00825511 Val Loss: 0.00658443\n",
      "Epoch:  94 Train Loss: 0.00643575 Val Loss: 0.00647849\n",
      "Epoch:  95 Train Loss: 0.00761147 Val Loss: 0.00672648\n",
      "Epoch:  96 Train Loss: 0.00695837 Val Loss: 0.00751042\n",
      "Epoch:  97 Train Loss: 0.00681529 Val Loss: 0.00677204\n",
      "Epoch:  98 Train Loss: 0.00685609 Val Loss: 0.00694330\n",
      "Epoch:  99 Train Loss: 0.00654840 Val Loss: 0.00624485\n",
      "Epoch: 100 Train Loss: 0.00608076 Val Loss: 0.00630389\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.10132519 Val Loss: 0.03234749\n",
      "Epoch:   2 Train Loss: 0.04059665 Val Loss: 0.02272424\n",
      "Epoch:   3 Train Loss: 0.01738364 Val Loss: 0.01440470\n",
      "Epoch:   4 Train Loss: 0.01140746 Val Loss: 0.01041493\n",
      "Epoch:   5 Train Loss: 0.01166526 Val Loss: 0.01033839\n",
      "Epoch:   6 Train Loss: 0.01041126 Val Loss: 0.01138420\n",
      "Epoch:   7 Train Loss: 0.01075297 Val Loss: 0.00930552\n",
      "Epoch:   8 Train Loss: 0.01111515 Val Loss: 0.00886250\n",
      "Epoch:   9 Train Loss: 0.00834823 Val Loss: 0.01468825\n",
      "Epoch:  10 Train Loss: 0.01113969 Val Loss: 0.00976061\n",
      "Epoch:  11 Train Loss: 0.01109761 Val Loss: 0.01286119\n",
      "Epoch:  12 Train Loss: 0.01471086 Val Loss: 0.00927091\n",
      "Epoch:  13 Train Loss: 0.01001041 Val Loss: 0.00966090\n",
      "Epoch:  14 Train Loss: 0.00843820 Val Loss: 0.00930877\n",
      "Epoch:  15 Train Loss: 0.01041070 Val Loss: 0.01478515\n",
      "Epoch:  16 Train Loss: 0.01235779 Val Loss: 0.01051617\n",
      "Epoch:  17 Train Loss: 0.00925500 Val Loss: 0.00987712\n",
      "Epoch:  18 Train Loss: 0.00873484 Val Loss: 0.00853297\n",
      "Epoch:  19 Train Loss: 0.00911189 Val Loss: 0.00848853\n",
      "Epoch:  20 Train Loss: 0.00858056 Val Loss: 0.00821537\n",
      "Epoch:  21 Train Loss: 0.00846323 Val Loss: 0.00888625\n",
      "Epoch:  22 Train Loss: 0.00859869 Val Loss: 0.01110839\n",
      "Epoch:  23 Train Loss: 0.00954114 Val Loss: 0.00913566\n",
      "Epoch:  24 Train Loss: 0.00994290 Val Loss: 0.00809248\n",
      "Epoch:  25 Train Loss: 0.00841524 Val Loss: 0.01002792\n",
      "Epoch:  26 Train Loss: 0.00977216 Val Loss: 0.00805504\n",
      "Epoch:  27 Train Loss: 0.00908007 Val Loss: 0.00767637\n",
      "Epoch:  28 Train Loss: 0.01002668 Val Loss: 0.00840813\n",
      "Epoch:  29 Train Loss: 0.00689971 Val Loss: 0.00719802\n",
      "Epoch:  30 Train Loss: 0.00736547 Val Loss: 0.00864703\n",
      "Epoch:  31 Train Loss: 0.01152538 Val Loss: 0.00921235\n",
      "Epoch:  32 Train Loss: 0.00899046 Val Loss: 0.00546689\n",
      "Epoch:  33 Train Loss: 0.00838498 Val Loss: 0.00571335\n",
      "Epoch:  34 Train Loss: 0.00601576 Val Loss: 0.00678678\n",
      "Epoch:  35 Train Loss: 0.01049437 Val Loss: 0.00914359\n",
      "Epoch:  36 Train Loss: 0.00872929 Val Loss: 0.00787963\n",
      "Epoch:  37 Train Loss: 0.00865999 Val Loss: 0.00864987\n",
      "Epoch:  38 Train Loss: 0.00804279 Val Loss: 0.00669649\n",
      "Epoch:  39 Train Loss: 0.00743120 Val Loss: 0.00797189\n",
      "Epoch:  40 Train Loss: 0.00495319 Val Loss: 0.00410654\n",
      "Epoch:  41 Train Loss: 0.00471292 Val Loss: 0.00420794\n",
      "Epoch:  42 Train Loss: 0.00597461 Val Loss: 0.01060066\n",
      "Epoch:  43 Train Loss: 0.01012841 Val Loss: 0.01010606\n",
      "Epoch:  44 Train Loss: 0.00694662 Val Loss: 0.00914426\n",
      "Epoch:  45 Train Loss: 0.00598683 Val Loss: 0.01107976\n",
      "Epoch:  46 Train Loss: 0.00817172 Val Loss: 0.00599369\n",
      "Epoch:  47 Train Loss: 0.00643832 Val Loss: 0.00436420\n",
      "Epoch:  48 Train Loss: 0.00517152 Val Loss: 0.00592388\n",
      "Epoch:  49 Train Loss: 0.00438791 Val Loss: 0.00372861\n",
      "Epoch:  50 Train Loss: 0.00392658 Val Loss: 0.00406681\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.2, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.09285992 Val Loss: 0.02877784\n",
      "Epoch:   2 Train Loss: 0.02915761 Val Loss: 0.04385222\n",
      "Epoch:   3 Train Loss: 0.02230286 Val Loss: 0.01519309\n",
      "Epoch:   4 Train Loss: 0.01789369 Val Loss: 0.01478956\n",
      "Epoch:   5 Train Loss: 0.01141524 Val Loss: 0.00992307\n",
      "Epoch:   6 Train Loss: 0.00942530 Val Loss: 0.00956965\n",
      "Epoch:   7 Train Loss: 0.01070927 Val Loss: 0.00949546\n",
      "Epoch:   8 Train Loss: 0.00940392 Val Loss: 0.01034593\n",
      "Epoch:   9 Train Loss: 0.01021518 Val Loss: 0.01066800\n",
      "Epoch:  10 Train Loss: 0.00916884 Val Loss: 0.00957896\n",
      "Epoch:  11 Train Loss: 0.00970421 Val Loss: 0.00835189\n",
      "Epoch:  12 Train Loss: 0.01054481 Val Loss: 0.00875582\n",
      "Epoch:  13 Train Loss: 0.01168947 Val Loss: 0.00792633\n",
      "Epoch:  14 Train Loss: 0.01230787 Val Loss: 0.01001070\n",
      "Epoch:  15 Train Loss: 0.01037136 Val Loss: 0.00800255\n",
      "Epoch:  16 Train Loss: 0.00826822 Val Loss: 0.01012356\n",
      "Epoch:  17 Train Loss: 0.01015553 Val Loss: 0.00988300\n",
      "Epoch:  18 Train Loss: 0.01036278 Val Loss: 0.00819460\n",
      "Epoch:  19 Train Loss: 0.00866212 Val Loss: 0.00769520\n",
      "Epoch:  20 Train Loss: 0.00977999 Val Loss: 0.00756762\n",
      "Epoch:  21 Train Loss: 0.00846153 Val Loss: 0.00612053\n",
      "Epoch:  22 Train Loss: 0.00730419 Val Loss: 0.00574491\n",
      "Epoch:  23 Train Loss: 0.00580665 Val Loss: 0.00721808\n",
      "Epoch:  24 Train Loss: 0.00476520 Val Loss: 0.00703472\n",
      "Epoch:  25 Train Loss: 0.00538594 Val Loss: 0.00439958\n",
      "Epoch:  26 Train Loss: 0.00498943 Val Loss: 0.00522189\n",
      "Epoch:  27 Train Loss: 0.00519398 Val Loss: 0.00587808\n",
      "Epoch:  28 Train Loss: 0.00689439 Val Loss: 0.00891866\n",
      "Epoch:  29 Train Loss: 0.00636843 Val Loss: 0.00410431\n",
      "Epoch:  30 Train Loss: 0.00533314 Val Loss: 0.00424619\n",
      "Epoch:  31 Train Loss: 0.00596796 Val Loss: 0.00481283\n",
      "Epoch:  32 Train Loss: 0.00728048 Val Loss: 0.00412379\n",
      "Epoch:  33 Train Loss: 0.00834617 Val Loss: 0.00613510\n",
      "Epoch:  34 Train Loss: 0.00674727 Val Loss: 0.00501849\n",
      "Epoch:  35 Train Loss: 0.00441827 Val Loss: 0.00469682\n",
      "Epoch:  36 Train Loss: 0.00389166 Val Loss: 0.00453352\n",
      "Epoch:  37 Train Loss: 0.00557471 Val Loss: 0.00391652\n",
      "Epoch:  38 Train Loss: 0.00583062 Val Loss: 0.00449492\n",
      "Epoch:  39 Train Loss: 0.00666985 Val Loss: 0.00559278\n",
      "Epoch:  40 Train Loss: 0.00563342 Val Loss: 0.00644593\n",
      "Epoch:  41 Train Loss: 0.00619586 Val Loss: 0.00688685\n",
      "Epoch:  42 Train Loss: 0.00594745 Val Loss: 0.00466291\n",
      "Epoch:  43 Train Loss: 0.00644424 Val Loss: 0.00508450\n",
      "Epoch:  44 Train Loss: 0.00668959 Val Loss: 0.00601596\n",
      "Epoch:  45 Train Loss: 0.00597967 Val Loss: 0.00414091\n",
      "Epoch:  46 Train Loss: 0.00486999 Val Loss: 0.00425906\n",
      "Epoch:  47 Train Loss: 0.00495806 Val Loss: 0.00389555\n",
      "Epoch:  48 Train Loss: 0.00438283 Val Loss: 0.00489060\n",
      "Epoch:  49 Train Loss: 0.00527030 Val Loss: 0.00874347\n",
      "Epoch:  50 Train Loss: 0.00701978 Val Loss: 0.00567049\n",
      "Epoch:  51 Train Loss: 0.00497092 Val Loss: 0.00682258\n",
      "Epoch:  52 Train Loss: 0.00570347 Val Loss: 0.00854072\n",
      "Epoch:  53 Train Loss: 0.00467863 Val Loss: 0.00482192\n",
      "Epoch:  54 Train Loss: 0.00445801 Val Loss: 0.00456074\n",
      "Epoch:  55 Train Loss: 0.00498042 Val Loss: 0.00621672\n",
      "Epoch:  56 Train Loss: 0.00450810 Val Loss: 0.00808402\n",
      "Epoch:  57 Train Loss: 0.00623004 Val Loss: 0.01624084\n",
      "Epoch:  58 Train Loss: 0.00976646 Val Loss: 0.00648802\n",
      "Epoch:  59 Train Loss: 0.00493425 Val Loss: 0.00530595\n",
      "Epoch:  60 Train Loss: 0.00440576 Val Loss: 0.00531902\n",
      "Epoch:  61 Train Loss: 0.00486858 Val Loss: 0.00530690\n",
      "Epoch:  62 Train Loss: 0.00444228 Val Loss: 0.00772634\n",
      "Epoch:  63 Train Loss: 0.00413094 Val Loss: 0.00612850\n",
      "Epoch:  64 Train Loss: 0.00470028 Val Loss: 0.00491608\n",
      "Epoch:  65 Train Loss: 0.00409617 Val Loss: 0.00596524\n",
      "Epoch:  66 Train Loss: 0.00447581 Val Loss: 0.00587473\n",
      "Epoch:  67 Train Loss: 0.00356101 Val Loss: 0.00426936\n",
      "Epoch:  68 Train Loss: 0.00286161 Val Loss: 0.00299051\n",
      "Epoch:  69 Train Loss: 0.00343078 Val Loss: 0.00440107\n",
      "Epoch:  70 Train Loss: 0.00410411 Val Loss: 0.00313450\n",
      "Epoch:  71 Train Loss: 0.00332521 Val Loss: 0.00337272\n",
      "Epoch:  72 Train Loss: 0.00321770 Val Loss: 0.00366718\n",
      "Epoch:  73 Train Loss: 0.00220444 Val Loss: 0.00272533\n",
      "Epoch:  74 Train Loss: 0.00243073 Val Loss: 0.00228363\n",
      "Epoch:  75 Train Loss: 0.00265949 Val Loss: 0.00461398\n",
      "Epoch:  76 Train Loss: 0.00392653 Val Loss: 0.00420075\n",
      "Epoch:  77 Train Loss: 0.00444879 Val Loss: 0.00924155\n",
      "Epoch:  78 Train Loss: 0.00509705 Val Loss: 0.00415001\n",
      "Epoch:  79 Train Loss: 0.00425862 Val Loss: 0.00610301\n",
      "Epoch:  80 Train Loss: 0.00462150 Val Loss: 0.00336451\n",
      "Epoch:  81 Train Loss: 0.00302347 Val Loss: 0.00333794\n",
      "Epoch:  82 Train Loss: 0.00299891 Val Loss: 0.00247227\n",
      "Epoch:  83 Train Loss: 0.00294759 Val Loss: 0.00275346\n",
      "Epoch:  84 Train Loss: 0.00286359 Val Loss: 0.00318253\n",
      "Epoch:  85 Train Loss: 0.00253487 Val Loss: 0.00296965\n",
      "Epoch:  86 Train Loss: 0.00205388 Val Loss: 0.00393372\n",
      "Epoch:  87 Train Loss: 0.00267334 Val Loss: 0.00283489\n",
      "Epoch:  88 Train Loss: 0.00209703 Val Loss: 0.00223514\n",
      "Epoch:  89 Train Loss: 0.00220766 Val Loss: 0.00484942\n",
      "Epoch:  90 Train Loss: 0.00474201 Val Loss: 0.00353682\n",
      "Epoch:  91 Train Loss: 0.00261510 Val Loss: 0.00314824\n",
      "Epoch:  92 Train Loss: 0.00240917 Val Loss: 0.00299041\n",
      "Epoch:  93 Train Loss: 0.00251171 Val Loss: 0.00319832\n",
      "Epoch:  94 Train Loss: 0.00216593 Val Loss: 0.00248039\n",
      "Epoch:  95 Train Loss: 0.00224468 Val Loss: 0.00224873\n",
      "Epoch:  96 Train Loss: 0.00213825 Val Loss: 0.00236652\n",
      "Epoch:  97 Train Loss: 0.00273904 Val Loss: 0.00227009\n",
      "Epoch:  98 Train Loss: 0.00210610 Val Loss: 0.00194382\n",
      "Epoch:  99 Train Loss: 0.00326074 Val Loss: 0.00316875\n",
      "Epoch: 100 Train Loss: 0.00291519 Val Loss: 0.00354142\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.08788937 Val Loss: 0.04216976\n",
      "Epoch:   2 Train Loss: 0.05305968 Val Loss: 0.03441804\n",
      "Epoch:   3 Train Loss: 0.04744467 Val Loss: 0.02917563\n",
      "Epoch:   4 Train Loss: 0.03646690 Val Loss: 0.02215851\n",
      "Epoch:   5 Train Loss: 0.02838668 Val Loss: 0.01496381\n",
      "Epoch:   6 Train Loss: 0.01346068 Val Loss: 0.01579517\n",
      "Epoch:   7 Train Loss: 0.01464633 Val Loss: 0.00893961\n",
      "Epoch:   8 Train Loss: 0.01333326 Val Loss: 0.00829518\n",
      "Epoch:   9 Train Loss: 0.01010029 Val Loss: 0.00964253\n",
      "Epoch:  10 Train Loss: 0.00876665 Val Loss: 0.00755354\n",
      "Epoch:  11 Train Loss: 0.00855302 Val Loss: 0.00834536\n",
      "Epoch:  12 Train Loss: 0.00885814 Val Loss: 0.00795806\n",
      "Epoch:  13 Train Loss: 0.00794076 Val Loss: 0.00767267\n",
      "Epoch:  14 Train Loss: 0.00870870 Val Loss: 0.00794772\n",
      "Epoch:  15 Train Loss: 0.00981534 Val Loss: 0.00846350\n",
      "Epoch:  16 Train Loss: 0.00881241 Val Loss: 0.00830955\n",
      "Epoch:  17 Train Loss: 0.01124813 Val Loss: 0.01282546\n",
      "Epoch:  18 Train Loss: 0.01091071 Val Loss: 0.01035414\n",
      "Epoch:  19 Train Loss: 0.01132270 Val Loss: 0.01234130\n",
      "Epoch:  20 Train Loss: 0.01015607 Val Loss: 0.00896587\n",
      "Epoch:  21 Train Loss: 0.00897991 Val Loss: 0.00884478\n",
      "Epoch:  22 Train Loss: 0.00868639 Val Loss: 0.00741835\n",
      "Epoch:  23 Train Loss: 0.00810779 Val Loss: 0.00760190\n",
      "Epoch:  24 Train Loss: 0.00782800 Val Loss: 0.00727701\n",
      "Epoch:  25 Train Loss: 0.00899308 Val Loss: 0.00727478\n",
      "Epoch:  26 Train Loss: 0.00744324 Val Loss: 0.00708089\n",
      "Epoch:  27 Train Loss: 0.00721070 Val Loss: 0.00705581\n",
      "Epoch:  28 Train Loss: 0.00798021 Val Loss: 0.00764327\n",
      "Epoch:  29 Train Loss: 0.00885851 Val Loss: 0.00730541\n",
      "Epoch:  30 Train Loss: 0.00779723 Val Loss: 0.00733864\n",
      "Epoch:  31 Train Loss: 0.00689716 Val Loss: 0.00699113\n",
      "Epoch:  32 Train Loss: 0.00724980 Val Loss: 0.00725348\n",
      "Epoch:  33 Train Loss: 0.00852799 Val Loss: 0.00741436\n",
      "Epoch:  34 Train Loss: 0.00869918 Val Loss: 0.00725392\n",
      "Epoch:  35 Train Loss: 0.00878868 Val Loss: 0.00910654\n",
      "Epoch:  36 Train Loss: 0.00814818 Val Loss: 0.00714635\n",
      "Epoch:  37 Train Loss: 0.00891291 Val Loss: 0.00679583\n",
      "Epoch:  38 Train Loss: 0.00743930 Val Loss: 0.00762992\n",
      "Epoch:  39 Train Loss: 0.00705784 Val Loss: 0.00712377\n",
      "Epoch:  40 Train Loss: 0.00770368 Val Loss: 0.00871564\n",
      "Epoch:  41 Train Loss: 0.00725430 Val Loss: 0.00694378\n",
      "Epoch:  42 Train Loss: 0.00696627 Val Loss: 0.00725642\n",
      "Epoch:  43 Train Loss: 0.00748253 Val Loss: 0.00689268\n",
      "Epoch:  44 Train Loss: 0.00725693 Val Loss: 0.00677230\n",
      "Epoch:  45 Train Loss: 0.00878374 Val Loss: 0.00701962\n",
      "Epoch:  46 Train Loss: 0.00775531 Val Loss: 0.00665176\n",
      "Epoch:  47 Train Loss: 0.00730594 Val Loss: 0.00657940\n",
      "Epoch:  48 Train Loss: 0.00710469 Val Loss: 0.00669946\n",
      "Epoch:  49 Train Loss: 0.00743192 Val Loss: 0.00655805\n",
      "Epoch:  50 Train Loss: 0.00768679 Val Loss: 0.00972350\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.001, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.09439447 Val Loss: 0.03774051\n",
      "Epoch:   2 Train Loss: 0.05625301 Val Loss: 0.03410243\n",
      "Epoch:   3 Train Loss: 0.04615136 Val Loss: 0.03193160\n",
      "Epoch:   4 Train Loss: 0.03324217 Val Loss: 0.02129230\n",
      "Epoch:   5 Train Loss: 0.02458293 Val Loss: 0.01315315\n",
      "Epoch:   6 Train Loss: 0.01283232 Val Loss: 0.01622690\n",
      "Epoch:   7 Train Loss: 0.01068966 Val Loss: 0.00796712\n",
      "Epoch:   8 Train Loss: 0.00980784 Val Loss: 0.00794371\n",
      "Epoch:   9 Train Loss: 0.01024232 Val Loss: 0.01198618\n",
      "Epoch:  10 Train Loss: 0.00886321 Val Loss: 0.00804892\n",
      "Epoch:  11 Train Loss: 0.00883344 Val Loss: 0.00936741\n",
      "Epoch:  12 Train Loss: 0.00797053 Val Loss: 0.00723243\n",
      "Epoch:  13 Train Loss: 0.00994787 Val Loss: 0.00743906\n",
      "Epoch:  14 Train Loss: 0.00814190 Val Loss: 0.00718674\n",
      "Epoch:  15 Train Loss: 0.00779158 Val Loss: 0.00724345\n",
      "Epoch:  16 Train Loss: 0.00753285 Val Loss: 0.00717008\n",
      "Epoch:  17 Train Loss: 0.00813703 Val Loss: 0.00730910\n",
      "Epoch:  18 Train Loss: 0.00848931 Val Loss: 0.00815430\n",
      "Epoch:  19 Train Loss: 0.00828594 Val Loss: 0.00807138\n",
      "Epoch:  20 Train Loss: 0.00750221 Val Loss: 0.00703774\n",
      "Epoch:  21 Train Loss: 0.00912028 Val Loss: 0.00747569\n",
      "Epoch:  22 Train Loss: 0.00705445 Val Loss: 0.00727832\n",
      "Epoch:  23 Train Loss: 0.00909395 Val Loss: 0.00832946\n",
      "Epoch:  24 Train Loss: 0.01003816 Val Loss: 0.00705938\n",
      "Epoch:  25 Train Loss: 0.00785808 Val Loss: 0.00690998\n",
      "Epoch:  26 Train Loss: 0.00911405 Val Loss: 0.00676101\n",
      "Epoch:  27 Train Loss: 0.00747915 Val Loss: 0.00711920\n",
      "Epoch:  28 Train Loss: 0.00858502 Val Loss: 0.00830772\n",
      "Epoch:  29 Train Loss: 0.00931981 Val Loss: 0.00881234\n",
      "Epoch:  30 Train Loss: 0.00767246 Val Loss: 0.00672173\n",
      "Epoch:  31 Train Loss: 0.00690941 Val Loss: 0.00733072\n",
      "Epoch:  32 Train Loss: 0.00699217 Val Loss: 0.00702870\n",
      "Epoch:  33 Train Loss: 0.00745384 Val Loss: 0.00719806\n",
      "Epoch:  34 Train Loss: 0.01121445 Val Loss: 0.01306537\n",
      "Epoch:  35 Train Loss: 0.01209451 Val Loss: 0.00688158\n",
      "Epoch:  36 Train Loss: 0.00888396 Val Loss: 0.00689914\n",
      "Epoch:  37 Train Loss: 0.00823817 Val Loss: 0.00819381\n",
      "Epoch:  38 Train Loss: 0.00770559 Val Loss: 0.00673293\n",
      "Epoch:  39 Train Loss: 0.00683048 Val Loss: 0.00712338\n",
      "Epoch:  40 Train Loss: 0.00741031 Val Loss: 0.00782338\n",
      "Epoch:  41 Train Loss: 0.00728624 Val Loss: 0.00714372\n",
      "Epoch:  42 Train Loss: 0.00701125 Val Loss: 0.00856248\n",
      "Epoch:  43 Train Loss: 0.00824731 Val Loss: 0.00693994\n",
      "Epoch:  44 Train Loss: 0.00893707 Val Loss: 0.00744772\n",
      "Epoch:  45 Train Loss: 0.00767236 Val Loss: 0.00663324\n",
      "Epoch:  46 Train Loss: 0.00740032 Val Loss: 0.00667084\n",
      "Epoch:  47 Train Loss: 0.00865280 Val Loss: 0.00662633\n",
      "Epoch:  48 Train Loss: 0.00796389 Val Loss: 0.01043970\n",
      "Epoch:  49 Train Loss: 0.00931036 Val Loss: 0.00894497\n",
      "Epoch:  50 Train Loss: 0.00831990 Val Loss: 0.00991486\n",
      "Epoch:  51 Train Loss: 0.00883860 Val Loss: 0.00661817\n",
      "Epoch:  52 Train Loss: 0.00805216 Val Loss: 0.00732761\n",
      "Epoch:  53 Train Loss: 0.01004475 Val Loss: 0.00764805\n",
      "Epoch:  54 Train Loss: 0.00848824 Val Loss: 0.00744319\n",
      "Epoch:  55 Train Loss: 0.00731212 Val Loss: 0.00689701\n",
      "Epoch:  56 Train Loss: 0.00848848 Val Loss: 0.00663801\n",
      "Epoch:  57 Train Loss: 0.00761749 Val Loss: 0.00700130\n",
      "Epoch:  58 Train Loss: 0.00632451 Val Loss: 0.00673949\n",
      "Epoch:  59 Train Loss: 0.00749627 Val Loss: 0.00739913\n",
      "Epoch:  60 Train Loss: 0.00845941 Val Loss: 0.00687245\n",
      "Epoch:  61 Train Loss: 0.00698785 Val Loss: 0.00686453\n",
      "Epoch:  62 Train Loss: 0.00696015 Val Loss: 0.00712158\n",
      "Epoch:  63 Train Loss: 0.00631989 Val Loss: 0.00683744\n",
      "Epoch:  64 Train Loss: 0.00691500 Val Loss: 0.00725382\n",
      "Epoch:  65 Train Loss: 0.00661572 Val Loss: 0.00693273\n",
      "Epoch:  66 Train Loss: 0.00727108 Val Loss: 0.00721738\n",
      "Epoch:  67 Train Loss: 0.00891462 Val Loss: 0.00662953\n",
      "Epoch:  68 Train Loss: 0.00724650 Val Loss: 0.00711292\n",
      "Epoch:  69 Train Loss: 0.00684277 Val Loss: 0.00644390\n",
      "Epoch:  70 Train Loss: 0.00684390 Val Loss: 0.00665577\n",
      "Epoch:  71 Train Loss: 0.00677330 Val Loss: 0.00647902\n",
      "Epoch:  72 Train Loss: 0.00946348 Val Loss: 0.00659930\n",
      "Epoch:  73 Train Loss: 0.00761681 Val Loss: 0.00649457\n",
      "Epoch:  74 Train Loss: 0.00715813 Val Loss: 0.00658742\n",
      "Epoch:  75 Train Loss: 0.00617915 Val Loss: 0.00633131\n",
      "Epoch:  76 Train Loss: 0.00607731 Val Loss: 0.00664174\n",
      "Epoch:  77 Train Loss: 0.00672937 Val Loss: 0.00695079\n",
      "Epoch:  78 Train Loss: 0.00663522 Val Loss: 0.00788978\n",
      "Epoch:  79 Train Loss: 0.00796702 Val Loss: 0.00670360\n",
      "Epoch:  80 Train Loss: 0.00702531 Val Loss: 0.00753343\n",
      "Epoch:  81 Train Loss: 0.00757826 Val Loss: 0.00653755\n",
      "Epoch:  82 Train Loss: 0.00661424 Val Loss: 0.00647695\n",
      "Epoch:  83 Train Loss: 0.00635349 Val Loss: 0.00645019\n",
      "Epoch:  84 Train Loss: 0.00743349 Val Loss: 0.00721345\n",
      "Epoch:  85 Train Loss: 0.00923185 Val Loss: 0.00633340\n",
      "Epoch:  86 Train Loss: 0.00682694 Val Loss: 0.00630388\n",
      "Epoch:  87 Train Loss: 0.00662613 Val Loss: 0.00747829\n",
      "Epoch:  88 Train Loss: 0.00756334 Val Loss: 0.00678391\n",
      "Epoch:  89 Train Loss: 0.00693418 Val Loss: 0.00775277\n",
      "Epoch:  90 Train Loss: 0.00781611 Val Loss: 0.00700109\n",
      "Epoch:  91 Train Loss: 0.00797348 Val Loss: 0.00716874\n",
      "Epoch:  92 Train Loss: 0.00840496 Val Loss: 0.00656898\n",
      "Epoch:  93 Train Loss: 0.00660522 Val Loss: 0.00633226\n",
      "Epoch:  94 Train Loss: 0.00571924 Val Loss: 0.00626570\n",
      "Epoch:  95 Train Loss: 0.00624795 Val Loss: 0.00652941\n",
      "Epoch:  96 Train Loss: 0.00605255 Val Loss: 0.00621239\n",
      "Epoch:  97 Train Loss: 0.00807926 Val Loss: 0.00633327\n",
      "Epoch:  98 Train Loss: 0.00810300 Val Loss: 0.00659790\n",
      "Epoch:  99 Train Loss: 0.00745523 Val Loss: 0.00785378\n",
      "Epoch: 100 Train Loss: 0.00608780 Val Loss: 0.00638926\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 50}\n",
      "Epoch:   1 Train Loss: 0.10361728 Val Loss: 0.07704644\n",
      "Epoch:   2 Train Loss: 0.05273526 Val Loss: 0.02215527\n",
      "Epoch:   3 Train Loss: 0.02591001 Val Loss: 0.01411830\n",
      "Epoch:   4 Train Loss: 0.01275214 Val Loss: 0.02348098\n",
      "Epoch:   5 Train Loss: 0.02086856 Val Loss: 0.01182229\n",
      "Epoch:   6 Train Loss: 0.02208889 Val Loss: 0.01577965\n",
      "Epoch:   7 Train Loss: 0.01167661 Val Loss: 0.01099250\n",
      "Epoch:   8 Train Loss: 0.01212987 Val Loss: 0.01034472\n",
      "Epoch:   9 Train Loss: 0.01180290 Val Loss: 0.00996391\n",
      "Epoch:  10 Train Loss: 0.00927119 Val Loss: 0.00967077\n",
      "Epoch:  11 Train Loss: 0.01110513 Val Loss: 0.01046570\n",
      "Epoch:  12 Train Loss: 0.01032319 Val Loss: 0.01355264\n",
      "Epoch:  13 Train Loss: 0.01164567 Val Loss: 0.00907772\n",
      "Epoch:  14 Train Loss: 0.01068061 Val Loss: 0.00986818\n",
      "Epoch:  15 Train Loss: 0.00993475 Val Loss: 0.00846401\n",
      "Epoch:  16 Train Loss: 0.00786659 Val Loss: 0.00642812\n",
      "Epoch:  17 Train Loss: 0.00666801 Val Loss: 0.00597802\n",
      "Epoch:  18 Train Loss: 0.00841443 Val Loss: 0.00959038\n",
      "Epoch:  19 Train Loss: 0.00987467 Val Loss: 0.00635446\n",
      "Epoch:  20 Train Loss: 0.00723822 Val Loss: 0.00912352\n",
      "Epoch:  21 Train Loss: 0.00749225 Val Loss: 0.01383935\n",
      "Epoch:  22 Train Loss: 0.00984437 Val Loss: 0.00846460\n",
      "Epoch:  23 Train Loss: 0.01252723 Val Loss: 0.00870013\n",
      "Epoch:  24 Train Loss: 0.01276463 Val Loss: 0.00797610\n",
      "Epoch:  25 Train Loss: 0.00895607 Val Loss: 0.00890274\n",
      "Epoch:  26 Train Loss: 0.00951375 Val Loss: 0.00825985\n",
      "Epoch:  27 Train Loss: 0.01057594 Val Loss: 0.01017331\n",
      "Epoch:  28 Train Loss: 0.00992359 Val Loss: 0.01792747\n",
      "Epoch:  29 Train Loss: 0.01409752 Val Loss: 0.01391998\n",
      "Epoch:  30 Train Loss: 0.00937668 Val Loss: 0.01208688\n",
      "Epoch:  31 Train Loss: 0.00795542 Val Loss: 0.00656154\n",
      "Epoch:  32 Train Loss: 0.00739092 Val Loss: 0.01161710\n",
      "Epoch:  33 Train Loss: 0.00750792 Val Loss: 0.00532681\n",
      "Epoch:  34 Train Loss: 0.00609069 Val Loss: 0.00553464\n",
      "Epoch:  35 Train Loss: 0.00459519 Val Loss: 0.00545042\n",
      "Epoch:  36 Train Loss: 0.00555621 Val Loss: 0.00503512\n",
      "Epoch:  37 Train Loss: 0.00507815 Val Loss: 0.00397768\n",
      "Epoch:  38 Train Loss: 0.00409356 Val Loss: 0.00579068\n",
      "Epoch:  39 Train Loss: 0.00437208 Val Loss: 0.00366547\n",
      "Epoch:  40 Train Loss: 0.00508454 Val Loss: 0.00519835\n",
      "Epoch:  41 Train Loss: 0.00394484 Val Loss: 0.00498893\n",
      "Epoch:  42 Train Loss: 0.00445721 Val Loss: 0.00316065\n",
      "Epoch:  43 Train Loss: 0.00371996 Val Loss: 0.00294705\n",
      "Epoch:  44 Train Loss: 0.00333441 Val Loss: 0.00345941\n",
      "Epoch:  45 Train Loss: 0.00315206 Val Loss: 0.00409076\n",
      "Epoch:  46 Train Loss: 0.00317477 Val Loss: 0.00421744\n",
      "Epoch:  47 Train Loss: 0.00395500 Val Loss: 0.00303770\n",
      "Epoch:  48 Train Loss: 0.00391314 Val Loss: 0.00371418\n",
      "Epoch:  49 Train Loss: 0.00274228 Val Loss: 0.00286745\n",
      "Epoch:  50 Train Loss: 0.00301753 Val Loss: 0.00269811\n",
      "\n",
      "Training with parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Epoch:   1 Train Loss: 0.11090480 Val Loss: 0.04864241\n",
      "Epoch:   2 Train Loss: 0.04075240 Val Loss: 0.01517710\n",
      "Epoch:   3 Train Loss: 0.02295051 Val Loss: 0.02252274\n",
      "Epoch:   4 Train Loss: 0.01782888 Val Loss: 0.02145133\n",
      "Epoch:   5 Train Loss: 0.01508333 Val Loss: 0.01057840\n",
      "Epoch:   6 Train Loss: 0.01246497 Val Loss: 0.01157711\n",
      "Epoch:   7 Train Loss: 0.01358224 Val Loss: 0.01565074\n",
      "Epoch:   8 Train Loss: 0.01396690 Val Loss: 0.00920970\n",
      "Epoch:   9 Train Loss: 0.01270128 Val Loss: 0.01267564\n",
      "Epoch:  10 Train Loss: 0.01330294 Val Loss: 0.00912483\n",
      "Epoch:  11 Train Loss: 0.01272107 Val Loss: 0.01877331\n",
      "Epoch:  12 Train Loss: 0.01792235 Val Loss: 0.01438099\n",
      "Epoch:  13 Train Loss: 0.01381678 Val Loss: 0.01035368\n",
      "Epoch:  14 Train Loss: 0.01388884 Val Loss: 0.01468944\n",
      "Epoch:  15 Train Loss: 0.01563203 Val Loss: 0.01003972\n",
      "Epoch:  16 Train Loss: 0.01273820 Val Loss: 0.00947794\n",
      "Epoch:  17 Train Loss: 0.00876745 Val Loss: 0.00948618\n",
      "Epoch:  18 Train Loss: 0.00887605 Val Loss: 0.00899091\n",
      "Epoch:  19 Train Loss: 0.00892327 Val Loss: 0.00948096\n",
      "Epoch:  20 Train Loss: 0.01149540 Val Loss: 0.01011786\n",
      "Epoch:  21 Train Loss: 0.01165808 Val Loss: 0.00952389\n",
      "Epoch:  22 Train Loss: 0.01124165 Val Loss: 0.01077169\n",
      "Epoch:  23 Train Loss: 0.01028409 Val Loss: 0.00940376\n",
      "Epoch:  24 Train Loss: 0.01237134 Val Loss: 0.01197643\n",
      "Epoch:  25 Train Loss: 0.01095042 Val Loss: 0.00999231\n",
      "Epoch:  26 Train Loss: 0.01003576 Val Loss: 0.00897750\n",
      "Epoch:  27 Train Loss: 0.00905870 Val Loss: 0.00890264\n",
      "Epoch:  28 Train Loss: 0.01006871 Val Loss: 0.01170842\n",
      "Epoch:  29 Train Loss: 0.01198030 Val Loss: 0.00870517\n",
      "Epoch:  30 Train Loss: 0.01353363 Val Loss: 0.00854947\n",
      "Epoch:  31 Train Loss: 0.01011130 Val Loss: 0.00920494\n",
      "Epoch:  32 Train Loss: 0.00937910 Val Loss: 0.00961726\n",
      "Epoch:  33 Train Loss: 0.01223729 Val Loss: 0.01155325\n",
      "Epoch:  34 Train Loss: 0.01240749 Val Loss: 0.00968450\n",
      "Epoch:  35 Train Loss: 0.00941744 Val Loss: 0.00981090\n",
      "Epoch:  36 Train Loss: 0.00995273 Val Loss: 0.00881062\n",
      "Epoch:  37 Train Loss: 0.01369040 Val Loss: 0.01054151\n",
      "Epoch:  38 Train Loss: 0.01152419 Val Loss: 0.01311830\n",
      "Epoch:  39 Train Loss: 0.00895848 Val Loss: 0.00923669\n",
      "Epoch:  40 Train Loss: 0.00878895 Val Loss: 0.00897421\n",
      "Epoch:  41 Train Loss: 0.00979839 Val Loss: 0.00858590\n",
      "Epoch:  42 Train Loss: 0.00826554 Val Loss: 0.01006821\n",
      "Epoch:  43 Train Loss: 0.00919263 Val Loss: 0.00895478\n",
      "Epoch:  44 Train Loss: 0.00913459 Val Loss: 0.01065042\n",
      "Epoch:  45 Train Loss: 0.00985899 Val Loss: 0.00912659\n",
      "Epoch:  46 Train Loss: 0.00897861 Val Loss: 0.01440214\n",
      "Epoch:  47 Train Loss: 0.01142101 Val Loss: 0.01117663\n",
      "Epoch:  48 Train Loss: 0.01109147 Val Loss: 0.00965312\n",
      "Epoch:  49 Train Loss: 0.01275554 Val Loss: 0.00683569\n",
      "Epoch:  50 Train Loss: 0.00995172 Val Loss: 0.00674487\n",
      "Epoch:  51 Train Loss: 0.00806556 Val Loss: 0.01163526\n",
      "Epoch:  52 Train Loss: 0.00920534 Val Loss: 0.00736422\n",
      "Epoch:  53 Train Loss: 0.00666066 Val Loss: 0.00503121\n",
      "Epoch:  54 Train Loss: 0.00511907 Val Loss: 0.00597279\n",
      "Epoch:  55 Train Loss: 0.00557558 Val Loss: 0.00536157\n",
      "Epoch:  56 Train Loss: 0.00590409 Val Loss: 0.00514399\n",
      "Epoch:  57 Train Loss: 0.00437523 Val Loss: 0.00723578\n",
      "Epoch:  58 Train Loss: 0.00549357 Val Loss: 0.00520637\n",
      "Epoch:  59 Train Loss: 0.00491113 Val Loss: 0.00466766\n",
      "Epoch:  60 Train Loss: 0.00464880 Val Loss: 0.00421285\n",
      "Epoch:  61 Train Loss: 0.00475366 Val Loss: 0.00458555\n",
      "Epoch:  62 Train Loss: 0.00529670 Val Loss: 0.00356829\n",
      "Epoch:  63 Train Loss: 0.00480206 Val Loss: 0.00414045\n",
      "Epoch:  64 Train Loss: 0.00461286 Val Loss: 0.01029305\n",
      "Epoch:  65 Train Loss: 0.00676570 Val Loss: 0.00577036\n",
      "Epoch:  66 Train Loss: 0.00522233 Val Loss: 0.00593544\n",
      "Epoch:  67 Train Loss: 0.00460614 Val Loss: 0.00479206\n",
      "Epoch:  68 Train Loss: 0.00430069 Val Loss: 0.00494952\n",
      "Epoch:  69 Train Loss: 0.00665289 Val Loss: 0.00728313\n",
      "Epoch:  70 Train Loss: 0.00583915 Val Loss: 0.00490244\n",
      "Epoch:  71 Train Loss: 0.00460649 Val Loss: 0.00449979\n",
      "Epoch:  72 Train Loss: 0.00609094 Val Loss: 0.00544203\n",
      "Epoch:  73 Train Loss: 0.00629736 Val Loss: 0.00404604\n",
      "Epoch:  74 Train Loss: 0.00409078 Val Loss: 0.00431011\n",
      "Epoch:  75 Train Loss: 0.00415676 Val Loss: 0.00430803\n",
      "Epoch:  76 Train Loss: 0.00403247 Val Loss: 0.00349166\n",
      "Epoch:  77 Train Loss: 0.00376381 Val Loss: 0.00386345\n",
      "Epoch:  78 Train Loss: 0.00401249 Val Loss: 0.00689587\n",
      "Epoch:  79 Train Loss: 0.00558665 Val Loss: 0.00342653\n",
      "Epoch:  80 Train Loss: 0.00365368 Val Loss: 0.00405498\n",
      "Epoch:  81 Train Loss: 0.00434136 Val Loss: 0.00517567\n",
      "Epoch:  82 Train Loss: 0.00448399 Val Loss: 0.00353481\n",
      "Epoch:  83 Train Loss: 0.00446257 Val Loss: 0.00494346\n",
      "Epoch:  84 Train Loss: 0.00529518 Val Loss: 0.00324255\n",
      "Epoch:  85 Train Loss: 0.00406160 Val Loss: 0.00427846\n",
      "Epoch:  86 Train Loss: 0.00387381 Val Loss: 0.00352594\n",
      "Epoch:  87 Train Loss: 0.00359083 Val Loss: 0.00419513\n",
      "Epoch:  88 Train Loss: 0.00411236 Val Loss: 0.00302611\n",
      "Epoch:  89 Train Loss: 0.00470867 Val Loss: 0.00570930\n",
      "Epoch:  90 Train Loss: 0.00706286 Val Loss: 0.00535958\n",
      "Epoch:  91 Train Loss: 0.00604399 Val Loss: 0.00501629\n",
      "Epoch:  92 Train Loss: 0.00476889 Val Loss: 0.00872063\n",
      "Epoch:  93 Train Loss: 0.00489242 Val Loss: 0.00700285\n",
      "Epoch:  94 Train Loss: 0.00448081 Val Loss: 0.00512189\n",
      "Epoch:  95 Train Loss: 0.00552675 Val Loss: 0.00427887\n",
      "Epoch:  96 Train Loss: 0.00841971 Val Loss: 0.00392153\n",
      "Epoch:  97 Train Loss: 0.00504936 Val Loss: 0.00502011\n",
      "Epoch:  98 Train Loss: 0.00520862 Val Loss: 0.00475256\n",
      "Epoch:  99 Train Loss: 0.00442833 Val Loss: 0.00427907\n",
      "Epoch: 100 Train Loss: 0.00425604 Val Loss: 0.00461510\n",
      "Best Parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1, 'learning_rate': 0.01, 'epochs': 100}\n",
      "Best Validation Loss: 0.00199430511565879\n",
      "Test Loss: 0.0026341244229115546\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "best_model, best_params, best_loss = grid_search(param_grid, train_loader, val_loader)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Validation Loss:\", best_loss)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = evaluate(best_model, test_loader, nn.MSELoss())\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Predictions: [[ 5175.531 ]\n",
      " [ 4414.773 ]\n",
      " [ 8704.764 ]\n",
      " [ 7852.542 ]\n",
      " [ 5286.2266]\n",
      " [ 4450.204 ]\n",
      " [12156.849 ]\n",
      " [ 9323.387 ]\n",
      " [12624.079 ]\n",
      " [10049.918 ]\n",
      " [ 7177.953 ]\n",
      " [ 4394.1367]\n",
      " [ 7435.7295]\n",
      " [11401.228 ]\n",
      " [10933.045 ]\n",
      " [ 5600.3955]\n",
      " [ 4714.942 ]\n",
      " [ 6346.586 ]\n",
      " [ 5185.4175]\n",
      " [ 9378.503 ]\n",
      " [ 4822.499 ]\n",
      " [ 8941.598 ]\n",
      " [11681.171 ]\n",
      " [ 6652.829 ]\n",
      " [ 9647.715 ]\n",
      " [ 5971.749 ]\n",
      " [ 7732.7046]\n",
      " [ 7978.944 ]\n",
      " [ 5101.7   ]\n",
      " [11068.48  ]\n",
      " [ 9677.647 ]\n",
      " [ 6746.274 ]\n",
      " [ 4456.763 ]\n",
      " [ 9613.387 ]\n",
      " [ 6173.508 ]\n",
      " [ 5094.836 ]\n",
      " [11575.153 ]\n",
      " [ 9416.955 ]\n",
      " [ 4860.6187]\n",
      " [ 9274.641 ]\n",
      " [ 4873.0967]\n",
      " [ 6850.4688]\n",
      " [ 4648.7427]\n",
      " [ 5850.078 ]\n",
      " [ 7976.46  ]\n",
      " [12919.559 ]\n",
      " [13550.831 ]\n",
      " [13817.873 ]\n",
      " [ 4597.2764]\n",
      " [12009.451 ]\n",
      " [ 5798.168 ]\n",
      " [ 7358.578 ]\n",
      " [ 6163.0586]\n",
      " [11495.384 ]\n",
      " [ 6985.5913]\n",
      " [ 6007.4785]\n",
      " [ 9821.364 ]\n",
      " [ 6648.3853]\n",
      " [ 7644.563 ]\n",
      " [10089.254 ]\n",
      " [11327.394 ]\n",
      " [10476.401 ]\n",
      " [ 8511.087 ]]\n"
     ]
    }
   ],
   "source": [
    "def make_predictions(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in data_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            predictions.extend(y_pred.numpy().flatten())\n",
    "    return predictions\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = make_predictions(best_model, test_loader)\n",
    "\n",
    "# Inverse transform predictions if you used scaling\n",
    "test_predictions_original_scale = scaler.inverse_transform(np.array(test_predictions).reshape(-1, 1))\n",
    "\n",
    "print(\"Test Predictions:\", test_predictions_original_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_original_scale = np.array(test_predictions_original_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7358.578 ],\n",
       "       [ 6163.0586],\n",
       "       [11495.384 ],\n",
       "       [ 6985.5913],\n",
       "       [ 6007.4785],\n",
       "       [ 9821.364 ],\n",
       "       [ 6648.3853],\n",
       "       [ 7644.563 ],\n",
       "       [10089.254 ],\n",
       "       [11327.394 ],\n",
       "       [10476.401 ],\n",
       "       [ 8511.087 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_original_scale = test_predictions_original_scale[-12:]\n",
    "test_predictions_original_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "2018-02-01    10415\n",
       "2018-03-01    12683\n",
       "2018-04-01    11919\n",
       "2018-05-01    14138\n",
       "2018-06-01    14583\n",
       "2018-07-01    12640\n",
       "2018-08-01    14257\n",
       "2018-09-01    12396\n",
       "2018-10-01    13914\n",
       "2018-11-01    14174\n",
       "2018-12-01    15504\n",
       "2019-01-01    10718\n",
       "Name: Sales, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sales'][-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "\n",
    "x = np.arange('2018-02-01', '2019-02-01', dtype='datetime64[M]').astype('datetime64[D]')\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title('Alcohol Sales')\n",
    "plt.ylabel('Sales in million dollars')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "plt.plot(df['Sales'], color='blue')\n",
    "plt.plot(x, test_predictions_original_scale, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zooming the test predictions\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "plt.title('Alcohol Sales')\n",
    "plt.ylabel('Sales in million dollars')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.plot(df['Sales']['2017-01-01':], color='blue')\n",
    "plt.plot(x, test_predictions_original_scale, color='red')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
